<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-03-24T15:46:10-06:00</updated><id>http://localhost:4000/</id><title type="html">Eggink Blog</title><subtitle>Jake W. Knigge's blah, blah, blog... a place to discuss statistics, math, and  whatever else comes to mind.
</subtitle><entry><title type="html">Some (convex) relaxation</title><link href="http://localhost:4000/jekyll/update/2018/03/24/convex-relaxation.html" rel="alternate" type="text/html" title="Some (convex) relaxation" /><published>2018-03-24T17:00:00-06:00</published><updated>2018-03-24T17:00:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/03/24/convex-relaxation</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/03/24/convex-relaxation.html">&lt;p&gt;Okay, last time I said that the next post would be about logarithmic 
Sobolev inequalities… well, it’s not. I lied. Instead, we’re taking a 
momentary break from concentration inequalities. It’s time for some
relaxation of the convex variety.&lt;/p&gt;

&lt;p&gt;Actually, that’s a bit of lie too because the post isn’t about convex 
relaxation, but it &lt;strong&gt;is&lt;/strong&gt; about convex optimization. Andiamo!&lt;/p&gt;

&lt;h3 id=&quot;maximum-likelihood-of-poisson-distributions&quot;&gt;Maximum likelihood of Poisson distributions&lt;/h3&gt;

&lt;p&gt;We’re going to work through a problem from &lt;em&gt;Convex Optimization&lt;/em&gt; by Boyd
and Vandenberghe. (If you’re curious it’s problem 7.7.) Here’s the set up.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We have &lt;script type=&quot;math/tex&quot;&gt;X_1,\dots,X_n&lt;/script&gt; independent Poisson random variables. That is
each &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; has a probability mass function parameterized by 
&lt;script type=&quot;math/tex&quot;&gt;\lambda_i &gt; 0&lt;/script&gt; that is given by&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\prob (X_i = k) = \frac{e^{-\lambda_i}\lambda_i^k}{k!}.&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;The data &lt;script type=&quot;math/tex&quot;&gt;x_1,\dots,x_n&lt;/script&gt; represent the number of times that one of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;
possible events occurred (independently of one another).&lt;/li&gt;
  &lt;li&gt;We have &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; detectors and event &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; is recorded by detector &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; with
probability &lt;script type=&quot;math/tex&quot;&gt;p_{ji}&lt;/script&gt;. We assume that &lt;script type=&quot;math/tex&quot;&gt;p_{ji} \ge 0&lt;/script&gt;, 
&lt;script type=&quot;math/tex&quot;&gt;\sum_{j=1}^m p_{ji} \le 1&lt;/script&gt;, &lt;strong&gt;and&lt;/strong&gt; that the &lt;script type=&quot;math/tex&quot;&gt;p_{ji}&lt;/script&gt; are given.&lt;/li&gt;
  &lt;li&gt;The total number of events recorded by detector &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; is&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_j = \sum_{i=1}^n y_{ji}, \quad j = 1,\dots,m.&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Our goal is to estimate the &lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt; based on observations &lt;script type=&quot;math/tex&quot;&gt;y_j&lt;/script&gt;
and the given probabilities &lt;script type=&quot;math/tex&quot;&gt;p_{ji}&lt;/script&gt; by solving a convex optimization
problem.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To pose this estimation problem as a convex optimization problem, we’ll
use the fact that the &lt;script type=&quot;math/tex&quot;&gt;y_{ji}&lt;/script&gt; were generated from a Poisson distribution
with mean &lt;script type=&quot;math/tex&quot;&gt;p_{ji}\lambda_i&lt;/script&gt;. Intuitively, this means we only capture
a fraction, in particular &lt;script type=&quot;math/tex&quot;&gt;p_{ji}&lt;/script&gt;, of the Poisson-distributed events 
associated with &lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt;. For those who have had a course on Markov 
chains this should cause your &lt;strong&gt;thinned Poisson process&lt;/strong&gt; neurons to fire.&lt;/p&gt;

&lt;p&gt;Now, we use the fact that the sum of Poisson random variables is itself 
a Poisson random variable. In particular, we have that the &lt;script type=&quot;math/tex&quot;&gt;y_j&lt;/script&gt; were
generated by a Poisson distribution with parameter&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_j^T \lambda = \sum_{i=1}^n p_{ji}\lambda_i,&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;p_j \in \reals^n&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\lambda \in \reals_+^n&lt;/script&gt; is the vector of
&lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;likelihood&lt;/strong&gt; of the data has the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\prod_{j=1}^m \frac{(p_j^T \lambda)^{y_j}e^{-(p_j^T \lambda)}}{y_j!}&lt;/script&gt;

&lt;p&gt;which gives a &lt;strong&gt;log-likelihood&lt;/strong&gt; of&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{j=1}^m \left( y_j\log(p_j^T \lambda)-p_j^T \lambda-\log(y_j!) \right).&lt;/script&gt;

&lt;p&gt;We’ll throw away the &lt;script type=&quot;math/tex&quot;&gt;-\log(y_j!)&lt;/script&gt; term because it is a constant. So, the 
estimation problem is the convex optimization problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}[ll]
\mbox{\text{maximize}} &amp; \sum_{j=1}^m \big( y_j \log(p_j^T \lambda) -
	 p_j^T \lambda \big)
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;with optimization variable &lt;script type=&quot;math/tex&quot;&gt;\lambda \in \reals_+^n&lt;/script&gt; and problem data
&lt;script type=&quot;math/tex&quot;&gt;y_1,\dots,y_m&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;p_{ji}&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;i = 1,\dots,n&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;j = 1,d\dots,m&lt;/script&gt;.
More explicitly, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}[ll]
\mbox{\text{maximize}} &amp; \sum_{j=1}^m \big( y_j \log(p_j^T \lambda) -
	 p_j^T \lambda \big) \\
\mbox{subject to} &amp; \lambda \succeq 0.
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Put it to use.&lt;/strong&gt; Now, go out and find yourself a particle accelerator, 
equip it with some detectors, start colliding particles and count your 
Higgs-Bosons and make some inferences.&lt;/p&gt;

&lt;h3 id=&quot;next-up&quot;&gt;Next up&lt;/h3&gt;

&lt;p&gt;… logarithmic Sobolev inequalities—for real this time (well, next time)!&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is based on material from…&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Convex Optimization&lt;/em&gt; by Stephen Boyd and Lieven Vandenberghe. Check it
out here: &lt;em&gt;&lt;a href=&quot;https://web.stanford.edu/~boyd/cvxbook/&quot;&gt;Convex Optimization&lt;/a&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Okay, last time I said that the next post would be about logarithmic Sobolev inequalities… well, it’s not. I lied. Instead, we’re taking a momentary break from concentration inequalities. It’s time for some relaxation of the convex variety.</summary></entry><entry><title type="html">Entropy and its subadditivity</title><link href="http://localhost:4000/jekyll/update/2018/03/17/sub-entropy.html" rel="alternate" type="text/html" title="Entropy and its subadditivity" /><published>2018-03-17T20:00:00-06:00</published><updated>2018-03-17T20:00:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/03/17/sub-entropy</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/03/17/sub-entropy.html">&lt;p&gt;(Happy St. Patrick’s Day!)&lt;/p&gt;

&lt;p&gt;In this post, we’re picking up from the previous post and continuing with
some information-theoretic ideas as they arise in concentration inequalities.
We’ll quickly revisit the Efron–Stein inequality and then proceed towards
the subadditivity of entropy.&lt;/p&gt;

&lt;h3 id=&quot;efronstein&quot;&gt;Efron–Stein&lt;/h3&gt;

&lt;p&gt;To encourage the formation of new (or reinforce existing) neuronal 
connections, let’s remind ourselves of the Efron–Stein inequality. 
If we have independent random variables &lt;script type=&quot;math/tex&quot;&gt;X_1,\dots,X_n&lt;/script&gt; and a 
square-integrable function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;Z = f(X)&lt;/script&gt;, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Var(Z) \le \sum_{i=1}^n \E \left[ \E^{(i)}[Z^2] - 
	\big( \E^{(i)}Z \big)^2 \right],&lt;/script&gt;

&lt;p&gt;where the operator &lt;script type=&quot;math/tex&quot;&gt;\E^{(i)}&lt;/script&gt; is the conditional expectation with respect
to the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th variable. The square-integrable condition on &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; ensures 
that its variance is finite.&lt;/p&gt;

&lt;h3 id=&quot;entropy&quot;&gt;Entropy&lt;/h3&gt;

&lt;p&gt;Now, let’s generalize the Efron–Stein inequality by observing that we
&lt;em&gt;could&lt;/em&gt; write it as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E [Z^2] - \big( \E Z \big)^2 \le 
	\sum_{i=1}^n \E \left[ \E^{(i)}[Z^2] - \big( \E^{(i)}Z \big)^2 \right],&lt;/script&gt;

&lt;p&gt;where we used the fact that &lt;script type=&quot;math/tex&quot;&gt;\Var(Y) = \E [Y^2] - \big( \E Y \big)^2&lt;/script&gt;.
If we stare at this representation of the various for a moment, 
we see that it has the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E [g(Z)] - g( \E Z),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;g(x) = x^2&lt;/script&gt;. When &lt;script type=&quot;math/tex&quot;&gt;g(x) = x \log x&lt;/script&gt;, then quantity 
&lt;script type=&quot;math/tex&quot;&gt;\E [g(Z)] - g( \E Z)&lt;/script&gt; is often called the &lt;strong&gt;entropy&lt;/strong&gt; and is
denoted by &lt;script type=&quot;math/tex&quot;&gt;\mathrm{Ent}(Z)&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;subadditivity-of-entropy&quot;&gt;Subadditivity of entropy&lt;/h3&gt;

&lt;p&gt;Using the Efron–Stein inequality as our source of inspiration, we
might guess (or hope) that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E [g(Z)] - g( \E Z) \le 
	\sum_{i=1}^n \E \left[ \E^{(i)}g(Z) - g\big( \E^{(i)}Z \big) \right].&lt;/script&gt;

&lt;p&gt;If we did guess this, then we’d be correct! So, let’s prove it.&lt;/p&gt;

&lt;p&gt;We’ll assume that &lt;script type=&quot;math/tex&quot;&gt;X_1,\dots,X_n&lt;/script&gt; are independent random variables 
taking values in a finite set &lt;script type=&quot;math/tex&quot;&gt;\mathcal{S}&lt;/script&gt; and that 
&lt;script type=&quot;math/tex&quot;&gt;f: \mathcal{S^n} \to \reals_+&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;Z = f(X_1,\dots,X_n)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The proof relies on an application of Han’s inequality for relative
entropies (which is an extension of what we showed in the 
&lt;a href=&quot;https://jakeknigge.github.io/jekyll/update/2018/03/11/han.html&quot;&gt;previous post&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Note that &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; is always positive, so the result holds for any
&lt;script type=&quot;math/tex&quot;&gt;cZ&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;c&gt;0&lt;/script&gt;. Let’s use this to simplify our lives and assume that
&lt;script type=&quot;math/tex&quot;&gt;\E Z = 1&lt;/script&gt;. So, now we can write&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E g(Z) - g( \E Z) = \E[Z \log Z] - 0.&lt;/script&gt;

&lt;p&gt;Now, we’re going to introduce the relative entropy component by
defining&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q(x) = f(x)p(x) \iff \frac{q(x)}{p(x)} = f(x).&lt;/script&gt;

&lt;p&gt;The relative entropy neurons in your brain are probably going wild 
because we can now write&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z = f(X) = \frac{q(X)}{p(X)}.&lt;/script&gt;

&lt;p&gt;So, let’s take an expectation of &lt;script type=&quot;math/tex&quot;&gt;g(Z)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\E g(Z) &amp;= \E g\big( f(X) \big) \\
	&amp;= \E_P \left[ \frac{q(X)}{p(X)} \log \frac{q(X)}{p(X)} \right] \\
	&amp;= D_{\mathrm{kl}}(q(X), p(X)).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Here’s where &lt;strong&gt;Han’s inequality&lt;/strong&gt; comes to the scene, because the
relative entropy form of Han’s inequality says that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_{\mathrm{kl}}(q(X), p(X)) \le \sum_{i=1}^n
	\left( D_{\mathrm{kl}}(q(X), p(X)) - 
	D_{\mathrm{kl}}\left(q^{(i)}(X), p^{(i)}(X)\right) \right).&lt;/script&gt;

&lt;p&gt;In other words, we have a bound on the (joint) relative entropy based on 
the sum of marginal relative entropies. Lastly, we’ll show that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n 
	\left( D_{\mathrm{kl}}(q(X), p(X)) - 
	D_{\mathrm{kl}}\left(q^{(i)}(X), p^{(i)}(X)\right) \right) =
\sum_{i=1}^n 
	\E \left[ \E^{(i)} g(Z) - 
	g \left( \E^{(i)} Z \right) \right],&lt;/script&gt;

&lt;p&gt;which will conclude the proof. The tower property of expectation let’s us
write&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_{\mathrm{kl}}\left(q(X), p(X)\right) = \E \left[ \E^{(i)} g(Z) \right]&lt;/script&gt;

&lt;p&gt;and (with a bit more work)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
D_{\mathrm{kl}}\left(q^{(i)}(X), p^{(i)}(X)\right) &amp;=
\sum_{x^{(i)} \in \mathcal{S}^{n-1}} p^{(i)} \left( x^{(i)} \right)
	\left( \sum_{y \in \mathcal{S}} q^{(i)} \left( x^{(i)} \right) \right) 
	\log \frac{p^{(i)}\left( x^{(i)} \right) 
			\sum_{y \in \mathcal{S}} q^{(i)} \left( x^{(i)} \right)}
		{p^{(i)} \left( x^{(i)} \right)} \\
	&amp;= \sum_{x^{(i)} \in \mathcal{S}^{n-1}} p^{(i)} \left( x^{(i)} \right) 
	 	\left( \E^{(i)} Z \log \E^{(i)} Z \right) \\
	&amp;= \E \left[ g(\E^{(i)}Z) \right].
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;So, chaining everything together we have that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\E [g(Z)] - g( \E Z) &amp;= D_{\mathrm{kl}}(Q, P) \\
	&amp;\le \sum_{i=1}^n 
	\left( D_{\mathrm{kl}}(Q, P) - 
			D_{\mathrm{kl}} \left( Q^{(i)}, P^{(i)} \right) \right) \\
	&amp;= \sum_{i=1}^n \E \left[ \E^{(i)}g(Z) - g\big( \E^{(i)}Z \big) \right].
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;That’s it, that’s all folks.&lt;/p&gt;

&lt;h3 id=&quot;next-up&quot;&gt;Next up&lt;/h3&gt;

&lt;p&gt;… logarithmic Sobolev inequalities! Stay tuned—it’s gonna be good.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is based on material from Chapter 4 of (my new favorite book):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Concentration Inequalities: A Nonasymptotic Theory of Independence&lt;/em&gt; by 
S. Boucheron, G. Lugosi, and P. Masart.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">(Happy St. Patrick’s Day!)</summary></entry><entry><title type="html">Han’s inequality</title><link href="http://localhost:4000/jekyll/update/2018/03/11/han.html" rel="alternate" type="text/html" title="Han's inequality" /><published>2018-03-11T23:00:00-06:00</published><updated>2018-03-11T23:00:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/03/11/han</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/03/11/han.html">&lt;p&gt;In this post, we’re putting on our information theory hats and digging into
to a simple but useful inequality gifted to us by Te Sun Han in 1978. Han’s
inequality is a generalization of the Efron–Stein inequality with roots in
information theory rather than the statistical analysis of the jackknife.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;
Before teeing up Han’s inequality, we’ll cover a few ideas from information 
theory. Andiamo!&lt;/p&gt;

&lt;h3 id=&quot;a-modicum-of-information-theory&quot;&gt;A modicum of information theory&lt;/h3&gt;

&lt;p&gt;We’ll need the idea of (Shannon’s) &lt;strong&gt;entropy&lt;/strong&gt; in Han’s inequality, so here it
is for a discrete random variable &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(X) = \E\big( -\log p(X)\big ) = -\sum_{x \in \mathcal{S}} p(x) \log p(x),&lt;/script&gt;

&lt;p&gt;with the standard convention that &lt;script type=&quot;math/tex&quot;&gt;0 \log 0 = 0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Also, we use the definition of &lt;strong&gt;conditional entropy&lt;/strong&gt;, which is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(X | Y) = H(X, Y) - H(Y).&lt;/script&gt;

&lt;p&gt;We can write the conditional entropy as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
H(X | Y) &amp;= -\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} 
			   p(x,y) \log p(x | y) \\
	&amp;= \sum_{y \in \mathcal{Y}} p_Y(y) \left(- \sum_{x \in \mathcal{X}} 
		p(x | y) \log p(x | y) \right) \\
	&amp;= \E [-\log p(X | Y)],		
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where we use the relationship between conditional and joint probability in
the second line.&lt;/p&gt;

&lt;p&gt;Lastly, we’ll use the intuitive idea that &lt;strong&gt;conditioning decreases entropy&lt;/strong&gt;.
This result follows from a relative entropy (&lt;em&gt;i.e.&lt;/em&gt;, Kullback–Leibler)
calculation. For two random variables &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;, with marginal 
distributions &lt;script type=&quot;math/tex&quot;&gt;P_X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;P_Y&lt;/script&gt;, and joint distribution &lt;script type=&quot;math/tex&quot;&gt;P_{X,Y}&lt;/script&gt;, the
relative entropy between &lt;script type=&quot;math/tex&quot;&gt;P_{X,Y}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;P_X \otimes P_Y&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
D_{\mathrm{kl}}(P_{X,Y}, P_X \otimes P_Y) 
	&amp;= \sum_{x \in \mathcal{X}, y \in \mathcal{Y}} P_{X,Y}(x,y)
	\log \frac{P_{X,Y}(x,y)}{P_X(x)P_Y(y)} \\
 	&amp;= \sum_{x \in \mathcal{X}, y \in \mathcal{Y}} P_{X,Y}(x,y)
		\log \frac{P_{X|Y}(x|y)P_Y(y)}{P_X(x)P_Y(y)} \\
	&amp;= \sum_{x \in \mathcal{X}, y \in \mathcal{Y}} P_{X,Y}(x,y)
		\left( \log P_{X|Y}(x|y) - \log P_X(x) \right) \\
	&amp;= H(X) - H(X|Y) \ge 0.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since this is greater than 0, we have &lt;script type=&quot;math/tex&quot;&gt;H(X) \ge H(X|Y)&lt;/script&gt;, meaning that
conditioning reduces entropy.&lt;/p&gt;

&lt;h3 id=&quot;hans-inequality&quot;&gt;Han’s inequality&lt;/h3&gt;

&lt;p&gt;In its simplest form, Han’s inequality works only with discrete random 
variables. We’re going to look at that case.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Han’s inequality.&lt;/strong&gt; Assume &lt;script type=&quot;math/tex&quot;&gt;X_1,\dots,X_n&lt;/script&gt; are discrete random variables.
Then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(X_1,\dots,X_n) \le \frac{1}{n-1} 
	\sum_{i=1}^n H(X_1,\dots,X_{i-1},X_{i+1},\dots,X_n).&lt;/script&gt;

&lt;p&gt;Before proving the inequality, let’s interpret it. Roughly, it says that the 
joint entropy is less than the average entropy with the $i$th observation
removed. So, we’re better off (&lt;em&gt;i.e.&lt;/em&gt;, have less uncertainty) if we know 
everything instead of accumulating (averaging) partial information.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Han’s inequality relies on two things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;the definition of conditional entropy and&lt;/li&gt;
  &lt;li&gt;the fact that conditioning reduces entropy.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For &lt;script type=&quot;math/tex&quot;&gt;i = 1,\dots,n&lt;/script&gt;, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
H(X_1,\dots,X_n) &amp;\stackrel{\textsf{(a)}}{=} 
	H(X_1,\dots,X_{i-1},X_{i+1},\dots,X_n) + 
	H(X_i|X_1\dots,X_{i-1},X_{i+1},\dots,X_n) \\
	&amp;\stackrel{\textsf{(b)}}{\le} H(X_1,\dots,X_{i-1},X_{i+1},\dots,X_n) + 
	H(X_i|X_1\dots,X_{i-1}),
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where in (a) we used the defintion of conditional entropy and in (b) we used
the fact that conditioning reduces entropy. In other words, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
H(X_1,\dots,X_n) &amp;\le H(X_2,\dots,X_n) + H(X_1) \\
H(X_1,\dots,X_n) &amp;\le H(X_1,X_3,\dots,X_n) + H(X_2|X_1) \\
H(X_1,\dots,X_n) &amp;\le H(X_1,X_2,X_4,\dots,X_n) + H(X_3|X_1, X_2) \\
				 &amp;\,\,\, \vdots \\
H(X_1,\dots,X_n) &amp;\le H(X_1,\dots,X_{n-1}) + H(X_n|X_1\dots,X_{n-1}).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now let’s sum both sides of the inequality, which gives&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;n H(X_1,\dots,X_n) \le 
	\left( \sum_{i=1}^n H(X_1,\dots,X_{i-1},X_{i+1},\dots,X_n) \right) +
						 H(X_1,\dots,X_n),&lt;/script&gt;

&lt;p&gt;which can be rewritten as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(n-1) H(X_1,\dots,X_n) \le H(X_1,\dots,X_{i-1},X_{i+1},\dots,X_n).&lt;/script&gt;

&lt;p&gt;And that’s it!&lt;/p&gt;

&lt;h3 id=&quot;next-up&quot;&gt;Next up&lt;/h3&gt;

&lt;p&gt;… we’ll put Han’s inequality to work and look at the sub-additivity of 
entropy! Stay tuned.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is based on material from Chapter 4 of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Concentration Inequalities: A Nonasymptotic Theory of Independence&lt;/em&gt; by 
S. Boucheron, G. Lugosi, and P. Masart.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;That said, we now know and appreciate how closely related information theory and statistics really are. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">In this post, we’re putting on our information theory hats and digging into to a simple but useful inequality gifted to us by Te Sun Han in 1978. Han’s inequality is a generalization of the Efron–Stein inequality with roots in information theory rather than the statistical analysis of the jackknife.1 Before teeing up Han’s inequality, we’ll cover a few ideas from information theory. Andiamo! That said, we now know and appreciate how closely related information theory and statistics really are. &amp;#8617;</summary></entry><entry><title type="html">Rademachers are rad - part II</title><link href="http://localhost:4000/jekyll/update/2018/03/06/rademacher_2.html" rel="alternate" type="text/html" title="Rademachers are rad - part II" /><published>2018-03-06T20:00:00-07:00</published><updated>2018-03-06T20:00:00-07:00</updated><id>http://localhost:4000/jekyll/update/2018/03/06/rademacher_2</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/03/06/rademacher_2.html">&lt;p&gt;In this post, we’re going to work through another Rademacher-related problem.
Like the previous post, the problem highlights the usefulness of the 
Efron–Stein inequality.&lt;/p&gt;

&lt;h3 id=&quot;conditional-rademacher-averages&quot;&gt;Conditional Rademacher averages&lt;/h3&gt;

&lt;p&gt;In this section, we’re going to continue with a close cousin of the Rademacher
average called the &lt;strong&gt;conditional Rademacher average&lt;/strong&gt;, which are used in high 
dimensional statistics to measure the complexity of model classes. Its defined
as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z = \E \left[ \max_{j=1,\dots,d} \sum_{i=1}^n Y_i X_{i,j} \mid
	X_1,\dots,X_n \right]&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;Y_i&lt;/script&gt; are Rademacher random variables and the &lt;script type=&quot;math/tex&quot;&gt;X_{i,j}&lt;/script&gt; are 
independent random variables that live in &lt;script type=&quot;math/tex&quot;&gt;[-1,1]&lt;/script&gt;. The random variable &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt;
is a function of the &lt;script type=&quot;math/tex&quot;&gt;X_{i} \in [-1,1]^d&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Without lifting a finger, we can call our bound differences inequality friend
and bound the variance of &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Var(Z) \le n/4,&lt;/script&gt;

&lt;p&gt;but that’s less-than-satisfying at this point in time. Instead, we can improve
on this bound by showing that &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; satisfies the &lt;strong&gt;self-bounding property&lt;/strong&gt;
via the Efron–Stein inequality.&lt;/p&gt;

&lt;p&gt;First, let’s define&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z_k = \E \left[ \max_{j=1,\dots,d} 
	\sum_{\substack{i=1 \\ i \neq k}}^n Y_i X_{i,j} \mid X^{(k)}\right]&lt;/script&gt;

&lt;p&gt;and observe that &lt;script type=&quot;math/tex&quot;&gt;0 \le Z - Z_k \le 1&lt;/script&gt; because the maximum value contributed
by &lt;script type=&quot;math/tex&quot;&gt;X_k&lt;/script&gt; to the sum is 1. By summing the inequality and using a convexity
argument (see below), we get that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{k=1}^n Z - Z_k \le Z.&lt;/script&gt;

&lt;p&gt;Now, we can use the Efron–Stein inequality to say&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\Var(Z) &amp;\le \E \left[ \sum_{k=1}^n (Z - Z_k)^2 \right] \\
	&amp;\le \E \left[ \sum_{k=1}^n (Z - Z_k) \right] 
	\qquad \textsf{(because $(Z - Z_k)^2 \in [0,1]$)}\\
	&amp;\le \E Z,
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;which shows that &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; is a self-bounding function.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is based on material from Chapter 3 of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Concentration Inequalities: A Nonasymptotic Theory of Independence&lt;/em&gt; by 
S. Boucheron, G. Lugosi, and P. Masart.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;self-bounding-property&quot;&gt;Self-bounding property&lt;/h4&gt;

&lt;p&gt;Loosely speaking, the self-bounding property states that the difference 
between a function and a &lt;em&gt;similar&lt;/em&gt; function that omits one variable is bounded
between 0 and 1.&lt;/p&gt;

&lt;p&gt;Formally, a nonnegative function &lt;script type=&quot;math/tex&quot;&gt;f: \mathcal{S}^n \to [0,\infty)&lt;/script&gt; satisfies
the self-bounding property when functions &lt;script type=&quot;math/tex&quot;&gt;f_i: \mathcal{S}^{n-1} \to \reals&lt;/script&gt;
exist and the following two conditions hold:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0 \le f(x_1,\dots,x_n) - f_i(x_1,\dots,x_{i-1},x_{i+1},\dots,x_n) \le 1&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n f(x_1,\dots,x_n) - f_i(x_1,\dots,x_{i-1},x_{i+1},\dots,x_n) 
	\le f(x_1,\dots,x_n)&lt;/script&gt;

&lt;p&gt;for &lt;script type=&quot;math/tex&quot;&gt;i = 1,\dots,n&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x_1,\dots,x_n&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;If we combine the first and second conditions, we can say that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n
\big( f(x_1,\dots,x_n) - f_i(x_1,\dots,x_{i-1},x_{i+1},\dots,x_n) \big)^2 
	\le f(x_1,\dots,x_n),&lt;/script&gt;

&lt;p&gt;which means we can use the Efron–Stein inequality to bound the variance
by the expected value.&lt;/p&gt;

&lt;h4 id=&quot;convexity-argument&quot;&gt;Convexity argument&lt;/h4&gt;

&lt;p&gt;The convexity argument used in the conditional Rademacher averages example
goes as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\sum_{k=1}^n Z - Z_k &amp;= \sum_{k=1}^n 
\E \left[ \max_{j=1,\dots,d} \sum_{i=1}^n Y_i X_{i,j} 
	\, \middle| \,
	X_1,\dots,X_n \right] - \E \left[ \max_{j=1,\dots,d} 
	\sum_{i \neq k}^n Y_i X_{i,j} 
	\middle| 
	X^{(k)}\right] \\

&amp;= \sum_{k=1}^n \E \left[ \max_{j=1,\dots,d} \sum_{i=1}^n Y_i X_{i,j} -  			 	
	\max_{j=1,\dots,d} \sum_{i \neq k}^n Y_i X_{i,j} 
	\, \middle| \,
	X_1,\dots,X_n \right] \\

&amp;\le \sum_{k=1}^n \E \left[ 
	 Y_i X_{i,j^\star}
	\, \middle| \,
	X_1,\dots,X_n \right] \\

&amp;\le \E \left[ \max_{j=1,\dots,d} \sum_{i=1}^n Y_i X_{i,j} 
	\, \middle| \,
	X_1,\dots,X_n \right] \\

&amp;= Z,
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k^\star = \argmax_{k=1,\dots,d} \sum_{i = 1}^n Y_i X_{i,k}&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0 \le \max_{j=1,\dots,d} \sum_{i=1}^n Y_i X_{i,j} -  			 	
	\max_{j=1,\dots,d} \sum_{i \neq k}^n Y_i X_{i,j}
	= Y_i X_{i,j^\star} - Y_k X_{k,j^\star} \le 1.&lt;/script&gt;

&lt;h4 id=&quot;r-simulation&quot;&gt;R simulation&lt;/h4&gt;

&lt;p&gt;If you want to play around with conditional Rademacher averages, here’s some
R code to do it.&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;n_trials&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_trials&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EZ&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_trials&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# simulate X variables&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;runif&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrow&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_trials&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_trials&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rbinom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yX&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colSums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">In this post, we’re going to work through another Rademacher-related problem. Like the previous post, the problem highlights the usefulness of the Efron–Stein inequality.</summary></entry><entry><title type="html">Rademachers are rad - part I</title><link href="http://localhost:4000/jekyll/update/2018/03/04/rademacher.html" rel="alternate" type="text/html" title="Rademachers are rad - part I" /><published>2018-03-04T20:00:00-07:00</published><updated>2018-03-04T20:00:00-07:00</updated><id>http://localhost:4000/jekyll/update/2018/03/04/rademacher</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/03/04/rademacher.html">&lt;p&gt;In this post, we’re going to play with Rademacher random variables, which are
quite useful in many problems that arise in “modern” statistics and machine 
learning problems. They also pop up in empirical process theory and geometry.
So, it’s safe to say that we should at least be acquainted with them.&lt;/p&gt;

&lt;p&gt;In case Rademacher random variables are new to you, please meet the Rademacher 
random variable &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;, which takes the values &lt;script type=&quot;math/tex&quot;&gt;+1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;-1&lt;/script&gt; with probability
&lt;script type=&quot;math/tex&quot;&gt;1/2&lt;/script&gt;. For those of you who prefer a more formal introduction, please meet the
probability mass function of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f_Y(y) = 
\begin{cases}
+1, &amp; \textsf{with probability } 0.5 \\
-1, &amp; \textsf{with probability } 0.5.
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;Sometimes Rademacher random variables introduce themselves as &lt;em&gt;random sign 
variables&lt;/em&gt;. Now, that we’ve been properly introduced, let’s move on to the fun 
stuff.&lt;/p&gt;

&lt;h3 id=&quot;rademacher-averages&quot;&gt;Rademacher averages&lt;/h3&gt;

&lt;p&gt;To build a Rademacher average, we’ll need a collection of real numbers that
are indexed by the number of variables we have &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; and an index set 
&lt;script type=&quot;math/tex&quot;&gt;\mathcal{T}&lt;/script&gt;. We’ll also need some Rademacher variables &lt;script type=&quot;math/tex&quot;&gt;Y_1,\dots,Y_n&lt;/script&gt;.
The &lt;strong&gt;Rademacher average&lt;/strong&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z = \sup_{t \in \mathcal{T}} \sum_{i=1}^n a_{i,t} Y_i.&lt;/script&gt;

&lt;p&gt;The random variable &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; depends on the &lt;script type=&quot;math/tex&quot;&gt;Y_i&lt;/script&gt; in a somewhat complicated way
but we can see that &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; cannot change by much if we changed the value of a
single &lt;script type=&quot;math/tex&quot;&gt;Y_i&lt;/script&gt;. In particular, when we flip the value of &lt;script type=&quot;math/tex&quot;&gt;Y_i&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; can 
only change by &lt;script type=&quot;math/tex&quot;&gt;2\sup_{t \in \mathcal{T}} |a_{i,t}|&lt;/script&gt;. (The 2 comes from the
Rademacher variable and the supremum-absolute value combo reflects the 
“maximum” change.)&lt;/p&gt;

&lt;p&gt;We can use this &lt;strong&gt;bounded differences property&lt;/strong&gt; and the Efron–Stein inequality
to bound the &lt;script type=&quot;math/tex&quot;&gt;\Var(Z)&lt;/script&gt; even though we can’t say much about the behavior of 
&lt;script type=&quot;math/tex&quot;&gt;\E Z&lt;/script&gt;, which is pretty rad.&lt;/p&gt;

&lt;p&gt;In particular, we can say that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Var(Z) \le \sum_{i=1}^n \frac{1}{4} \left( 
		 	2\sup_{t \in \mathcal{T}} |a_{i,t}| \right)^2 =
		 	\sum_{i=1}^n \sup_{t \in \mathcal{T}} a_{i,t}^2.&lt;/script&gt;

&lt;p&gt;The inequality results from the bounded differences inequality (which we can
prove using the Efron–Stein inequality).&lt;/p&gt;

&lt;p&gt;But… we can do better.&lt;/p&gt;

&lt;h4 id=&quot;sneaking-the-supremum-past-the-sum&quot;&gt;Sneaking the supremum past the sum&lt;/h4&gt;

&lt;p&gt;Now, we define &lt;script type=&quot;math/tex&quot;&gt;X_1,\dots,X_n&lt;/script&gt; as an independent copy of &lt;script type=&quot;math/tex&quot;&gt;Y_1,\dots,Y_n&lt;/script&gt;.
We’ll also set &lt;script type=&quot;math/tex&quot;&gt;t^\star&lt;/script&gt; to be the (possibly random) index that achieves
the supremum, &lt;em&gt;i.e.&lt;/em&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sup_{t \in \mathcal{T}} \sum_{i=1}^n a_{i,t} Y_i 
	= \sum_{i=1}^n a_{i,t^\star} Y_i.&lt;/script&gt;

&lt;p&gt;So, we can bound the difference between &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; with the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th
coordinate changed from &lt;script type=&quot;math/tex&quot;&gt;Y_i&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt;. That is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z - Z_i \le (Y_i - X_i) a_{i,t^\star}
\implies
(Z - Z_i)_+^2 \le (Y_i - X_i)^2 a_{i,t^\star}^2.&lt;/script&gt;

&lt;p&gt;where in the implied inequality we square only the positive portion of
&lt;script type=&quot;math/tex&quot;&gt;Z - Z_i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Now, we’ll use an expectation trick and the independence of &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;
to say&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E \left[ (Z - Z_i)_+^2 \right] \le 
	\E \left[ \E \left( 
		(Y_i - X_i)^2 a_{i,t^\star}^2 \mid Y_1,\dots,Y_n)
	\right) \right] =
	2 \E \left[ a_{i,t^\star}^2 \right].&lt;/script&gt;

&lt;p&gt;We do the expectation-conditioning combo on &lt;script type=&quot;math/tex&quot;&gt;Y_1,\dots,Y_n&lt;/script&gt; to get rid of all
randomness expect for that of &lt;script type=&quot;math/tex&quot;&gt;t^\star&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Finally, we use the Efron–Stein inequality to say&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Var(Z) \le 2 \E \left( \sum_{i=1}^n a_{i,t^\star}^2 \right) \le 
	2 \sup_{t \in \mathcal{T}} \sum_{i=1}^n a_{i,t}^2.&lt;/script&gt;

&lt;p&gt;So, through some Efron–Stein inspired analysis, we moved the supremum outside
the sum at the expense of a factor of 2.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is based on material from Chapter 3 of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Concentration Inequalities: A Nonasymptotic Theory of Independence&lt;/em&gt; by 
S. Boucheron, G. Lugosi, and P. Masart.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;bounded-differences-property&quot;&gt;Bounded differences property&lt;/h4&gt;

&lt;p&gt;The bounded differences property states that the value of the function changes
by a bounded amount if the value of the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th variable is changed. A function
&lt;script type=&quot;math/tex&quot;&gt;f: \mathcal{S}^n \to \reals&lt;/script&gt; satisfies the bounded differences property if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sup_{\substack{x_1,\dots,x_n \\ x_i' \in \mathcal{S}}}
	| f(x_1,\dots,x_n) - f_i(x_1,\dots,x_{i-1},x_i',x_{i+1},\dots,x_n) |
	\le c_i,&lt;/script&gt;

&lt;p&gt;for &lt;script type=&quot;math/tex&quot;&gt;i = 1,\dots,n&lt;/script&gt; and constants &lt;script type=&quot;math/tex&quot;&gt;c_1,\dots,c_n&lt;/script&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">In this post, we’re going to play with Rademacher random variables, which are quite useful in many problems that arise in “modern” statistics and machine learning problems. They also pop up in empirical process theory and geometry. So, it’s safe to say that we should at least be acquainted with them.</summary></entry><entry><title type="html">Better bounds: moment vs. Chernoff</title><link href="http://localhost:4000/jekyll/update/2018/02/28/moment-v-chernoff.html" rel="alternate" type="text/html" title="Better bounds: moment vs. Chernoff" /><published>2018-02-28T20:00:00-07:00</published><updated>2018-02-28T20:00:00-07:00</updated><id>http://localhost:4000/jekyll/update/2018/02/28/moment-v-chernoff</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/02/28/moment-v-chernoff.html">&lt;p&gt;In this post, we’re going to work through a problem from Chapter 2 of
&lt;em&gt;Concentration Inequalities: A Nonasymptotic Theory of Independence&lt;/em&gt;. The exercise 
shows that moment bounds can be tighter than Cramér–Chernoff-type bounds.&lt;/p&gt;

&lt;h3 id=&quot;the-problem&quot;&gt;The problem&lt;/h3&gt;

&lt;p&gt;Our task to show that moment bounds are &lt;strong&gt;always&lt;/strong&gt; better than Cramér–Chernoff bounds.
Assume we have a nonnegative random variable &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; and a positive number &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;. The best
moment bound is &lt;script type=&quot;math/tex&quot;&gt;\prob(Y \ge t) = \min_{q \in \integers} \E(Y^q)t^{-q}&lt;/script&gt;. On the other
hand, the best Cramér–Chernoff bound is &lt;script type=&quot;math/tex&quot;&gt;\inf_{\lambda &gt; 0} \E e^{\lambda(Y-t)}&lt;/script&gt;. So,
our job is to show that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{q \in \integers} \E(Y^q)t^{-q} \le \inf_{\lambda &gt; 0} \E e^{\lambda(Y-t)}.&lt;/script&gt;

&lt;h3 id=&quot;the-proof&quot;&gt;The proof&lt;/h3&gt;

&lt;p&gt;We’re going to write the &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt;-th moment of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;m^q&lt;/script&gt;. Now, we’ll write the 
moment generating function of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; as a series&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E e^{\lambda Y} = \sum_{k=1}^\infty \frac{\lambda^k m^k}{k!},&lt;/script&gt;

&lt;p&gt;which allows us to write the shifted moment generating function as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E e^{\lambda(Y-t)} = \left. \sum_{k=1}^\infty \frac{\lambda^k m^k}{k!} 
						 \middle/
						\sum_{k=1}^\infty \frac{\lambda^k t^k}{k!}  \right.&lt;/script&gt;

&lt;p&gt;By assumption, &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; is the integer that minimizes &lt;script type=&quot;math/tex&quot;&gt;\E(Y^q)t^{-q}&lt;/script&gt;, which means
that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E(Y^q)t^{-q} = \frac{m^q}{t^q}
	\le \left. \frac{\lambda^k m^k}{k!} \middle/ \frac{\lambda^k t^k}{k!} \right.
	= \frac{m^k}{t^k!} \qquad \textsf{for } t \in \reals_{++} \text{ and } k \in \integers_+.&lt;/script&gt;

&lt;p&gt;At this point, we’ve got the result because this holds for all &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;, in particular, 
when &lt;script type=&quot;math/tex&quot;&gt;k = 0&lt;/script&gt;. So, when we sum up the right-hand-side of the inequality 
(from &lt;script type=&quot;math/tex&quot;&gt;k=0&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;\infty&lt;/script&gt;) and then take the infimum over &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;, we’re still 
larger than the (optimal) moment bound.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tips and tricks.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For the analysts out there, there’s a nice result on ratios of series that provides
(precise) justification for the result. Assume that &lt;script type=&quot;math/tex&quot;&gt;\sum_{k=0}^\infty x_k&lt;/script&gt; and 
&lt;script type=&quot;math/tex&quot;&gt;\sum_{k=0}^\infty z_k&lt;/script&gt; are finite and that &lt;script type=&quot;math/tex&quot;&gt;c \le x_k / z_k&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;. Then
it can be shown that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
c \le \frac{\sum_{k=0}^\infty x_k}{\sum_{k=0}^\infty z_k}
	\quad \text{and} \quad
c &lt; \frac{\sum_{k=0}^\infty x_k}{\sum_{k=0}^\infty z_k} %]]&gt;&lt;/script&gt;

&lt;p&gt;when &lt;script type=&quot;math/tex&quot;&gt;x_i / z_i \neq x_j / z_j&lt;/script&gt; for at least one &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; pair.&lt;/p&gt;

&lt;p&gt;We can use this result by setting&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;c = \min_{q \in \integers} \E(Y^q)t^{-q}, \qquad
	x_k = \frac{\lambda^k m^k}{k!}, \qquad 
	z_k = \frac{\lambda^k t^k}{k!},&lt;/script&gt;

&lt;p&gt;and noting that the ratio &lt;script type=&quot;math/tex&quot;&gt;x_k/z_k&lt;/script&gt; changes for different values of &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;final-thoughts&quot;&gt;Final thoughts&lt;/h3&gt;

&lt;p&gt;Although moment bounds can be much tighter than Cramér–Chernoff bounds, working with 
the latter is often (much, much) easier. For example, Cramér–Chernoff bounds play 
nicely with sums of random variables.&lt;/p&gt;

&lt;p&gt;Interestingly, Markov’s inequality and Chebyshev’s inequality, &lt;em&gt;i.e.&lt;/em&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\prob(Y \ge t) \le \frac{\E Y}{t} \quad \text{and} \quad
\prob(|Y - \E Y| \ge t) \le \frac{\Var(Y)}{t^2}&lt;/script&gt;

&lt;p&gt;are often not as sharp as Cramér–Chernoff bounds.&lt;/p&gt;

&lt;p&gt;For example, let &lt;script type=&quot;math/tex&quot;&gt;Y \sim \text{Poisson}(\beta)&lt;/script&gt;. The mean of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is 
&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; and the variance is &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;. The moment generating function of the centered
version of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;, &lt;em&gt;i.e.&lt;/em&gt;, &lt;script type=&quot;math/tex&quot;&gt;Z = Y - \beta&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E e^{\lambda Y} = e^{-\lambda \beta - \beta} e^{\beta e^\lambda}.&lt;/script&gt;

&lt;p&gt;The optimal value of &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; which minimizes this expression is 
&lt;script type=&quot;math/tex&quot;&gt;\lambda = \log(1 + t/\beta)&lt;/script&gt;. So, we have that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\inf_{\lambda &gt; 0} \E e^{\lambda(Y-t)} = e^t(1+t/\beta)^{-(\beta+t)}.&lt;/script&gt;

&lt;p&gt;Say &lt;script type=&quot;math/tex&quot;&gt;\beta = 100&lt;/script&gt; and we set &lt;script type=&quot;math/tex&quot;&gt;t = 20&lt;/script&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Markov: 100/20 = 5, which is useless;&lt;/li&gt;
  &lt;li&gt;Chebyshev: 100/400 = 0.25;&lt;/li&gt;
  &lt;li&gt;Cramér–Chernoff: see formula above = 0.153;&lt;/li&gt;
  &lt;li&gt;actual: 0.023.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now turn up &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; to 50.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Markov: 100/50 = 2, which is still useless;&lt;/li&gt;
  &lt;li&gt;Chebyshev: 100/2500 = 0.004;&lt;/li&gt;
  &lt;li&gt;Cramér–Chernoff: &lt;script type=&quot;math/tex&quot;&gt;2\times10^{-5}&lt;/script&gt;;&lt;/li&gt;
  &lt;li&gt;actual: &lt;script type=&quot;math/tex&quot;&gt;1.23\times10^{-6}&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is based on material from:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Concentration Inequalities: A Nonasymptotic Theory of Independence&lt;/em&gt; by 
S. Boucheron, G. Lugosi, and P. Masart.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">In this post, we’re going to work through a problem from Chapter 2 of Concentration Inequalities: A Nonasymptotic Theory of Independence. The exercise shows that moment bounds can be tighter than Cramér–Chernoff-type bounds.</summary></entry><entry><title type="html">A Poincaré inequality for the Laplace distribution</title><link href="http://localhost:4000/jekyll/update/2018/02/24/poincare-inequality.html" rel="alternate" type="text/html" title="A Poincaré inequality for the Laplace distribution" /><published>2018-02-24T13:30:00-07:00</published><updated>2018-02-24T13:30:00-07:00</updated><id>http://localhost:4000/jekyll/update/2018/02/24/poincare-inequality</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/02/24/poincare-inequality.html">&lt;p&gt;Lately, I’ve been reading and working through Boucheron, Lugosi, and Massart’s
&lt;em&gt;Concentration Inequalities: A Nonasymptotic Theory of Independence&lt;/em&gt;. The book is (very)
well written, self-contained, and full of lots of useful tools (&lt;em&gt;i.e.&lt;/em&gt;, theorems, lemmas,
corollaries, and the like), as well as many thought provoking examples and exercises.&lt;/p&gt;

&lt;p&gt;In this post, we’re going to work through a problem from Chapter 3, which is titled
&lt;em&gt;Bounding the Variance&lt;/em&gt;, that proves a Poincaré-type inequality.&lt;/p&gt;

&lt;h3 id=&quot;the-problem&quot;&gt;The problem&lt;/h3&gt;

&lt;p&gt;Suppose we have a random variable &lt;script type=&quot;math/tex&quot;&gt;X \sim \mathrm{Laplace}(\lambda = 1)&lt;/script&gt;, with a density 
given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g(x) = \frac{1}{2}\exp(-|x|), \qquad x \in \reals.&lt;/script&gt;

&lt;p&gt;Assume that we have a differentiable function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; that has finite variance, &lt;em&gt;i.e.&lt;/em&gt;,
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\Var(f(X)) &lt; \infty %]]&gt;&lt;/script&gt;. We’re going to prove that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\Var(f(X)) &lt; 4 \E \left[ (f'(X))^2 \right], %]]&gt;&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;f'&lt;/script&gt; is the derivative of &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;the-proof&quot;&gt;The proof&lt;/h3&gt;

&lt;p&gt;First, take a moment to appreciate what this result says: the variance of the function
of a relatively heavy-tailed distribution is controlled by its expected value; in other
words, the tails of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; &lt;strong&gt;don’t&lt;/strong&gt; lead to wild behavior of &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;. That’s cool!&lt;/p&gt;

&lt;p&gt;We’ll use the following (&lt;em&gt;integration by parts&lt;/em&gt;) fact in the proof:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E \big( h(X) \big) = h(0) + E \big( \mathrm{sgn}(X) h'(X) \big),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\mathrm{sgn}&lt;/script&gt; is the function that returns -1 when its argument is negative and
+1 when its argument is positive.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\Var(f(X)) &amp;\stackrel{\textsf{(a)}}{=} \Var(f(X) - f(0)) \\
	&amp;\stackrel{\textsf{def}}{=} \Var(h(X)) \\
	&amp;\stackrel{\textsf{(b)}}{\le} \E \left[ \big( h(X) \big)^2 \right] \\
	&amp;\stackrel{\textsf{(c)}}{=} 2 \E \left[ \mathrm{sgn}(X) h'(X) h(X) \right] \\
	&amp;\stackrel{\textsf{(d)}}{\le} 2 \left(\E \left[\big(h'(X)\big)^2\right]\right)^{1/2}
								  \left(\E \left[\big(h(X)\big)^2\right]\right)^{1/2}. \\	
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now we compare the third and fifth lines and note that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\E \left[ \big( h(X) \big)^2 \right] &amp;\le 
	2 \left(\E \left[\big(h'(X)\big)^2\right]\right)^{1/2}
	  \left(\E \left[\big(h(X)\big)^2\right]\right)^{1/2} \\
\left(\E \left[ \big( h(X) \big)^2 \right]\right)^{1/2} &amp;\le 
	2 \left(\E \left[\big(h'(X)\big)^2\right]\right)^{1/2} \\
\E \left[ \big( h(X) \big)^2 \right] &amp;\le 
	4 \E \left[\big(h'(X)\big)^2\right].
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Finally, we chain everything together and we complete the proof:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Var(f(X)) \le 4 \E \left[\big(h'(X)\big)^2\right].&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Tips and tricks.&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In &lt;strong&gt;(a)&lt;/strong&gt; we used that the variance of a random variable is not changed when a constant
is added to the random variable, &lt;em&gt;i.e.&lt;/em&gt;, when we translate it.&lt;/li&gt;
  &lt;li&gt;In &lt;strong&gt;(b)&lt;/strong&gt; we used that &lt;script type=&quot;math/tex&quot;&gt;\Var(Y) + (\E Y)^2 = \E(Y^2)&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;In &lt;strong&gt;(c)&lt;/strong&gt; we used the &lt;em&gt;integration by parts&lt;/em&gt; fact.&lt;/li&gt;
  &lt;li&gt;In &lt;strong&gt;(d)&lt;/strong&gt; we used the Cauchy-Schwarz inequality.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is based on material from:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Concentration Inequalities: A Nonasymptotic Theory of Independence&lt;/em&gt; by 
S. Boucheron, G. Lugosi, and P. Masart.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Lately, I’ve been reading and working through Boucheron, Lugosi, and Massart’s Concentration Inequalities: A Nonasymptotic Theory of Independence. The book is (very) well written, self-contained, and full of lots of useful tools (i.e., theorems, lemmas, corollaries, and the like), as well as many thought provoking examples and exercises.</summary></entry><entry><title type="html">Idea chaining</title><link href="http://localhost:4000/jekyll/update/2018/02/11/idea-chaining.html" rel="alternate" type="text/html" title="Idea chaining" /><published>2018-02-11T20:40:00-07:00</published><updated>2018-02-11T20:40:00-07:00</updated><id>http://localhost:4000/jekyll/update/2018/02/11/idea-chaining</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/02/11/idea-chaining.html">&lt;p&gt;In this post, we’re going to talk about a concept that is useful in many areas…
statistics, optimization, life-in-general… I call it &lt;strong&gt;idea chaining&lt;/strong&gt;. My guess is that
most people are already aware of this notion on an intuitive level. Systems thinkers would
probably call it something like “problem decomposition”, mathematicians would think of
lemmas… basically we take a problem, split it into smaller, easier-to-solve 
sub-problems, and then chain our results together to solve the original problem.&lt;/p&gt;

&lt;h3 id=&quot;a-convex-calculus-for-convexity-verification&quot;&gt;A (convex) calculus for convexity verification&lt;/h3&gt;

&lt;p&gt;In convex analysis and optimization, we’re often interested in whether or not a given
function is convex. There are a few ways to go about showing the “vexity” of a 
particular function, such as using:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the definition of convexity;&lt;/li&gt;
  &lt;li&gt;the restriction-to-a-line technique;&lt;/li&gt;
  &lt;li&gt;first-order (&lt;em&gt;i.e.&lt;/em&gt;, derivative) conditions;&lt;/li&gt;
  &lt;li&gt;second-order conditions; or&lt;/li&gt;
  &lt;li&gt;a convex calculus.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;strong&gt;convex calculus&lt;/strong&gt; approach is appealing because it gives us a toolbox to 
(mindlessly) construct or decompose a function, using a set of basic functions and a few 
rules that preserve convexity. In the context of convex optimization, this 
&lt;strong&gt;constructive convex verification process&lt;/strong&gt; 
is part of an approach known as &lt;strong&gt;disciplined convex 
programming&lt;/strong&gt; (DCP). The DCP ideas arose from the work of Michael Grant and 
Stephen Boyd in the mid-2000s. Hiriart-Urruty and Lemaréchal discuss the notion of a 
convex calculus in their books from the 1990s.&lt;/p&gt;

&lt;p&gt;Let’s put the ideas to work on an example that came up on the quiz page of the 
&lt;a href=&quot;http://dcp.stanford.edu/&quot;&gt;DCP&lt;/a&gt; website (which teaches the principles of disciplined convex programming).&lt;/p&gt;

&lt;p&gt;Our function of interest is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(u,w,y,z) = \log \left( \exp \left\{ e^{\max(u, z)} \right\} + 
						 \exp \left\{ g_{\mathrm{hub}}(w) \right\} +
						 \exp \left\{ y - 42 \right\}  \right),&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
g_{\mathrm{hub}}(w) = 
\begin{cases}
	2|x| - 1, &amp; |x| \ge 1 \\
	|x|^2, 	  &amp; |x| &lt; 1.
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;At first sight, this looks ugly because we have a concave function with &lt;script type=&quot;math/tex&quot;&gt;\log&lt;/script&gt;, a couple
convex functions with &lt;script type=&quot;math/tex&quot;&gt;\max&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\exp&lt;/script&gt;, an affine function &lt;script type=&quot;math/tex&quot;&gt;y - 42&lt;/script&gt;, and that Huber
function &lt;script type=&quot;math/tex&quot;&gt;g_{\mathrm{hub}}&lt;/script&gt; (which is actually convex, but may not be recognized as 
such). Moreover, we have four variables!&lt;/p&gt;

&lt;p&gt;The DCP / convex calculus approach turns this problem into an easy verification process.
The non-obvious and key ingredient is the “log-sum-exp” function, also called the soft-max
function. For &lt;script type=&quot;math/tex&quot;&gt;x \in \reals^n&lt;/script&gt;, the log-sum-exp function is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(x) = \log \left( \sum_{i = 1}^n \exp ( x_i ) \right).&lt;/script&gt;

&lt;p&gt;This function is convex—if you’re suspicious, calculate its Hessian and 
convince yourself. With this single fact, we’re nearly done because the functions 
appearing within the exponentials are convex and affine. Then we use that log-sum-exp is 
convex and we’re done.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tips and tricks.&lt;/strong&gt; Here are the DCP rules we used:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;an increasing convex function of a convex function is convex; and&lt;/li&gt;
  &lt;li&gt;a sum of convex functions is convex.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, with two rules and a few basic functions we’ve show a rather complicated function to 
be convex—no gradients, subgradients, or Hessians required. We reduced the problem
to a few simple subproblems (&lt;em&gt;e.g.&lt;/em&gt;, verifying the convexity of 
&lt;script type=&quot;math/tex&quot;&gt;e^{g_{\mathrm{hub}}(w)}&lt;/script&gt; and the like) and then strung everything together using a
couple rules. Bam.&lt;/p&gt;

&lt;p&gt;If you’re interested in seeing the “vexity” parse tree for this function, check out the 
DCP site’s “&lt;a href=&quot;http://dcp.stanford.edu/analyzer&quot;&gt;Analyzer&lt;/a&gt;” and type in 
&lt;em&gt;log_sum_exp(exp(max(u, z)), huber(w) + y - 42)&lt;/em&gt;. Do it!&lt;/p&gt;

&lt;h3 id=&quot;le-cams-inequality&quot;&gt;Le Cam’s inequality&lt;/h3&gt;

&lt;p&gt;Our next example of idea chaining stems from a problem from Tom Ferguson’s (excellent) 
mathematical statistics book called &lt;em&gt;A Course in Large Sample Theory&lt;/em&gt;. Problem 5 of 
chapter 3 (“Convergence in Law”) walks us through Le Cam’s inequality.&lt;/p&gt;

&lt;p&gt;Here’s the set up.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We have &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; independent Bernoulli random variables each with its own probability of
success, &lt;script type=&quot;math/tex&quot;&gt;p_i = \prob(X_i = 1)&lt;/script&gt;. We’ll call these &lt;script type=&quot;math/tex&quot;&gt;X_1,\dots,X_n&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;We define &lt;script type=&quot;math/tex&quot;&gt;S_n = \sum_{i=1}^n X_i&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;We define &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; as a Poisson random variable with parameter 
&lt;script type=&quot;math/tex&quot;&gt;\lambda = \sum_{i=1}^n p_i&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We want to show that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;| \prob(S_n \in A) - \prob(Z \in A)| \le \sum_{i=1}^n p_i^2&lt;/script&gt;

&lt;p&gt;for all sets &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;. In other words, we’re computing a bound on the error of using a 
Poisson approximation.&lt;/p&gt;

&lt;p&gt;We’ll solve this problem by solving three simpler sub-problems.&lt;/p&gt;

&lt;p&gt;(1) First, we’ll show that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;| \prob(S_n \in A) - \prob(Z \in A)| \le \prob(S_n \neq Z).&lt;/script&gt;

&lt;p&gt;(2) Then, we’ll show that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\prob(S_n \neq Z) \le \sum_{i=1}^n \prob(X_i \neq Y_i).&lt;/script&gt;

&lt;p&gt;(3) Finally, we’ll show that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\prob(X_i \neq Y_i) \le \sum_{i=1}^n p_i^2.&lt;/script&gt;

&lt;p&gt;To show (1), (2), and (3), we’re going to couple &lt;script type=&quot;math/tex&quot;&gt;S_n&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; to the same underlying 
probability space.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The variables &lt;script type=&quot;math/tex&quot;&gt;U_1,\dots,U_n&lt;/script&gt; are uniformly distributed on &lt;script type=&quot;math/tex&quot;&gt;[0,1]&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;We can define &lt;script type=&quot;math/tex&quot;&gt;X_i = \ind{}(U_i &gt; 1 - p_i)&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;\E X_i = p_i&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;We introduce &lt;script type=&quot;math/tex&quot;&gt;Y \sim \mathrm{Poisson}(p_i)&lt;/script&gt; as&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
Y_i =
\begin{cases}
0, &amp; U_i &lt; e^{-p_i} \\ 
k, &amp; F(k-1) &lt; U_i &lt; F(k),
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;F(k) = e^{-p_i} \sum_{j=0}^{k} p_i^j/j!&lt;/script&gt; is the cumulative distribution function
for a Poisson random variable with parameter &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof of (1).&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Without loss of generality we can assume that &lt;script type=&quot;math/tex&quot;&gt;\prob(S_n \in A) \ge \prob(Z \in A)&lt;/script&gt;. 
This allows us to conclude that &lt;script type=&quot;math/tex&quot;&gt;\{S_n \in A\} \supseteq \{Z \in A\}&lt;/script&gt;. Now, we isolate
the set where &lt;script type=&quot;math/tex&quot;&gt;\{S_n \neq Z\}&lt;/script&gt; by noting that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\prob(S_n \in A) - \prob(Z \in A) \le \prob(S_n \in A) - \prob(S_n \cap Z \in A) 
	= \prob(S_n \neq Z).&lt;/script&gt;

&lt;p&gt;One down, two to go.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof of (2).&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;First, let’s rewrite &lt;script type=&quot;math/tex&quot;&gt;\prob(S_n \neq Z)&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;\prob \left( \sum_i X_i \neq Z \right)&lt;/script&gt;.
This implies that there’s at least one &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; not equal to &lt;script type=&quot;math/tex&quot;&gt;Y_i&lt;/script&gt;. (The &lt;em&gt;at least&lt;/em&gt; 
phrase should be triggering the part of your brain associated with &lt;em&gt;unions&lt;/em&gt;.) 
So, we have that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\prob \left( \bigcup_{i=1}^n \{X_i \neq Y_i\} \right) \le \sum_{i=1}^n \prob(X_i \neq Y_i)&lt;/script&gt;

&lt;p&gt;by the subadditivity of probability.&lt;/p&gt;

&lt;p&gt;Two down, one to go.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof of (3).&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Using our definitions of &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y_i&lt;/script&gt;, we can write&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\prob(X_i \neq Y_i) &amp;= 1 - \prob(X_i = 0) - \prob(Y_i = 1) \\
	&amp;= 1 - (1 - p_i) - p_i e^{-p_i} \\
	&amp;= p_i - p_i e^{-p_i} \qquad \qquad \textsf{(because $(1-e^{-p_i}) \le p_i$)}\\
	&amp;\le p_i^2.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now, we sum both sides of the equation and we’ve got it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bringing it all back home.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Finally, chain the inequalities in (1), (2), and (3) together and we’ve proved Le Cam’s
inequality. Heck yeah!&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post draws on material from:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Stephen Boyd’s “&lt;a href=&quot;https://web.stanford.edu/~boyd/papers/cvx_short_course.html&quot;&gt;Convex Optimization Short Course&lt;/a&gt;” notes;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Fundamentals of Convex Analysis&lt;/em&gt; by Hiriart-Urruty and Lemaréchal; and&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;A Course in Large Sample Theory&lt;/em&gt; by Thomas Ferguson.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">In this post, we’re going to talk about a concept that is useful in many areas… statistics, optimization, life-in-general… I call it idea chaining. My guess is that most people are already aware of this notion on an intuitive level. Systems thinkers would probably call it something like “problem decomposition”, mathematicians would think of lemmas… basically we take a problem, split it into smaller, easier-to-solve sub-problems, and then chain our results together to solve the original problem.</summary></entry><entry><title type="html">A taste of decision theory</title><link href="http://localhost:4000/jekyll/update/2018/02/03/a-taste-of-decision-theory.html" rel="alternate" type="text/html" title="A taste of decision theory" /><published>2018-02-03T13:00:00-07:00</published><updated>2018-02-03T13:00:00-07:00</updated><id>http://localhost:4000/jekyll/update/2018/02/03/a-taste-of-decision-theory</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/02/03/a-taste-of-decision-theory.html">&lt;p&gt;In this post, we’re going to talk about statistical decision theory, which is a topic
with its origins in the 1950s. Decision theory is cool because it gives us a way to
compare statistical procedures. Moreover, it connects frequentist and Bayesian inference.&lt;/p&gt;

&lt;p&gt;This post sets us on the path towards minimax rules and estimators. 
&lt;em&gt;Caveat:&lt;/em&gt; In no way is this an attempt to fully cover this topic—it’s huge and 
philosophically deep.&lt;/p&gt;

&lt;h3 id=&quot;a-bit-of-background&quot;&gt;A bit of background&lt;/h3&gt;

&lt;p&gt;Let’s say we have a parameter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; that lives in some parameter space 
&lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt;. We’re going to estimate our parameter of interest &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; with 
&lt;script type=&quot;math/tex&quot;&gt;\hat{\theta}&lt;/script&gt;. Since we’re in the business of comparing estimators, we need a way
to assess their quality. We’ll do this using &lt;strong&gt;loss functions&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;A loss function measures the discrepancy between &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\hat{\theta}&lt;/script&gt;. 
Formally,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L: \Theta \times \Theta \to \reals.&lt;/script&gt;

&lt;p&gt;Some common loss functions are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;squared-error loss: &lt;script type=&quot;math/tex&quot;&gt;L(\theta, \hat{\theta}) = \|\theta - \hat{\theta}\|^2_2&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;absolute-error: &lt;script type=&quot;math/tex&quot;&gt;L(\theta, \hat{\theta}) = \| \theta - \hat{\theta} \|_1&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Kullback–Leibler: &lt;script type=&quot;math/tex&quot;&gt;L(\theta, \hat{\theta}) = \int \log 
                     \frac{f(x;\theta)}{f(x;\hat{\theta})} f(x;\theta) \, dx&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now we have some tools to assess the quality of our estimators, but we still have some 
work to do because our estimators (most likely) depend on random variables. 
To remove the randomness, we’ll take an average which will give us the &lt;strong&gt;risk&lt;/strong&gt;
of our estimator. The risk of &lt;script type=&quot;math/tex&quot;&gt;\hat{\theta}&lt;/script&gt; is given&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R(\theta, \hat{\theta}) = \E_\theta \big( L(\theta, \hat{\theta}) \big) 
	= \int L(\theta, \hat{\theta}) f(x; \theta) \, dx&lt;/script&gt;

&lt;p&gt;where the expectation is over the data. (The &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; subscript on the 
expectation is just a way for us to index our distribution—in other words, we can read
it as “the expectation when the parameter is &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.”)
The risk quantifies how good or bad our estimator is, where “low risk”
is good and “high risk” is bad.&lt;/p&gt;

&lt;p&gt;If we took an additional expectation across the parameter (&lt;em&gt;your Bayesian neurons should
be lighting up&lt;/em&gt;), then we have the &lt;strong&gt;Bayes risk&lt;/strong&gt; of the estimator:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_\mathrm{Bayes}(g,\hat{\theta}) = \int R(\theta, \hat{\theta}) g(\theta) \, d\theta
	= \int \left( \int L(\theta, \hat{\theta}) f(x; \theta) \, dx \right) g(\theta) \, d\theta&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;g(\theta)&lt;/script&gt; is a prior distribution for &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. But don’t get too attached to
the name because this is an expectation over the data (frequentist) and over the 
parameter (Bayesian).&lt;/p&gt;

&lt;p&gt;A true Bayesian statistician would be more interested in the &lt;strong&gt;posterior risk&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_\mathrm{post}(\hat{\theta} \mid x) = \int L(\theta, \hat{\theta}) g(\theta \mid x) \, dx&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;g(\theta \mid x)&lt;/script&gt; is the posterior density of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;So, now we have three notions of risk at our disposal:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the (regular) &lt;strong&gt;risk&lt;/strong&gt;, which has a frequentist feel;&lt;/li&gt;
  &lt;li&gt;the &lt;strong&gt;posterior risk&lt;/strong&gt;, which has a Bayesian feel; and&lt;/li&gt;
  &lt;li&gt;the &lt;strong&gt;Bayes risk&lt;/strong&gt;, which has an independent-of-statistical-philosophy feel.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Neural connection!&lt;/strong&gt;
When we use the squared-error loss (also known as the &lt;em&gt;&lt;script type=&quot;math/tex&quot;&gt;\ell_2&lt;/script&gt;-loss&lt;/em&gt; or
the &lt;em&gt;mean squared error&lt;/em&gt;), we can write the risk using bias and variance of our estimator
&lt;script type=&quot;math/tex&quot;&gt;\hat{\theta}&lt;/script&gt;. We’ll use the “add-and-subtract” trick to show this relationship.
Let &lt;script type=&quot;math/tex&quot;&gt;\mu = \E_\theta (\hat{\theta})&lt;/script&gt;, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\E_\theta (\theta - \hat{\theta})^2 
	&amp;= \E_\theta (\theta - \mu + \mu - \hat{\theta})^2 \\
	&amp;= \E_\theta(\mu - \hat{\theta})^2 + (\theta - \mu)^2 +
		2 \E_\theta(\mu - \hat{\theta})(\theta - \mu) \\
	&amp;= \Var_\theta (\hat{\theta}) + \big( \E_\theta (\hat{\theta} - \mu) \big)^2.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;a-warm-up-calculation&quot;&gt;A warm-up calculation&lt;/h3&gt;

&lt;p&gt;Let’s assume we have &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; data points, &lt;script type=&quot;math/tex&quot;&gt;X_1&lt;/script&gt;,…,&lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; sampled from Bernoulli 
distribution with an unknown probability of success &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;. We’ll estimate &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; with 
two different estimators&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}_1 = \bar{X}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}_2 = 0.5&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;and compare their risk functions, using the squared-error loss function.&lt;/p&gt;

&lt;p&gt;The estimator &lt;script type=&quot;math/tex&quot;&gt;\hat{p}_1 = \bar{X}&lt;/script&gt; is an unbiased estimator of the true parameter 
&lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;. So, the risk of &lt;script type=&quot;math/tex&quot;&gt;\hat{p}_1&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R(p, \hat{p}_1) = \E_p (p - \bar{X})^2 
	= \Var_p (\bar{X}) 
	= \frac{p(1-p)}{n}.&lt;/script&gt;

&lt;p&gt;The estimator &lt;script type=&quot;math/tex&quot;&gt;\hat{p}_2 = 0.5&lt;/script&gt; has a bias but no variance term. Its risk is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R(p, \hat{p}_2) = \E_p (p - 0.5)^2 
	= (E(0.5) - p)^2 
	= (0.5 - p)^2.&lt;/script&gt;

&lt;p&gt;The risk of the first estimator is a concave quadratic and the risk of the second 
estimator is a convex quadratic.
So, neither one of these risk functions is strictly better (&lt;em&gt;i.e.&lt;/em&gt;, uniformly dominates)
the other one as &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; ranges from 0 to 1.&lt;/p&gt;

&lt;h3 id=&quot;maximum-risk&quot;&gt;Maximum risk&lt;/h3&gt;

&lt;p&gt;In the spirit of (working towards) minimax, we could ask: what value of &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; maximizes
the risk of our estimator?&lt;/p&gt;

&lt;p&gt;For &lt;script type=&quot;math/tex&quot;&gt;\hat{p}_1&lt;/script&gt;, we can expand the quadratic &lt;script type=&quot;math/tex&quot;&gt;(p - p^2)/n&lt;/script&gt;, take derivatives
&lt;script type=&quot;math/tex&quot;&gt;(1 - 2p)/n&lt;/script&gt;, set it equal to 0, and find that &lt;script type=&quot;math/tex&quot;&gt;p = 1/2&lt;/script&gt; maximizes the risk. (Note
that this results holds independent of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;; however, the value of the risk function
decreases at a rate of &lt;script type=&quot;math/tex&quot;&gt;1/n&lt;/script&gt;.)&lt;/p&gt;

&lt;p&gt;For &lt;script type=&quot;math/tex&quot;&gt;\hat{p}_2&lt;/script&gt;, the story is a bit different because we’re dealing with a convex 
quadratic—so, think endpoints. Because we chose &lt;script type=&quot;math/tex&quot;&gt;\hat{p}_2 = 0.5&lt;/script&gt;, both 0 and 1
maximize the risk of this expression.&lt;/p&gt;

&lt;h3 id=&quot;setting-up-the-minimax-game&quot;&gt;Setting up the minimax game&lt;/h3&gt;

&lt;p&gt;The central object of this post is the minimax risk, which is a (hopefully) minimized, 
worst-case risk, defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\inf_{\hat{\theta}} \sup_{\theta} R(\theta, \hat{\theta}).&lt;/script&gt;

&lt;p&gt;We can interpret the minimax risk as a type of game: it’s us (&lt;em&gt;the statisticians&lt;/em&gt;) 
against nature.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Nature gets to choose &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;We get to choose &lt;script type=&quot;math/tex&quot;&gt;\hat{\theta}&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Nature chooses &lt;script type=&quot;math/tex&quot;&gt;\sup_{\theta} R(\theta, \hat{\theta})&lt;/script&gt; 
adversarially to maximize our risk.&lt;/li&gt;
  &lt;li&gt;We do our best to minimize our risk given nature’s choice of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.
    &lt;ul&gt;
      &lt;li&gt;That is, we pick &lt;script type=&quot;math/tex&quot;&gt;\inf_{\hat{\theta}} \sup_{\theta} R(\theta, \hat{\theta})&lt;/script&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post draws on material from&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Theoretical Statistics&lt;/em&gt; by Robert Keener;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;All of Statistics&lt;/em&gt; by Larry Wasserman;&lt;/li&gt;
  &lt;li&gt;lectures by Michael Jordan.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">In this post, we’re going to talk about statistical decision theory, which is a topic with its origins in the 1950s. Decision theory is cool because it gives us a way to compare statistical procedures. Moreover, it connects frequentist and Bayesian inference.</summary></entry><entry><title type="html">Game on.</title><link href="http://localhost:4000/jekyll/update/2018/02/03/game-on.html" rel="alternate" type="text/html" title="Game on." /><published>2018-02-03T12:30:00-07:00</published><updated>2018-02-03T12:30:00-07:00</updated><id>http://localhost:4000/jekyll/update/2018/02/03/game-on</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/02/03/game-on.html">&lt;p&gt;Hey blogosphere!&lt;/p&gt;

&lt;p&gt;My name’s Jake Knigge and this blog’s name is Eggink Blog. It’s a place to ink-up (or markdown) some (quasi) egghead-ed topics from statistics, machine learning, mathematics, 
computer science and physics.&lt;/p&gt;

&lt;p&gt;Anyways, here’s a little bit about me…&lt;/p&gt;

&lt;p&gt;I am a Nebraska-native turned Colorado-convert since 2001. 
I graduated from the University of Colorado in 2013 after studying quantitative finance.
I worked at Deloitte for a couple of years following college as a consultant.
In 2014, Angela and I got married. It was a heckuva a party.
I changed jobs in 2015 and started working for CoBank, an agricultural lender, in Denver, Colorado. This my current working home.&lt;/p&gt;

&lt;p&gt;Some of my hobbies are…&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;snowboarding with Angela,&lt;/li&gt;
  &lt;li&gt;using quantitative tools to tell stories and solve problems,&lt;/li&gt;
  &lt;li&gt;trying to understand the world through the “languages” of math and science,&lt;/li&gt;
  &lt;li&gt;trying to understand languages (Italian, German, and French),&lt;/li&gt;
  &lt;li&gt;trail running with Maya (and Angela and Raven, when they’ll partake),&lt;/li&gt;
  &lt;li&gt;reading,&lt;/li&gt;
  &lt;li&gt;learning and understanding the learning process,&lt;/li&gt;
  &lt;li&gt;meditating and stretching (&lt;em&gt;i.e.&lt;/em&gt;, “yoga-ing”),&lt;/li&gt;
  &lt;li&gt;listening to music,&lt;/li&gt;
  &lt;li&gt;playing guitar, banjo, and mandolin.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I have a traditional website that lives &lt;a href=&quot;http://knigge.us&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">Hey blogosphere!</summary></entry></feed>