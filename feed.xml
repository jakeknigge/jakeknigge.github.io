<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-04-21T19:25:42-06:00</updated><id>http://localhost:4000/</id><title type="html">Eggink Blog</title><subtitle>Jake W. Knigge's blah, blah, blog... a place to discuss statistics, math, and  whatever else comes to mind.
</subtitle><entry><title type="html">A neat inequality</title><link href="http://localhost:4000/jekyll/update/2018/04/21/ineq.html" rel="alternate" type="text/html" title="A neat inequality" /><published>2018-04-21T17:00:00-06:00</published><updated>2018-04-21T17:00:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/04/21/ineq</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/04/21/ineq.html">&lt;p&gt;As promised, this post will be about the useful inequality from 
&lt;a href=&quot;/jekyll/update/2018/04/07/lsi.html&quot;&gt;logarithmic Sobolev inequality post&lt;/a&gt;. Without further ado‚Ä¶&lt;/p&gt;

&lt;h3 id=&quot;starting-simple&quot;&gt;Starting simple&lt;/h3&gt;

&lt;p&gt;We are going to prove the inequality&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left( e^{z/2} - e^{y/2} \right)^2 \le \frac{(z-y)^2}{8}(e^{z} + e^{y})&lt;/script&gt;

&lt;p&gt;for real numbers &lt;script type=&quot;math/tex&quot;&gt;z \ge y&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;proof&quot;&gt;Proof&lt;/h3&gt;

&lt;p&gt;We can rewrite the inequality as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left( e^{(z-y)/2} - 1 \right)^2 \le \frac{(z-y)^2}{8}(e^{z-y} + 1).&lt;/script&gt;

&lt;p&gt;Now, let‚Äôs define &lt;script type=&quot;math/tex&quot;&gt;x = z - y&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;x \ge 0&lt;/script&gt; because &lt;script type=&quot;math/tex&quot;&gt;z \ge y&lt;/script&gt;. Let‚Äôs
move everything to the righthand side and open up the square, which gives&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0 \le \frac{x^2}{8} (e^x + 1) - \left( e^x - 2e^{x/2} + 1 \right).&lt;/script&gt;

&lt;p&gt;Now, let‚Äôs collect like terms&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0 \le e^x \left( \frac{x^2}{8} - 1 \right) + \frac{x^2}{8} + 2e^{x/2} - 1 = f(x).&lt;/script&gt;

&lt;p&gt;When &lt;script type=&quot;math/tex&quot;&gt;x = 0&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt;, &lt;em&gt;i.e.&lt;/em&gt;, the righthand side, is also 0. Let‚Äôs look at the
derivative of &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; to see whether the function is increasing or decreasing:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{df(x)}{dx} = \frac{1}{8} e^x (x^2 + 2x - 8) + \frac{x}{4} + e^{x/2}.&lt;/script&gt;

&lt;p&gt;Unfortunately, that pesky &lt;script type=&quot;math/tex&quot;&gt;-e^x&lt;/script&gt; means we still have some work to do. Let‚Äôs 
keep taking derivatives and see if we can‚Äôt get rid of that negative:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\frac{d^2f(x)}{dx^2} &amp;= 
	\frac{1}{8} \left( e^x \left( x^2 + 4x - 6 \right) + 4e^{x/2} + 2 \right) \\
\frac{d^3f(x)}{dx^3} &amp;= 
	\frac{1}{8} \left( e^x \left( x^2 + 6x - 2 \right) + 2e^{x/2} \right) \\
\frac{d^4f(x)}{dx^4} &amp;= 
	\frac{1}{8} \left( e^x \left( x^2 + 8x + 4 \right) + e^{x/2} \right).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Cool! The righthand side of &lt;script type=&quot;math/tex&quot;&gt;\frac{d^4f(x)}{dx^4} &gt; 0&lt;/script&gt; since &lt;script type=&quot;math/tex&quot;&gt;x \ge 0&lt;/script&gt;. 
Also, we have that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{df(0)}{dx} = \frac{d^2f(0)}{dx^2} = \frac{d^3f(0)}{dx^3} = 0,&lt;/script&gt;

&lt;p&gt;which means we‚Äôre in good shape (because the fourth derivative being positive). 
In particular, we integrate the fourth derivative four times and conclude that 
our original function that is always greater than or equal to zero&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
0 \le
\int \frac{d^4f(x)}{dx^4} \, d^4x &amp;= \frac{e^x (x^2 - 8)}{8} + 2 e^{(x/2)}
	+ \frac{c_1 x^3}{6} + \frac{c_2 x^2}{2} + c_3 x + c_4 \\
		&amp;= e^x \left( \frac{x^2}{8} - 1 \right) + \frac{x^2}{8} + 2e^{x/2} - 1,
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;c_4 = -1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;c_3 = 0&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;c_2 = 1/4&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;c_1 = 0&lt;/script&gt;.&lt;/p&gt;

&lt;h4 id=&quot;remarks&quot;&gt;Remarks&lt;/h4&gt;

&lt;p&gt;This inequality is neat because you can approach it a few different ways. If
we didn‚Äôt use &lt;script type=&quot;math/tex&quot;&gt;x = z - y&lt;/script&gt;, and worked directly with the function of &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;
and &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;, we could‚Äôve gotten away with taking two derivatives instead of 
four. I find the ‚Äú&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;-consolidated‚Äù, ‚Äúfourth derivative‚Äù approach easier 
as it requires less bookkeeping for &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;And the best for last: the easiest way to solve this problem is to plot it. üòè&lt;/p&gt;

&lt;h3 id=&quot;next-up&quot;&gt;Next up&lt;/h3&gt;

&lt;p&gt;‚Ä¶ more Rademacher rad-ness, modified logarithmic Sobolev inequalities &lt;strong&gt;or&lt;/strong&gt; 
bandits!&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is related to material from Chapter 5 of (my new favorite book):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Concentration Inequalities: A Nonasymptotic Theory of Independence&lt;/em&gt; by 
S. Boucheron, G. Lugosi, and P. Masart.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">As promised, this post will be about the useful inequality from logarithmic Sobolev inequality post. Without further ado‚Ä¶</summary></entry><entry><title type="html">Upper confidence bounds</title><link href="http://localhost:4000/jekyll/update/2018/04/18/ucb.html" rel="alternate" type="text/html" title="Upper confidence bounds" /><published>2018-04-18T21:00:00-06:00</published><updated>2018-04-18T21:00:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/04/18/ucb</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/04/18/ucb.html">&lt;p&gt;Today we‚Äôre going to prove the upper bound that results from the upper
confidence bound (UCB) strategy. The UCB strategy allowed us to bound
the pseudo-regret by a term which is logarithmic in the number of
rounds played. Generally speaking, ‚Äúgood‚Äù regret bounds for bandit problems
are sublinear in the number of rounds played.&lt;/p&gt;

&lt;h3 id=&quot;stochastic-bandit-problems&quot;&gt;Stochastic bandit problems&lt;/h3&gt;

&lt;p&gt;As a quick review, here‚Äôs the set-up for the stochastic multi-armed bandit
problem.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There are &lt;script type=&quot;math/tex&quot;&gt;\{1,\dots,K\}&lt;/script&gt; arms.&lt;/li&gt;
  &lt;li&gt;Each arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; corresponds to an unknown probability distribution &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;At each time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, the player selects an arm &lt;script type=&quot;math/tex&quot;&gt;I_t&lt;/script&gt; (independently of 
previous choices) and receives a reward &lt;script type=&quot;math/tex&quot;&gt;X_{I_t,t} \sim p_{I_t}&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;We assess a player‚Äôs performance by her &lt;strong&gt;regret&lt;/strong&gt;, which
compares her performance to (something like) the best-possible performance.&lt;/li&gt;
  &lt;li&gt;After the first &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; rounds of the game, the player‚Äôs regret is&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_n = \max_{i=1,\dots,K} \sum_{i=1}^n X_{i,t} - \sum_{i=1}^n X_{I_t,t},&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;where &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; is the number of bandit arms, &lt;script type=&quot;math/tex&quot;&gt;X_{i,t}&lt;/script&gt; is the reward of arm 
&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;I_t&lt;/script&gt; is the player‚Äôs (possibly randomly) chosen arm.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We‚Äôll denote the mean of distribution &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;\mu_i&lt;/script&gt; and define&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu^\star = \max_{i=1,\dots,K} \mu_i \quad \textsf{and} \quad
	i^\star = \argmax_{i=1,\dots,K} \mu_i&lt;/script&gt;

&lt;p&gt;as optimal parameters. Instead of using the regret to assess our player‚Äôs performance,
we‚Äôll use the &lt;strong&gt;pseudo-regret&lt;/strong&gt;, which is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\bar{R}_n &amp;= \max_{i=1,\dots,K} \E 
	\left[ \sum_{i=1}^n X_{i,t} - \sum_{i=1}^n X_{I_t,t} \right] \\
	&amp;= n \mu^\star - \E \sum_{t=1}^n \mu_{I_t}.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We know that &lt;script type=&quot;math/tex&quot;&gt;\bar{R}_n \le \E R_n&lt;/script&gt; because of Jensen‚Äôs inequality for convex 
functions. For our purposes, a more informative form of the pseudo-regret is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{R}_n = \sum_{i=1}^K \Delta_i \E T_i(n),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\Delta_i = \mu^\star - \mu_i&lt;/script&gt; is the suboptimality of arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; and the
random variable &lt;script type=&quot;math/tex&quot;&gt;T_i(n) = \sum_{i=1}^n \ind{}\{I_t = i\}&lt;/script&gt; is the number of times 
arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; is selected in the first &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; rounds.&lt;/p&gt;

&lt;h3 id=&quot;upper-confidence-bounds&quot;&gt;Upper confidence bounds&lt;/h3&gt;

&lt;p&gt;One approach for simultaneously performing &lt;strong&gt;exploration&lt;/strong&gt; and &lt;strong&gt;exploitation&lt;/strong&gt; is 
to use &lt;strong&gt;upper confidence bound&lt;/strong&gt; (UCB) strategies. UCB strategies produce upper
bounds (based on a confidence level) on the current estimate for each arm‚Äôs mean 
reward. The player selects the arm that is the best using the UCB-modified estimate.&lt;/p&gt;

&lt;p&gt;In the spirit of brevity, we‚Äôre going to gloss over a few important details 
(&lt;em&gt;e.g.&lt;/em&gt;, Legendre‚ÄìFenchel duals and Hoeffding‚Äôs lemma) but here‚Äôs the gist of 
the UCB set-up.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Define &lt;script type=&quot;math/tex&quot;&gt;\widehat{\mu}_{i,n} = \frac{1}{n} \sum_{j=1}^n X_{i,j}&lt;/script&gt; as the sample
mean of arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; after being played &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; times.&lt;/li&gt;
  &lt;li&gt;For a convex function &lt;script type=&quot;math/tex&quot;&gt;\psi&lt;/script&gt; with convex conjugate &lt;script type=&quot;math/tex&quot;&gt;\psi^*&lt;/script&gt; and parameter 
&lt;script type=&quot;math/tex&quot;&gt;\lambda &gt; 0&lt;/script&gt;, the cumulant generating function of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is bounded by &lt;script type=&quot;math/tex&quot;&gt;\psi&lt;/script&gt;:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ln \E e^{\lambda(X - \E X)} \le \psi(\lambda) \quad \textsf{and} \quad
\ln \E e^{\lambda(\E X - X)} \le \psi(\lambda).&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;By Markov‚Äôs inequality and the Cram√©r‚ÄìChernoff technique&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\prob \left( \mu_i - \widehat{\mu}_{i,n} \ge \varepsilon \right) \le
	e^{-n \psi^*(\varepsilon)}.&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Markov‚Äôs inequality means that, with probability &lt;script type=&quot;math/tex&quot;&gt;1 - \delta&lt;/script&gt;, we have an 
upper bound on the estimate of the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th arm:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mu_i &lt; \widehat{\mu}_{i,n} + (\psi^*)^{-1} 
	\left(\frac{1}{n} \ln \frac{1}{\delta} \right). %]]&gt;&lt;/script&gt;

&lt;p&gt;The &lt;strong&gt;UCB strategy&lt;/strong&gt; is (very) simple; at each time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, select the arm &lt;script type=&quot;math/tex&quot;&gt;I_t&lt;/script&gt; by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I_t \in \argmax_{i=1,\dots,K} \left\{ \widehat{\mu}_{i,T_i(t-1)} + 
	(\psi^*)^{-1} \left( \frac{\alpha \ln t}{T_i(t-1)} \right) \right\}.&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;The tradeoff.&lt;/strong&gt;
Let‚Äôs pick apart this term and see how it encourages exploration and exploitation.
We‚Äôre going to use &lt;script type=&quot;math/tex&quot;&gt;\psi(x) = x^2/8&lt;/script&gt; with convex conjugate 
&lt;script type=&quot;math/tex&quot;&gt;\psi^*(z) = 2z^2&lt;/script&gt;, so that &lt;script type=&quot;math/tex&quot;&gt;(\psi^*)^{-1}(z) = (x/2)^{1/2}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;When arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; has not been played many times before time $t$, &lt;em&gt;i.e.&lt;/em&gt;, when 
&lt;script type=&quot;math/tex&quot;&gt;T_i(t-1)&lt;/script&gt; is small numerator of &lt;script type=&quot;math/tex&quot;&gt;\frac{\alpha \ln t}{T_i(t-1)}&lt;/script&gt; dominates 
and the UCB-adjusted estimate of arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; is likely to relatively high, which
encourages arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; to be chosen. In other words, it encourages exploration.&lt;/p&gt;

&lt;p&gt;As arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; is chosen more, &lt;script type=&quot;math/tex&quot;&gt;T_i(t-1)&lt;/script&gt; increases and the estimate
&lt;script type=&quot;math/tex&quot;&gt;\widehat{\mu}_{i,T_i(t-1)}&lt;/script&gt; improves. If arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; is the optimal arm, then the 
&lt;script type=&quot;math/tex&quot;&gt;\widehat{\mu}_{i,T_i(t-1)}&lt;/script&gt; term dominates the estimates for other arms and the
UCB-adjustment further encourages selection of this arm. This means that the 
algorithm exploits high-yielding arms.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Upper bound on pseudo-regret.&lt;/strong&gt;
If our player follows the UCB scheme specified above, then for &lt;script type=&quot;math/tex&quot;&gt;\alpha &gt; 2&lt;/script&gt;
her pseudo-regret is upperbounded as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{R}_n \le \sum_{i: \Delta_i &gt; 0} \left( 
\frac{\alpha \Delta_i}{\psi^*(\Delta_i / 2)} \ln n + \frac{\alpha}{\alpha - 2} 
\right).&lt;/script&gt;

&lt;h3 id=&quot;proof&quot;&gt;Proof&lt;/h3&gt;

&lt;p&gt;Okay, here goes. We incur regret when we don‚Äôt pick the optimal arm, &lt;em&gt;i.e.&lt;/em&gt;,
when &lt;script type=&quot;math/tex&quot;&gt;I_t \neq i^\star&lt;/script&gt;. So, we need to analyze the conditions that lead
us to choose a suboptimal arm.&lt;/p&gt;

&lt;p&gt;Assume we‚Äôve played arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; a total of &lt;script type=&quot;math/tex&quot;&gt;T_i(t-1)&lt;/script&gt; times and arm &lt;script type=&quot;math/tex&quot;&gt;i^\star&lt;/script&gt; 
a total of &lt;script type=&quot;math/tex&quot;&gt;T_{i^\star}(t-1)&lt;/script&gt; times. By the set up of our bandit scenario, 
if &lt;script type=&quot;math/tex&quot;&gt;I_t = i&lt;/script&gt;, &lt;em&gt;i.e.&lt;/em&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I_t = i = \argmax_{j=1,\dots,K} \left\{ \widehat{\mu}_{j,T_j(t-1)} + 
	(\psi^*)^{-1} \left( \frac{\alpha \ln t}{T_i(t-1)} \right) \right\},&lt;/script&gt;

&lt;p&gt;then at least one of the following holds&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\widehat{\mu}_{i^\star, T_{i^\star}(t-1)} + 
	(\psi^*)^{-1} \left( \frac{\alpha \ln t}{T_{i^\star}(t-1)} \right) &amp;\le \mu^\star 
	&amp;&amp; \#1 \\
\mu_i + (\psi^*)^{-1} \left( \frac{\alpha \ln t}{T_i(t-1)} \right) &amp;&lt; 
	\widehat{\mu}_{i, T_i(t-1)} &amp;&amp; \#2 \\
\mu^\star - 2(\psi^*)^{-1} \left( \frac{\alpha \ln n}{T_i(t-1)} \right) &amp;&lt; \mu_i &amp;&amp; \#3.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;The first indicates that our current UCB-adjusted estimate of the mean for the
optimal arm is less than its true value. It occurs with probability&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\prob \left(  \widehat{\mu}_{i^\star, T_{i^\star}(t-1)} + 
	(\psi^*)^{-1} \left( \frac{\alpha \ln t}{T_{i^\star}(t-1)} \right) 
	\ge \mu^\star \right) \le 
	\exp \left ( -\alpha \ln t \right) = t^{-\alpha}.&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;The second indicates that our estimate for arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; exceeds its true value by
an amount greater than the UCB‚Äîhence the UCB adjustment is not yet helpful. 
It occurs with probability&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\prob \left(  \mu_i + (\psi^*)^{-1} \left( \frac{\alpha \ln t}{T_i(t-1)} \right) &gt; 
	\widehat{\mu}_{i, T_i(t-1)} \right) \le 
	\exp \left ( -\alpha \ln t \right) = t^{-\alpha}.&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;The third follows from similar logic as the second, and is illuminated by 
the rewriting:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\Delta_i  = \mu^\star - \mu_i &amp;&lt; 
	2(\psi^*)^{-1} \left( \frac{\alpha \ln n}{T_i(t-1)} \right) \\
\psi^*(\Delta_i / 2) &lt; \frac{\alpha \ln n}{T_i(t-1)} &amp;\iff
T_i(t-1) &lt; \frac{\alpha \ln n}{\psi^*(\Delta_i / 2)}.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The probability of the third event is 0 when 
&lt;script type=&quot;math/tex&quot;&gt;T_i(n) \ge u = 
  \left\lceil \frac{\alpha \ln n}{\psi^*(\Delta_i / 2)} \right\rceil&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If #1, #2, and #3 are false, then we pick the optimal arm, meaning we incur
no regret.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, we let‚Äôs consider the implications of these events on &lt;script type=&quot;math/tex&quot;&gt;\E T_i(t-1)&lt;/script&gt;,
because this will allow us to bound the pseudo-regret. We have that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\E T_i(n) &amp;= \E \sum_{t=1}^n \ind{}\{I_t = i\} \\
	&amp;\le u + \E \sum_{t=u+1}^n \ind{}\{I_t = i \textsf{ and inequality #3 is false}\} \\
	&amp;\le u + \E \sum_{t=u+1}^n \ind{}\{\textsf{inequality #1 or #2 is true}\} \\
	&amp;= u + \sum_{t=u+1}^n \prob\left( \ind{\textsf{inequality #1}} \right) + 
		\prob\left( \ind{\textsf{inequality #2}} \right) \\
	&amp;\le u + 2\sum_{t=1}^n \sum_{s=1}^t \frac{1}{t^\alpha} \\
	&amp;= u + 2\sum_{t=1}^n \frac{1}{t^{\alpha-1}} \\
	&amp;\le \frac{\alpha \ln n}{\psi^*(\Delta_i / 2)} + 
		2 \int_{1}^{\infty} \frac{dt}{t^{\alpha-1}} \\
	&amp;= \frac{\alpha \ln n}{\psi^*(\Delta_i / 2)} + \frac{\alpha}{\alpha-2}.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;em&gt;(It took me a few times to follow that argument, so I would re-read it a few times.
I also put some notes in the &lt;a href=&quot;#the-intermediate-steps&quot;&gt;intermediate steps&lt;/a&gt;
subsection below.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Now, we just sum over the suboptimal arms to get the pseudo-regret bound&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{R}_n \le \sum_{i: \Delta_i &gt; 0} \left( 
\frac{\alpha \Delta_i}{\psi^*(\Delta_i / 2)} \ln n + \frac{\alpha}{\alpha - 2}
\right).&lt;/script&gt;

&lt;p&gt;This is the proof given in Bubeck and Cesa-Bianchi‚Äôs &lt;a href=&quot;http://sbubeck.com/SurveyBCB12.pdf&quot;&gt;bandits survey&lt;/a&gt;, 
which is an adaptation of the proof of Auer, Cesa-Bianchi, and Fischer‚Äôs
&lt;a href=&quot;http://cesa-bianchi.di.unimi.it/Pubblicazioni/ml-02.pdf&quot;&gt;original analysis&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;next-up&quot;&gt;Next up&lt;/h3&gt;

&lt;p&gt;‚Ä¶ a proof of that cool inequality that helped us in the proof of 
concentration inequality implied by the 
&lt;a href=&quot;/jekyll/update/2018/04/07/lsi.html&quot;&gt;logarithmic Sobolev inequality&lt;/a&gt; a few posts back!&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is based on material from:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems&lt;/em&gt;
by S. Bubeck and N. Cesa-Bianchi. A digital copy can be found 
&lt;a href=&quot;http://sbubeck.com/SurveyBCB12.pdf&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Finite-time Analysis of the Multiarmed Bandit Problem&lt;/em&gt; by P. Auer, N. 
Cesa-Bianchi, and P. Fischer. A digital copy can be found &lt;a href=&quot;http://cesa-bianchi.di.unimi.it/Pubblicazioni/ml-02.pdf&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Prediction, Learning, and Games&lt;/em&gt; (a.k.a. ‚ÄúPerdition, Burning, and Flames‚Äù) 
by N. Cesa-Bianchi and G. Lugosi. Find a description of the book and its 
table of contents &lt;a href=&quot;http://cesa-bianchi.di.unimi.it/predbook/&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;the-intermediate-steps&quot;&gt;The intermediate steps&lt;/h4&gt;

&lt;p&gt;We can interpret the relationship&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E T_i(n) = \E \sum_{t=1}^n \ind{}\{I_t = i\} \le 
	u + \E \sum_{t=u+1}^n \ind{}\{I_t = i \textsf{ and inequality #3 is false}\}&lt;/script&gt;

&lt;p&gt;by observing that the indicator &lt;script type=&quot;math/tex&quot;&gt;\ind{}\{I_t = i\}&lt;/script&gt; and be written as
&lt;script type=&quot;math/tex&quot;&gt;\ind{}\{I_t = i \textsf{ and inequality #3 is false or true}\}&lt;/script&gt;. 
Inequality #3 is true when 
&lt;script type=&quot;math/tex&quot;&gt;T_i(k) \le u = 
	\left\lceil \frac{\alpha \ln n}{\psi^*(\Delta_i / 2)} \right\rceil&lt;/script&gt;.
So, the bound follows by assuming that indicator is active at &lt;script type=&quot;math/tex&quot;&gt;k=1,\dots,u&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Because we allow either #1 or #2 to be true, the event&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\big\{I_t = i \textsf{ and inequality #3 is false}\big\} \subseteq
	\big\{\textsf{inequality #1 or #2 is true}\big\},&lt;/script&gt;

&lt;p&gt;meaning the probability of the second event is larger.&lt;/p&gt;</content><author><name></name></author><summary type="html">Today we‚Äôre going to prove the upper bound that results from the upper confidence bound (UCB) strategy. The UCB strategy allowed us to bound the pseudo-regret by a term which is logarithmic in the number of rounds played. Generally speaking, ‚Äúgood‚Äù regret bounds for bandit problems are sublinear in the number of rounds played.</summary></entry><entry><title type="html">Bandits!</title><link href="http://localhost:4000/jekyll/update/2018/04/14/bandits.html" rel="alternate" type="text/html" title="Bandits!" /><published>2018-04-14T17:00:00-06:00</published><updated>2018-04-14T17:00:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/04/14/bandits</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/04/14/bandits.html">&lt;p&gt;Today we‚Äôre talking about bandits. Bandit problems, not dissimilar from Markov 
chains, are one of those simple models that apply to a wide variety of
problems. Indeed, many results of ‚Äúsequential decision making‚Äù stem from
the study of bandit problems. Plus the name is cool, so that should provide
some motivation for wanting to study them. The name &lt;strong&gt;bandit&lt;/strong&gt; originates 
from the term &lt;strong&gt;one-armed bandit&lt;/strong&gt;, which was used to describe slot machines.&lt;/p&gt;

&lt;p&gt;In their &lt;a href=&quot;http://sbubeck.com/SurveyBCB12.pdf&quot;&gt;survey on bandits&lt;/a&gt;, Bubeck and Cesa-Bianchi define 
a multi-armed bandit problem as ‚Äúa sequential allocation problem defined by a 
set of actions.‚Äù The allocation problem is similar to a (repeated) game: at 
each point in time, the player chooses an action &lt;em&gt;i.e.&lt;/em&gt;, an arm, and receives 
a reward.&lt;/p&gt;

&lt;p&gt;The player‚Äôs goal is to maximize her cumulative reward, which naturally leads to
an &lt;strong&gt;exploitation vs. exploration&lt;/strong&gt; problem. The player needs to simultaneously 
exploit arms known to yield high rewards and explore other arms in the hopes 
of finding other high-yield rewards.&lt;/p&gt;

&lt;p&gt;In bandit problems, we assess a player‚Äôs performance by her &lt;strong&gt;regret&lt;/strong&gt;, which
compares her performance to (something like) the best-possible performance. 
After the first &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; rounds of the game, the player‚Äôs regret is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_n = \max_{i=1,\dots,K} \sum_{i=1}^n X_{i,t} - \sum_{i=1}^n X_{I_t,t},&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; is the number of bandit arms, &lt;script type=&quot;math/tex&quot;&gt;X_{i,t}&lt;/script&gt; is the reward of arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; at
time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;I_t&lt;/script&gt; is the player‚Äôs (possibly randomly) chosen arm.&lt;/p&gt;

&lt;h3 id=&quot;stochastic-bandit-problems&quot;&gt;Stochastic bandit problems&lt;/h3&gt;

&lt;p&gt;In this post, we‚Äôre going to focus on &lt;strong&gt;stochastic bandit problems&lt;/strong&gt; as opposed
to &lt;strong&gt;adversarially bandit problems&lt;/strong&gt;. Here‚Äôs the set-up.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There are &lt;script type=&quot;math/tex&quot;&gt;\{1,\dots,K\}&lt;/script&gt; arms.&lt;/li&gt;
  &lt;li&gt;Each arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; corresponds to an unknown probability distribution &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;At each time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, the player selects an arm &lt;script type=&quot;math/tex&quot;&gt;I_t&lt;/script&gt; (independently of 
previous choices) and receives a reward &lt;script type=&quot;math/tex&quot;&gt;X_{I_t,t} \sim p_{I_t}&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We‚Äôll denote the mean of distribution &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;\mu_i&lt;/script&gt; and define&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu^\star = \max_{i=1,\dots,K} \mu_i \quad \textsf{and} \quad
	i^\star = \argmax_{i=1,\dots,K} \mu_i&lt;/script&gt;

&lt;p&gt;as optimal parameters. Instead of using the regret to assess our player‚Äôs performance,
we‚Äôll use the &lt;strong&gt;pseudo-regret&lt;/strong&gt;, which is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\bar{R}_n &amp;= \max_{i=1,\dots,K} \E 
	\left[ \sum_{i=1}^n X_{i,t} - \sum_{i=1}^n X_{I_t,t} \right] \\
	&amp;= n \mu^\star - \E \sum_{t=1}^n \mu_{I_t}.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We know that &lt;script type=&quot;math/tex&quot;&gt;\bar{R}_n \le \E R_n&lt;/script&gt; because of Jensen‚Äôs inequality for convex 
functions. For our purposes, a more informative form of the pseudo-regret is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{R}_n = \sum_{i=1}^K \Delta_i \E T_i(n),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\Delta_i = \mu^\star - \mu_i&lt;/script&gt; is the suboptimality of arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; and the
random variable &lt;script type=&quot;math/tex&quot;&gt;T_i(n) = \sum_{i=1}^n \ind{}\{I_t = i\}&lt;/script&gt; is the number of times 
arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; is selected in the first &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; rounds.&lt;/p&gt;

&lt;p&gt;This alternative expression for the pseudo-regret sums over arms rather than rounds.
To see how we switched from arms to rounds, play around with random variables 
&lt;script type=&quot;math/tex&quot;&gt;T_i(n)&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;upper-confidence-bounds&quot;&gt;Upper confidence bounds&lt;/h3&gt;

&lt;p&gt;One approach for simultaneously performing &lt;strong&gt;exploration&lt;/strong&gt; and &lt;strong&gt;exploitation&lt;/strong&gt; is 
to use &lt;strong&gt;upper confidence bound&lt;/strong&gt; (UCB) strategies. UCB strategies produce upper
bounds (based on a confidence level) on the current estimate for each arm‚Äôs mean 
reward. The player selects the arm that is the best using the UCB-modified estimate.&lt;/p&gt;

&lt;p&gt;In the spirit of brevity, we‚Äôre going to gloss over a few important details 
(&lt;em&gt;e.g.&lt;/em&gt;, Legendre‚ÄìFenchel duals and Hoeffding‚Äôs lemma) but here‚Äôs the gist of 
the UCB set-up.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Define &lt;script type=&quot;math/tex&quot;&gt;\widehat{\mu}_{i,n} = \frac{1}{n} \sum_{j=1}^n X_{i,j}&lt;/script&gt; as the sample
mean of arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; after being played &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; times.&lt;/li&gt;
  &lt;li&gt;For a convex function &lt;script type=&quot;math/tex&quot;&gt;\psi&lt;/script&gt; with convex conjugate &lt;script type=&quot;math/tex&quot;&gt;\psi^*&lt;/script&gt; and parameter 
&lt;script type=&quot;math/tex&quot;&gt;\lambda &gt; 0&lt;/script&gt;, the cumulant generating function of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is bounded by &lt;script type=&quot;math/tex&quot;&gt;\psi&lt;/script&gt;:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ln \E e^{\lambda(X - \E X)} \le \psi(\lambda) \quad \textsf{and} \quad
\ln \E e^{\lambda(\E X - X)} \le \psi(\lambda).&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;By Markov‚Äôs inequality and the Cram√©r‚ÄìChernoff technique&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\prob \left( \mu_i - \widehat{\mu}_{i,n} \ge \varepsilon \right) \le
	e^{-n \psi^*(\varepsilon)}.&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Markov‚Äôs inequality means that, with probability &lt;script type=&quot;math/tex&quot;&gt;1 - \delta&lt;/script&gt;, we have an 
upper bound on the estimate of the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th arm:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mu_i &lt; \widehat{\mu}_{i,n} + (\psi^*)^{-1} 
	\left(\frac{1}{n} \ln \frac{1}{\delta} \right). %]]&gt;&lt;/script&gt;

&lt;p&gt;The UCB strategy is (very) simple; at each time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, select the arm &lt;script type=&quot;math/tex&quot;&gt;I_t&lt;/script&gt; by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I_t \in \argmax_{i=1,\dots,K} \left\{ \widehat{\mu}_{i,T_i(t-1)} + 
	(\psi^*)^{-1} \left( \frac{\alpha \ln t}{T_i(t-1)} \right) \right\}.&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;The tradeoff.&lt;/strong&gt;
Let‚Äôs pick apart this term and see how it encourages exploration and exploitation.
We‚Äôre going to use &lt;script type=&quot;math/tex&quot;&gt;\psi(x) = x^2/8&lt;/script&gt; with convex conjugate 
&lt;script type=&quot;math/tex&quot;&gt;\psi^*(z) = 2z^2&lt;/script&gt;, so that &lt;script type=&quot;math/tex&quot;&gt;(\psi^*)^{-1}(z) = (x/2)^{1/2}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;When arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; has not been played many times before time $t$, &lt;em&gt;i.e.&lt;/em&gt;, when 
&lt;script type=&quot;math/tex&quot;&gt;T_i(t-1)&lt;/script&gt; is small numerator of &lt;script type=&quot;math/tex&quot;&gt;\frac{\alpha \ln t}{T_i(t-1)}&lt;/script&gt; dominates 
and the UCB-adjusted estimate of arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; is likely to relatively high, which
encourages arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; to be chosen. In other words, it encourages exploration.&lt;/p&gt;

&lt;p&gt;As arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; is chosen more, &lt;script type=&quot;math/tex&quot;&gt;T_i(t-1)&lt;/script&gt; increases and the estimate
&lt;script type=&quot;math/tex&quot;&gt;\widehat{\mu}_{i,T_i(t-1)}&lt;/script&gt; improves. If arm &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; is the optimal arm, then the 
&lt;script type=&quot;math/tex&quot;&gt;\widehat{\mu}_{i,T_i(t-1)}&lt;/script&gt; term dominates the estimates for other arms and the
UCB-adjustment further encourages selection of this arm. This means that the 
algorithm exploits high-yielding arms.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Upper bound on pseudo-regret.&lt;/strong&gt;
If our player follows the UCB scheme specified above, then for &lt;script type=&quot;math/tex&quot;&gt;\alpha &gt; 2&lt;/script&gt;
her pseudo-regret is upperbounded as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{R}_n \le \sum_{i: \Delta_i &gt; 0} \left( 
\frac{\alpha \Delta_i}{\psi^*(\Delta_i / 2)} \ln n + \frac{\alpha}{\alpha - 2} 
\right).&lt;/script&gt;

&lt;p&gt;We‚Äôre going to prove the bound in the next post.&lt;/p&gt;

&lt;h3 id=&quot;bandit-simulations&quot;&gt;Bandit simulations&lt;/h3&gt;

&lt;p&gt;Here‚Äôs an &lt;a href=&quot;https://github.com/jakeknigge/ucb-bandits/blob/master/bandits.R&quot;&gt;R script&lt;/a&gt; which uses the upper confidence bound algorithm.
For the plots below, there are &lt;script type=&quot;math/tex&quot;&gt;K = 3&lt;/script&gt; arms and &lt;script type=&quot;math/tex&quot;&gt;n = 1000&lt;/script&gt; rounds. The reward 
distributions are Bernoulli‚Äôs and the mean parameters the three arms are 0.46, 0.72, 
and 0.55.&lt;/p&gt;

&lt;p&gt;The first plot shows the evolution of each mean estimate as different rounds are
played.
&lt;img src=&quot;/assets/2018-04-14_bandit_arm_est.png&quot; alt=&quot;Bandit arm estimates&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The next plot illustrates the exploration vs. exploitation trade-off. Even
though arm 2 yields the highest rewards, the algorithm occasionally plays 
arms 1 and 3 in later rounds.
&lt;img src=&quot;/assets/2018-04-14_bandit_arms_played.png&quot; alt=&quot;Bandit regret&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The final plot shows the player‚Äôs cumulative regret compared to the UCB-implied
upper bound on the regret; the lower bound is distribution-dependent and
asymptotic, which is why it exceeds the pseudo-regret until round 400ish.
&lt;img src=&quot;/assets/2018-04-14_bandit_regret.png&quot; alt=&quot;Bandit regret&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;next-up&quot;&gt;Next up&lt;/h3&gt;

&lt;p&gt;‚Ä¶ a proof of the upper confidence bound regret rate!&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is based on material from:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems&lt;/em&gt;
by S. Bubeck and N. Cesa-Bianchi. A digital copy can be found 
&lt;a href=&quot;http://sbubeck.com/SurveyBCB12.pdf&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Prediction, Learning, and Games&lt;/em&gt; (a.k.a. ‚ÄúPerdition, Burning, and Flames‚Äù) 
by N. Cesa-Bianchi and G. Lugosi. Find a description of the book and its 
table of contents &lt;a href=&quot;http://cesa-bianchi.di.unimi.it/predbook/&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I would also recommend checking out S√©bastien Bubeck‚Äôs blog 
&lt;a href=&quot;https://blogs.princeton.edu/imabandit&quot;&gt;‚ÄúI‚Äôm a bandit‚Äù&lt;/a&gt; and, in particular, his bandit tutorials:
&lt;a href=&quot;https://blogs.princeton.edu/imabandit/2016/05/11/bandit-theory-part-i/&quot;&gt;part 1&lt;/a&gt; and &lt;a href=&quot;https://blogs.princeton.edu/imabandit/2016/05/13/bandit-theory-part-ii/&quot;&gt;part 2&lt;/a&gt;. Also check out his new YouTube
channel: https://www.youtube.com/user/sebastienbubeck/videos.&lt;/p&gt;</content><author><name></name></author><summary type="html">Today we‚Äôre talking about bandits. Bandit problems, not dissimilar from Markov chains, are one of those simple models that apply to a wide variety of problems. Indeed, many results of ‚Äúsequential decision making‚Äù stem from the study of bandit problems. Plus the name is cool, so that should provide some motivation for wanting to study them. The name bandit originates from the term one-armed bandit, which was used to describe slot machines.</summary></entry><entry><title type="html">Logarithmic Sobolev inequalities</title><link href="http://localhost:4000/jekyll/update/2018/04/07/lsi.html" rel="alternate" type="text/html" title="Logarithmic Sobolev inequalities" /><published>2018-04-07T17:00:00-06:00</published><updated>2018-04-07T17:00:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/04/07/lsi</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/04/07/lsi.html">&lt;p&gt;As promised, this post will be about logarithmic Sobolev inequalities, which
are (exponential) concentration inequalities for functions defined on the
binary hypercube. In particular, we‚Äôll review a specific case where we
get speedy concentration thanks to a variance-like bound on our function of
interest. Let‚Äôs concentrate some measure!&lt;/p&gt;

&lt;h3 id=&quot;starting-simple&quot;&gt;Starting simple&lt;/h3&gt;

&lt;p&gt;To get things going, we‚Äôll look at a logarithmic Sobolev inequality for a 
function defined on the &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; dimensional binary hypercube&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f: \{-1,1\}^n \to \reals.&lt;/script&gt;

&lt;p&gt;Our random variables &lt;script type=&quot;math/tex&quot;&gt;X \in \{-1,1\}^n&lt;/script&gt; are Rademacher random variables, 
&lt;em&gt;i.e.&lt;/em&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
g_X(x) = 
\begin{cases}
+1, &amp; \textsf{with probability } 0.5 \\
-1, &amp; \textsf{with probability } 0.5.
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;We define &lt;script type=&quot;math/tex&quot;&gt;Z = f(X)&lt;/script&gt; as our real-valued random variable of interest. 
Logarithmic Sobolev inequalities compare to quantities related to &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;entropy functional&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathrm{ent}(f) = \E \big[ f(X) \log f(X) \big] - 
				  \E f(X) \log \big( \E f(X) \big)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Efron‚ÄìStein-like functional&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{E}(f) = \frac{1}{2} \E \left[ \sum_{i=1}^n \left( f(X) - 
	f \left( \tilde{X}^{(i)} \right) \right)^2 \right].&lt;/script&gt;

&lt;p&gt;The quantity &lt;script type=&quot;math/tex&quot;&gt;\tilde{X}^{(i)}&lt;/script&gt; is the vector &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; with its &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th
component drawn from an independent copy‚Äîthis is the same idea as in the
Efron‚ÄìStein inequality. For our purposes, this implies that &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; changes
sign.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;logarithmic Sobolev inequality&lt;/strong&gt;for this set up is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathrm{ent}(f^2) \le 2 \mathcal{E}(f).&lt;/script&gt;

&lt;h3 id=&quot;concentration-inequality&quot;&gt;Concentration inequality&lt;/h3&gt;

&lt;p&gt;With our logarithmic Sobolev inequality pinned down, we‚Äôll now prove
a concentration inequality for a function satisfying&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n \left( f(x) - f \left( \tilde{x}^{(i)} \right) \right)^2 \le v&lt;/script&gt;

&lt;p&gt;for some &lt;script type=&quot;math/tex&quot;&gt;v &gt; 0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x \in \{-1,1\}^n&lt;/script&gt;. In particular, we‚Äôll show that
for all &lt;script type=&quot;math/tex&quot;&gt;t &gt; 0&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\prob(Z &gt; \E Z + t) \le e^{-2t^2/v}.&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Technical detail‚Äîhint.&lt;/strong&gt;
The proof hinges on the following fact (which we‚Äôll show in a later post
because it uses some nice ideas)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left( e^{z/2} - e^{y/2} \right)^2 \le \frac{(z-y)^2}{8}(e^{z} + e^{y})&lt;/script&gt;

&lt;p&gt;for &lt;script type=&quot;math/tex&quot;&gt;z \ge y&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;proof&quot;&gt;Proof&lt;/h3&gt;

&lt;p&gt;We‚Äôre going to follow the so-called &lt;strong&gt;Herbst argument&lt;/strong&gt; to prove the
concentration inequality. The key ingredient of Herbst‚Äôs argument is the use
of a &lt;strong&gt;differential inequality&lt;/strong&gt; on a function &lt;script type=&quot;math/tex&quot;&gt;g(x) = e^{\lambda f(x)/2}&lt;/script&gt;
with parameter &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Part 1: defining a differential inequality.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The entropy of &lt;script type=&quot;math/tex&quot;&gt;g^2&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathrm{ent}(g^2) = \mathrm{ent}(e^{\lambda f}) 
	= \lambda \E \big[ e^{\lambda f(X)} f(X) \big] -  
	  \E e^{\lambda f(X)} \log \big( \E e^{\lambda f(X)} \big).&lt;/script&gt;

&lt;p&gt;Now, if we stare at this for a moment and let our &lt;em&gt;cumulant generating 
function&lt;/em&gt; neurons fire, then we see that if we let 
&lt;script type=&quot;math/tex&quot;&gt;F(\lambda) = \E e^{\lambda f(X)}&lt;/script&gt;, then we get a &lt;strong&gt;differential inequality&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathrm{ent}(g^2) = \lambda F'(\lambda) - F(\lambda) \log F(\lambda).&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Part 2: putting the logarithmic Sobolev inequality to work.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The logarithmic Sobolev inequality states&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathrm{ent}(g^2) &amp;\le 2 \mathcal{E}(g) \\
&amp;\stackrel{\textsf{def}}{=} 
	\E \left[ \sum_{i=1}^n \left( e^{\lambda f(X) / 2} - 
		e^{\lambda f \left( \tilde{X}^{(i)} \right) / 2} \right)^2 \right] \\
&amp;\stackrel{\textsf{hint}}{\le} 
	\frac{1}{8} \sum_{i=1}^n \E \left[ 
	\lambda^2 \left( f(X) - f \left( \tilde{X}^{(i)} \right) \right)^2 
	\left( e^{\lambda f(X)} - e^{\lambda f \left( \tilde{X}^{(i)} \right)} \right)
	\right] \\
&amp;\le \frac{\lambda^2 v}{8} \E \left[ e^{\lambda f(X)} \right].
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Part 3: connecting the dots with some calculus.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Okay, cool; we‚Äôve shown that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathrm{ent}(e^{\lambda f}) \le \frac{\lambda^2 v}{8} 
	\E \left[ e^{\lambda f(X)} \right]&lt;/script&gt;

&lt;p&gt;which we can rewrite as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda F'(\lambda) - F(\lambda) \log F(\lambda) \le 
	\frac{\lambda^2 v}{8} F(\lambda).&lt;/script&gt;

&lt;p&gt;Now, let‚Äôs write this guy in a slightly different form in the spirit of
the Herbst argument:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\lambda F'(\lambda) - F(\lambda) \log F(\lambda)}{\lambda^2 F(\lambda)} 
	= \frac{\lambda F'(\lambda) - \log F(\lambda)}{\lambda^2 F(\lambda)}
	\le \frac{v}{8}.&lt;/script&gt;

&lt;p&gt;The calculus part of brain might be screaming something like&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{d}{dz} \left( \frac{\log z}{z} \right) = \frac{1 - \log z}{z^2},&lt;/script&gt;

&lt;p&gt;which we translate to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{d}{dz} \left( \frac{\log F(\lambda)}{\lambda} \right) \le \frac{v}{8}.&lt;/script&gt;

&lt;p&gt;Invoking more calculus ideas, l‚ÄôHospital‚Äôs rule says&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lim_{\lambda \to 0} \frac{\log F(\lambda)}{\lambda} 
	= \frac{F'(0)}{F(0)} = \E Z.&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Part 4: bounding via Cram√©r‚ÄìChernoff and Markov.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We‚Äôre almost there! Let‚Äôs assume that &lt;script type=&quot;math/tex&quot;&gt;\lambda &gt; 0&lt;/script&gt; and then integrate
the inequality between &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;. From l‚ÄôHospital‚Äôs rule, we get
that &lt;script type=&quot;math/tex&quot;&gt;\log F(\lambda) / \lambda \le \E Z + \lambda v / 8&lt;/script&gt;. Now, let‚Äôs undo
our &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;-in-the-denominator and our logarithmic term and write&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(\lambda) \le e^{\lambda \E Z + \lambda^2 v / 8}.&lt;/script&gt;

&lt;p&gt;Finally, we can use the Cram√©r‚ÄìChernoff technique and Markov‚Äôs inequality to 
say that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\prob(Z &gt; \E Z + t) 
	&amp;\le \inf_{\lambda &gt; 0} F(\lambda)e^{-\lambda \E Z - \lambda t} \\
	&amp;\le \inf_{\lambda &gt; 0}  e^{\lambda^2 v / 8 - \lambda t} \\
	&amp;\le e^{-2t^2/v}.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;(Note that &lt;script type=&quot;math/tex&quot;&gt;\lambda = 4t/v&lt;/script&gt; minimizes the upper bound.) Pat yourself on the 
back and give Mr. Herbst a (pretend) pat on the back too.&lt;/p&gt;

&lt;h3 id=&quot;next-up&quot;&gt;Next up&lt;/h3&gt;

&lt;p&gt;‚Ä¶bandits!&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is based on material from Chapter 5 of (my new favorite book):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Concentration Inequalities: A Nonasymptotic Theory of Independence&lt;/em&gt; by 
S. Boucheron, G. Lugosi, and P. Masart.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">As promised, this post will be about logarithmic Sobolev inequalities, which are (exponential) concentration inequalities for functions defined on the binary hypercube. In particular, we‚Äôll review a specific case where we get speedy concentration thanks to a variance-like bound on our function of interest. Let‚Äôs concentrate some measure!</summary></entry><entry><title type="html">Some (convex) relaxation</title><link href="http://localhost:4000/jekyll/update/2018/03/24/convex-relaxation.html" rel="alternate" type="text/html" title="Some (convex) relaxation" /><published>2018-03-24T17:00:00-06:00</published><updated>2018-03-24T17:00:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/03/24/convex-relaxation</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/03/24/convex-relaxation.html">&lt;p&gt;Okay, last time I said that the next post would be about logarithmic 
Sobolev inequalities‚Ä¶ well, it‚Äôs not. I lied. Instead, we‚Äôre taking a 
momentary break from concentration inequalities. It‚Äôs time for some
relaxation of the convex variety.&lt;/p&gt;

&lt;p&gt;Actually, that‚Äôs a bit of lie too because the post isn‚Äôt about convex 
relaxation, but it &lt;strong&gt;is&lt;/strong&gt; about convex optimization. Andiamo!&lt;/p&gt;

&lt;h3 id=&quot;maximum-likelihood-of-poisson-distributions&quot;&gt;Maximum likelihood of Poisson distributions&lt;/h3&gt;

&lt;p&gt;We‚Äôre going to work through a problem from &lt;em&gt;Convex Optimization&lt;/em&gt; by Boyd
and Vandenberghe. (If you‚Äôre curious it‚Äôs problem 7.7.) Here‚Äôs the set up.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We have &lt;script type=&quot;math/tex&quot;&gt;X_1,\dots,X_n&lt;/script&gt; independent Poisson random variables. That is
each &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; has a probability mass function parameterized by 
&lt;script type=&quot;math/tex&quot;&gt;\lambda_i &gt; 0&lt;/script&gt; that is given by&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\prob (X_i = k) = \frac{e^{-\lambda_i}\lambda_i^k}{k!}.&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;The data &lt;script type=&quot;math/tex&quot;&gt;x_1,\dots,x_n&lt;/script&gt; represent the number of times that one of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;
possible events occurred (independently of one another).&lt;/li&gt;
  &lt;li&gt;We have &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; detectors and event &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; is recorded by detector &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; with
probability &lt;script type=&quot;math/tex&quot;&gt;p_{ji}&lt;/script&gt;. We assume that &lt;script type=&quot;math/tex&quot;&gt;p_{ji} \ge 0&lt;/script&gt;, 
&lt;script type=&quot;math/tex&quot;&gt;\sum_{j=1}^m p_{ji} \le 1&lt;/script&gt;, &lt;strong&gt;and&lt;/strong&gt; that the &lt;script type=&quot;math/tex&quot;&gt;p_{ji}&lt;/script&gt; are given.&lt;/li&gt;
  &lt;li&gt;The total number of events recorded by detector &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; is&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_j = \sum_{i=1}^n y_{ji}, \quad j = 1,\dots,m.&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Our goal is to estimate the &lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt; based on observations &lt;script type=&quot;math/tex&quot;&gt;y_j&lt;/script&gt;
and the given probabilities &lt;script type=&quot;math/tex&quot;&gt;p_{ji}&lt;/script&gt; by solving a convex optimization
problem.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To pose this estimation problem as a convex optimization problem, we‚Äôll
use the fact that the &lt;script type=&quot;math/tex&quot;&gt;y_{ji}&lt;/script&gt; were generated from a Poisson distribution
with mean &lt;script type=&quot;math/tex&quot;&gt;p_{ji}\lambda_i&lt;/script&gt;. Intuitively, this means we only capture
a fraction, in particular &lt;script type=&quot;math/tex&quot;&gt;p_{ji}&lt;/script&gt;, of the Poisson-distributed events 
associated with &lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt;. For those who have had a course on Markov 
chains this should cause your &lt;strong&gt;thinned Poisson process&lt;/strong&gt; neurons to fire.&lt;/p&gt;

&lt;p&gt;Now, we use the fact that the sum of Poisson random variables is itself 
a Poisson random variable. In particular, we have that the &lt;script type=&quot;math/tex&quot;&gt;y_j&lt;/script&gt; were
generated by a Poisson distribution with parameter&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_j^T \lambda = \sum_{i=1}^n p_{ji}\lambda_i,&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;p_j \in \reals^n&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\lambda \in \reals_+^n&lt;/script&gt; is the vector of
&lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;likelihood&lt;/strong&gt; of the data has the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\prod_{j=1}^m \frac{(p_j^T \lambda)^{y_j}e^{-(p_j^T \lambda)}}{y_j!}&lt;/script&gt;

&lt;p&gt;which gives a &lt;strong&gt;log-likelihood&lt;/strong&gt; of&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{j=1}^m \left( y_j\log(p_j^T \lambda)-p_j^T \lambda-\log(y_j!) \right).&lt;/script&gt;

&lt;p&gt;We‚Äôll throw away the &lt;script type=&quot;math/tex&quot;&gt;-\log(y_j!)&lt;/script&gt; term because it is a constant. So, the 
estimation problem is the convex optimization problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}[ll]
\mbox{\text{maximize}} &amp; \sum_{j=1}^m \big( y_j \log(p_j^T \lambda) -
	 p_j^T \lambda \big)
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;with optimization variable &lt;script type=&quot;math/tex&quot;&gt;\lambda \in \reals_+^n&lt;/script&gt; and problem data
&lt;script type=&quot;math/tex&quot;&gt;y_1,\dots,y_m&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;p_{ji}&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;i = 1,\dots,n&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;j = 1,d\dots,m&lt;/script&gt;.
More explicitly, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}[ll]
\mbox{\text{maximize}} &amp; \sum_{j=1}^m \big( y_j \log(p_j^T \lambda) -
	 p_j^T \lambda \big) \\
\mbox{subject to} &amp; \lambda \succeq 0.
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Put it to use.&lt;/strong&gt; Now, go out and find yourself a particle accelerator, 
equip it with some detectors, start colliding particles and count your 
Higgs-Bosons and make some inferences.&lt;/p&gt;

&lt;h3 id=&quot;next-up&quot;&gt;Next up&lt;/h3&gt;

&lt;p&gt;‚Ä¶ logarithmic Sobolev inequalities‚Äîfor real this time (well, next time)!&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is based on material from‚Ä¶&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Convex Optimization&lt;/em&gt; by Stephen Boyd and Lieven Vandenberghe. Check it
out here: &lt;em&gt;&lt;a href=&quot;https://web.stanford.edu/~boyd/cvxbook/&quot;&gt;Convex Optimization&lt;/a&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Okay, last time I said that the next post would be about logarithmic Sobolev inequalities‚Ä¶ well, it‚Äôs not. I lied. Instead, we‚Äôre taking a momentary break from concentration inequalities. It‚Äôs time for some relaxation of the convex variety.</summary></entry><entry><title type="html">Entropy and its subadditivity</title><link href="http://localhost:4000/jekyll/update/2018/03/17/sub-entropy.html" rel="alternate" type="text/html" title="Entropy and its subadditivity" /><published>2018-03-17T20:00:00-06:00</published><updated>2018-03-17T20:00:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/03/17/sub-entropy</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/03/17/sub-entropy.html">&lt;p&gt;(Happy St. Patrick‚Äôs Day!)&lt;/p&gt;

&lt;p&gt;In this post, we‚Äôre picking up from the previous post and continuing with
some information-theoretic ideas as they arise in concentration inequalities.
We‚Äôll quickly revisit the Efron‚ÄìStein inequality and then proceed towards
the subadditivity of entropy.&lt;/p&gt;

&lt;h3 id=&quot;efronstein&quot;&gt;Efron‚ÄìStein&lt;/h3&gt;

&lt;p&gt;To encourage the formation of new (or reinforce existing) neuronal 
connections, let‚Äôs remind ourselves of the Efron‚ÄìStein inequality. 
If we have independent random variables &lt;script type=&quot;math/tex&quot;&gt;X_1,\dots,X_n&lt;/script&gt; and a 
square-integrable function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;Z = f(X)&lt;/script&gt;, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Var(Z) \le \sum_{i=1}^n \E \left[ \E^{(i)}[Z^2] - 
	\big( \E^{(i)}Z \big)^2 \right],&lt;/script&gt;

&lt;p&gt;where the operator &lt;script type=&quot;math/tex&quot;&gt;\E^{(i)}&lt;/script&gt; is the conditional expectation with respect
to the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th variable. The square-integrable condition on &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; ensures 
that its variance is finite.&lt;/p&gt;

&lt;h3 id=&quot;entropy&quot;&gt;Entropy&lt;/h3&gt;

&lt;p&gt;Now, let‚Äôs generalize the Efron‚ÄìStein inequality by observing that we
&lt;em&gt;could&lt;/em&gt; write it as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E [Z^2] - \big( \E Z \big)^2 \le 
	\sum_{i=1}^n \E \left[ \E^{(i)}[Z^2] - \big( \E^{(i)}Z \big)^2 \right],&lt;/script&gt;

&lt;p&gt;where we used the fact that &lt;script type=&quot;math/tex&quot;&gt;\Var(Y) = \E [Y^2] - \big( \E Y \big)^2&lt;/script&gt;.
If we stare at this representation of the various for a moment, 
we see that it has the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E [g(Z)] - g( \E Z),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;g(x) = x^2&lt;/script&gt;. When &lt;script type=&quot;math/tex&quot;&gt;g(x) = x \log x&lt;/script&gt;, then quantity 
&lt;script type=&quot;math/tex&quot;&gt;\E [g(Z)] - g( \E Z)&lt;/script&gt; is often called the &lt;strong&gt;entropy&lt;/strong&gt; and is
denoted by &lt;script type=&quot;math/tex&quot;&gt;\mathrm{Ent}(Z)&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;subadditivity-of-entropy&quot;&gt;Subadditivity of entropy&lt;/h3&gt;

&lt;p&gt;Using the Efron‚ÄìStein inequality as our source of inspiration, we
might guess (or hope) that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E [g(Z)] - g( \E Z) \le 
	\sum_{i=1}^n \E \left[ \E^{(i)}g(Z) - g\big( \E^{(i)}Z \big) \right].&lt;/script&gt;

&lt;p&gt;If we did guess this, then we‚Äôd be correct! So, let‚Äôs prove it.&lt;/p&gt;

&lt;p&gt;We‚Äôll assume that &lt;script type=&quot;math/tex&quot;&gt;X_1,\dots,X_n&lt;/script&gt; are independent random variables 
taking values in a finite set &lt;script type=&quot;math/tex&quot;&gt;\mathcal{S}&lt;/script&gt; and that 
&lt;script type=&quot;math/tex&quot;&gt;f: \mathcal{S^n} \to \reals_+&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;Z = f(X_1,\dots,X_n)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The proof relies on an application of Han‚Äôs inequality for relative
entropies (which is an extension of what we showed in the 
&lt;a href=&quot;https://jakeknigge.github.io/jekyll/update/2018/03/11/han.html&quot;&gt;previous post&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Note that &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; is always positive, so the result holds for any
&lt;script type=&quot;math/tex&quot;&gt;cZ&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;c&gt;0&lt;/script&gt;. Let‚Äôs use this to simplify our lives and assume that
&lt;script type=&quot;math/tex&quot;&gt;\E Z = 1&lt;/script&gt;. So, now we can write&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E g(Z) - g( \E Z) = \E[Z \log Z] - 0.&lt;/script&gt;

&lt;p&gt;Now, we‚Äôre going to introduce the relative entropy component by
defining&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q(x) = f(x)p(x) \iff \frac{q(x)}{p(x)} = f(x).&lt;/script&gt;

&lt;p&gt;The relative entropy neurons in your brain are probably going wild 
because we can now write&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z = f(X) = \frac{q(X)}{p(X)}.&lt;/script&gt;

&lt;p&gt;So, let‚Äôs take an expectation of &lt;script type=&quot;math/tex&quot;&gt;g(Z)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\E g(Z) &amp;= \E g\big( f(X) \big) \\
	&amp;= \E_P \left[ \frac{q(X)}{p(X)} \log \frac{q(X)}{p(X)} \right] \\
	&amp;= D_{\mathrm{kl}}(q(X), p(X)).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Here‚Äôs where &lt;strong&gt;Han‚Äôs inequality&lt;/strong&gt; comes to the scene, because the
relative entropy form of Han‚Äôs inequality says that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_{\mathrm{kl}}(q(X), p(X)) \le \sum_{i=1}^n
	\left( D_{\mathrm{kl}}(q(X), p(X)) - 
	D_{\mathrm{kl}}\left(q^{(i)}(X), p^{(i)}(X)\right) \right).&lt;/script&gt;

&lt;p&gt;In other words, we have a bound on the (joint) relative entropy based on 
the sum of marginal relative entropies. Lastly, we‚Äôll show that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n 
	\left( D_{\mathrm{kl}}(q(X), p(X)) - 
	D_{\mathrm{kl}}\left(q^{(i)}(X), p^{(i)}(X)\right) \right) =
\sum_{i=1}^n 
	\E \left[ \E^{(i)} g(Z) - 
	g \left( \E^{(i)} Z \right) \right],&lt;/script&gt;

&lt;p&gt;which will conclude the proof. The tower property of expectation let‚Äôs us
write&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_{\mathrm{kl}}\left(q(X), p(X)\right) = \E \left[ \E^{(i)} g(Z) \right]&lt;/script&gt;

&lt;p&gt;and (with a bit more work)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
D_{\mathrm{kl}}\left(q^{(i)}(X), p^{(i)}(X)\right) &amp;=
\sum_{x^{(i)} \in \mathcal{S}^{n-1}} p^{(i)} \left( x^{(i)} \right)
	\left( \sum_{y \in \mathcal{S}} q^{(i)} \left( x^{(i)} \right) \right) 
	\log \frac{p^{(i)}\left( x^{(i)} \right) 
			\sum_{y \in \mathcal{S}} q^{(i)} \left( x^{(i)} \right)}
		{p^{(i)} \left( x^{(i)} \right)} \\
	&amp;= \sum_{x^{(i)} \in \mathcal{S}^{n-1}} p^{(i)} \left( x^{(i)} \right) 
	 	\left( \E^{(i)} Z \log \E^{(i)} Z \right) \\
	&amp;= \E \left[ g(\E^{(i)}Z) \right].
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;So, chaining everything together we have that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\E [g(Z)] - g( \E Z) &amp;= D_{\mathrm{kl}}(Q, P) \\
	&amp;\le \sum_{i=1}^n 
	\left( D_{\mathrm{kl}}(Q, P) - 
			D_{\mathrm{kl}} \left( Q^{(i)}, P^{(i)} \right) \right) \\
	&amp;= \sum_{i=1}^n \E \left[ \E^{(i)}g(Z) - g\big( \E^{(i)}Z \big) \right].
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;That‚Äôs it, that‚Äôs all folks.&lt;/p&gt;

&lt;h3 id=&quot;next-up&quot;&gt;Next up&lt;/h3&gt;

&lt;p&gt;‚Ä¶ logarithmic Sobolev inequalities! Stay tuned‚Äîit‚Äôs gonna be good.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is based on material from Chapter 4 of (my new favorite book):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Concentration Inequalities: A Nonasymptotic Theory of Independence&lt;/em&gt; by 
S. Boucheron, G. Lugosi, and P. Masart.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">(Happy St. Patrick‚Äôs Day!)</summary></entry><entry><title type="html">Han‚Äôs inequality</title><link href="http://localhost:4000/jekyll/update/2018/03/11/han.html" rel="alternate" type="text/html" title="Han's inequality" /><published>2018-03-11T23:00:00-06:00</published><updated>2018-03-11T23:00:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/03/11/han</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/03/11/han.html">&lt;p&gt;In this post, we‚Äôre putting on our information theory hats and digging into
to a simple but useful inequality gifted to us by Te Sun Han in 1978. Han‚Äôs
inequality is a generalization of the Efron‚ÄìStein inequality with roots in
information theory rather than the statistical analysis of the jackknife.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;
Before teeing up Han‚Äôs inequality, we‚Äôll cover a few ideas from information 
theory. Andiamo!&lt;/p&gt;

&lt;h3 id=&quot;a-modicum-of-information-theory&quot;&gt;A modicum of information theory&lt;/h3&gt;

&lt;p&gt;We‚Äôll need the idea of (Shannon‚Äôs) &lt;strong&gt;entropy&lt;/strong&gt; in Han‚Äôs inequality, so here it
is for a discrete random variable &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(X) = \E\big( -\log p(X)\big ) = -\sum_{x \in \mathcal{S}} p(x) \log p(x),&lt;/script&gt;

&lt;p&gt;with the standard convention that &lt;script type=&quot;math/tex&quot;&gt;0 \log 0 = 0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Also, we use the definition of &lt;strong&gt;conditional entropy&lt;/strong&gt;, which is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(X | Y) = H(X, Y) - H(Y).&lt;/script&gt;

&lt;p&gt;We can write the conditional entropy as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
H(X | Y) &amp;= -\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} 
			   p(x,y) \log p(x | y) \\
	&amp;= \sum_{y \in \mathcal{Y}} p_Y(y) \left(- \sum_{x \in \mathcal{X}} 
		p(x | y) \log p(x | y) \right) \\
	&amp;= \E [-\log p(X | Y)],		
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where we use the relationship between conditional and joint probability in
the second line.&lt;/p&gt;

&lt;p&gt;Lastly, we‚Äôll use the intuitive idea that &lt;strong&gt;conditioning decreases entropy&lt;/strong&gt;.
This result follows from a relative entropy (&lt;em&gt;i.e.&lt;/em&gt;, Kullback‚ÄìLeibler)
calculation. For two random variables &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;, with marginal 
distributions &lt;script type=&quot;math/tex&quot;&gt;P_X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;P_Y&lt;/script&gt;, and joint distribution &lt;script type=&quot;math/tex&quot;&gt;P_{X,Y}&lt;/script&gt;, the
relative entropy between &lt;script type=&quot;math/tex&quot;&gt;P_{X,Y}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;P_X \otimes P_Y&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
D_{\mathrm{kl}}(P_{X,Y}, P_X \otimes P_Y) 
	&amp;= \sum_{x \in \mathcal{X}, y \in \mathcal{Y}} P_{X,Y}(x,y)
	\log \frac{P_{X,Y}(x,y)}{P_X(x)P_Y(y)} \\
 	&amp;= \sum_{x \in \mathcal{X}, y \in \mathcal{Y}} P_{X,Y}(x,y)
		\log \frac{P_{X|Y}(x|y)P_Y(y)}{P_X(x)P_Y(y)} \\
	&amp;= \sum_{x \in \mathcal{X}, y \in \mathcal{Y}} P_{X,Y}(x,y)
		\left( \log P_{X|Y}(x|y) - \log P_X(x) \right) \\
	&amp;= H(X) - H(X|Y) \ge 0.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since this is greater than 0, we have &lt;script type=&quot;math/tex&quot;&gt;H(X) \ge H(X|Y)&lt;/script&gt;, meaning that
conditioning reduces entropy.&lt;/p&gt;

&lt;h3 id=&quot;hans-inequality&quot;&gt;Han‚Äôs inequality&lt;/h3&gt;

&lt;p&gt;In its simplest form, Han‚Äôs inequality works only with discrete random 
variables. We‚Äôre going to look at that case.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Han‚Äôs inequality.&lt;/strong&gt; Assume &lt;script type=&quot;math/tex&quot;&gt;X_1,\dots,X_n&lt;/script&gt; are discrete random variables.
Then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(X_1,\dots,X_n) \le \frac{1}{n-1} 
	\sum_{i=1}^n H(X_1,\dots,X_{i-1},X_{i+1},\dots,X_n).&lt;/script&gt;

&lt;p&gt;Before proving the inequality, let‚Äôs interpret it. Roughly, it says that the 
joint entropy is less than the average entropy with the $i$th observation
removed. So, we‚Äôre better off (&lt;em&gt;i.e.&lt;/em&gt;, have less uncertainty) if we know 
everything instead of accumulating (averaging) partial information.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof.&lt;/strong&gt; Han‚Äôs inequality relies on two things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;the definition of conditional entropy and&lt;/li&gt;
  &lt;li&gt;the fact that conditioning reduces entropy.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For &lt;script type=&quot;math/tex&quot;&gt;i = 1,\dots,n&lt;/script&gt;, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
H(X_1,\dots,X_n) &amp;\stackrel{\textsf{(a)}}{=} 
	H(X_1,\dots,X_{i-1},X_{i+1},\dots,X_n) + 
	H(X_i|X_1\dots,X_{i-1},X_{i+1},\dots,X_n) \\
	&amp;\stackrel{\textsf{(b)}}{\le} H(X_1,\dots,X_{i-1},X_{i+1},\dots,X_n) + 
	H(X_i|X_1\dots,X_{i-1}),
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where in (a) we used the defintion of conditional entropy and in (b) we used
the fact that conditioning reduces entropy. In other words, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
H(X_1,\dots,X_n) &amp;\le H(X_2,\dots,X_n) + H(X_1) \\
H(X_1,\dots,X_n) &amp;\le H(X_1,X_3,\dots,X_n) + H(X_2|X_1) \\
H(X_1,\dots,X_n) &amp;\le H(X_1,X_2,X_4,\dots,X_n) + H(X_3|X_1, X_2) \\
				 &amp;\,\,\, \vdots \\
H(X_1,\dots,X_n) &amp;\le H(X_1,\dots,X_{n-1}) + H(X_n|X_1\dots,X_{n-1}).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now let‚Äôs sum both sides of the inequality, which gives&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;n H(X_1,\dots,X_n) \le 
	\left( \sum_{i=1}^n H(X_1,\dots,X_{i-1},X_{i+1},\dots,X_n) \right) +
						 H(X_1,\dots,X_n),&lt;/script&gt;

&lt;p&gt;which can be rewritten as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(n-1) H(X_1,\dots,X_n) \le H(X_1,\dots,X_{i-1},X_{i+1},\dots,X_n).&lt;/script&gt;

&lt;p&gt;And that‚Äôs it!&lt;/p&gt;

&lt;h3 id=&quot;next-up&quot;&gt;Next up&lt;/h3&gt;

&lt;p&gt;‚Ä¶ we‚Äôll put Han‚Äôs inequality to work and look at the sub-additivity of 
entropy! Stay tuned.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is based on material from Chapter 4 of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Concentration Inequalities: A Nonasymptotic Theory of Independence&lt;/em&gt; by 
S. Boucheron, G. Lugosi, and P. Masart.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;That said, we now know and appreciate how closely related information theory and statistics really are.¬†&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">In this post, we‚Äôre putting on our information theory hats and digging into to a simple but useful inequality gifted to us by Te Sun Han in 1978. Han‚Äôs inequality is a generalization of the Efron‚ÄìStein inequality with roots in information theory rather than the statistical analysis of the jackknife.1 Before teeing up Han‚Äôs inequality, we‚Äôll cover a few ideas from information theory. Andiamo! That said, we now know and appreciate how closely related information theory and statistics really are.¬†&amp;#8617;</summary></entry><entry><title type="html">Rademachers are rad - part II</title><link href="http://localhost:4000/jekyll/update/2018/03/06/rademacher_2.html" rel="alternate" type="text/html" title="Rademachers are rad - part II" /><published>2018-03-06T20:00:00-07:00</published><updated>2018-03-06T20:00:00-07:00</updated><id>http://localhost:4000/jekyll/update/2018/03/06/rademacher_2</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/03/06/rademacher_2.html">&lt;p&gt;In this post, we‚Äôre going to work through another Rademacher-related problem.
Like the previous post, the problem highlights the usefulness of the 
Efron‚ÄìStein inequality.&lt;/p&gt;

&lt;h3 id=&quot;conditional-rademacher-averages&quot;&gt;Conditional Rademacher averages&lt;/h3&gt;

&lt;p&gt;In this section, we‚Äôre going to continue with a close cousin of the Rademacher
average called the &lt;strong&gt;conditional Rademacher average&lt;/strong&gt;, which are used in high 
dimensional statistics to measure the complexity of model classes. Its defined
as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z = \E \left[ \max_{j=1,\dots,d} \sum_{i=1}^n Y_i X_{i,j} \mid
	X_1,\dots,X_n \right]&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;Y_i&lt;/script&gt; are Rademacher random variables and the &lt;script type=&quot;math/tex&quot;&gt;X_{i,j}&lt;/script&gt; are 
independent random variables that live in &lt;script type=&quot;math/tex&quot;&gt;[-1,1]&lt;/script&gt;. The random variable &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt;
is a function of the &lt;script type=&quot;math/tex&quot;&gt;X_{i} \in [-1,1]^d&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Without lifting a finger, we can call our bound differences inequality friend
and bound the variance of &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Var(Z) \le n/4,&lt;/script&gt;

&lt;p&gt;but that‚Äôs less-than-satisfying at this point in time. Instead, we can improve
on this bound by showing that &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; satisfies the &lt;strong&gt;self-bounding property&lt;/strong&gt;
via the Efron‚ÄìStein inequality.&lt;/p&gt;

&lt;p&gt;First, let‚Äôs define&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z_k = \E \left[ \max_{j=1,\dots,d} 
	\sum_{\substack{i=1 \\ i \neq k}}^n Y_i X_{i,j} \mid X^{(k)}\right]&lt;/script&gt;

&lt;p&gt;and observe that &lt;script type=&quot;math/tex&quot;&gt;0 \le Z - Z_k \le 1&lt;/script&gt; because the maximum value contributed
by &lt;script type=&quot;math/tex&quot;&gt;X_k&lt;/script&gt; to the sum is 1. By summing the inequality and using a convexity
argument (see below), we get that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{k=1}^n Z - Z_k \le Z.&lt;/script&gt;

&lt;p&gt;Now, we can use the Efron‚ÄìStein inequality to say&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\Var(Z) &amp;\le \E \left[ \sum_{k=1}^n (Z - Z_k)^2 \right] \\
	&amp;\le \E \left[ \sum_{k=1}^n (Z - Z_k) \right] 
	\qquad \textsf{(because $(Z - Z_k)^2 \in [0,1]$)}\\
	&amp;\le \E Z,
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;which shows that &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; is a self-bounding function.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is based on material from Chapter 3 of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Concentration Inequalities: A Nonasymptotic Theory of Independence&lt;/em&gt; by 
S. Boucheron, G. Lugosi, and P. Masart.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;self-bounding-property&quot;&gt;Self-bounding property&lt;/h4&gt;

&lt;p&gt;Loosely speaking, the self-bounding property states that the difference 
between a function and a &lt;em&gt;similar&lt;/em&gt; function that omits one variable is bounded
between 0 and 1.&lt;/p&gt;

&lt;p&gt;Formally, a nonnegative function &lt;script type=&quot;math/tex&quot;&gt;f: \mathcal{S}^n \to [0,\infty)&lt;/script&gt; satisfies
the self-bounding property when functions &lt;script type=&quot;math/tex&quot;&gt;f_i: \mathcal{S}^{n-1} \to \reals&lt;/script&gt;
exist and the following two conditions hold:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0 \le f(x_1,\dots,x_n) - f_i(x_1,\dots,x_{i-1},x_{i+1},\dots,x_n) \le 1&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n f(x_1,\dots,x_n) - f_i(x_1,\dots,x_{i-1},x_{i+1},\dots,x_n) 
	\le f(x_1,\dots,x_n)&lt;/script&gt;

&lt;p&gt;for &lt;script type=&quot;math/tex&quot;&gt;i = 1,\dots,n&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x_1,\dots,x_n&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;If we combine the first and second conditions, we can say that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n
\big( f(x_1,\dots,x_n) - f_i(x_1,\dots,x_{i-1},x_{i+1},\dots,x_n) \big)^2 
	\le f(x_1,\dots,x_n),&lt;/script&gt;

&lt;p&gt;which means we can use the Efron‚ÄìStein inequality to bound the variance
by the expected value.&lt;/p&gt;

&lt;h4 id=&quot;convexity-argument&quot;&gt;Convexity argument&lt;/h4&gt;

&lt;p&gt;The convexity argument used in the conditional Rademacher averages example
goes as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\sum_{k=1}^n Z - Z_k &amp;= \sum_{k=1}^n 
\E \left[ \max_{j=1,\dots,d} \sum_{i=1}^n Y_i X_{i,j} 
	\, \middle| \,
	X_1,\dots,X_n \right] - \E \left[ \max_{j=1,\dots,d} 
	\sum_{i \neq k}^n Y_i X_{i,j} 
	\middle| 
	X^{(k)}\right] \\

&amp;= \sum_{k=1}^n \E \left[ \max_{j=1,\dots,d} \sum_{i=1}^n Y_i X_{i,j} -  			 	
	\max_{j=1,\dots,d} \sum_{i \neq k}^n Y_i X_{i,j} 
	\, \middle| \,
	X_1,\dots,X_n \right] \\

&amp;\le \sum_{k=1}^n \E \left[ 
	 Y_i X_{i,j^\star}
	\, \middle| \,
	X_1,\dots,X_n \right] \\

&amp;\le \E \left[ \max_{j=1,\dots,d} \sum_{i=1}^n Y_i X_{i,j} 
	\, \middle| \,
	X_1,\dots,X_n \right] \\

&amp;= Z,
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k^\star = \argmax_{k=1,\dots,d} \sum_{i = 1}^n Y_i X_{i,k}&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0 \le \max_{j=1,\dots,d} \sum_{i=1}^n Y_i X_{i,j} -  			 	
	\max_{j=1,\dots,d} \sum_{i \neq k}^n Y_i X_{i,j}
	= Y_i X_{i,j^\star} - Y_k X_{k,j^\star} \le 1.&lt;/script&gt;

&lt;h4 id=&quot;r-simulation&quot;&gt;R simulation&lt;/h4&gt;

&lt;p&gt;If you want to play around with conditional Rademacher averages, here‚Äôs some
R code to do it.&lt;/p&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;n_trials&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_trials&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EZ&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_trials&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# simulate X variables&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;runif&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrow&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_trials&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_trials&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rbinom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yX&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%*%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colSums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">In this post, we‚Äôre going to work through another Rademacher-related problem. Like the previous post, the problem highlights the usefulness of the Efron‚ÄìStein inequality.</summary></entry><entry><title type="html">Rademachers are rad - part I</title><link href="http://localhost:4000/jekyll/update/2018/03/04/rademacher.html" rel="alternate" type="text/html" title="Rademachers are rad - part I" /><published>2018-03-04T20:00:00-07:00</published><updated>2018-03-04T20:00:00-07:00</updated><id>http://localhost:4000/jekyll/update/2018/03/04/rademacher</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/03/04/rademacher.html">&lt;p&gt;In this post, we‚Äôre going to play with Rademacher random variables, which are
quite useful in many problems that arise in ‚Äúmodern‚Äù statistics and machine 
learning problems. They also pop up in empirical process theory and geometry.
So, it‚Äôs safe to say that we should at least be acquainted with them.&lt;/p&gt;

&lt;p&gt;In case Rademacher random variables are new to you, please meet the Rademacher 
random variable &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;, which takes the values &lt;script type=&quot;math/tex&quot;&gt;+1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;-1&lt;/script&gt; with probability
&lt;script type=&quot;math/tex&quot;&gt;1/2&lt;/script&gt;. For those of you who prefer a more formal introduction, please meet the
probability mass function of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f_Y(y) = 
\begin{cases}
+1, &amp; \textsf{with probability } 0.5 \\
-1, &amp; \textsf{with probability } 0.5.
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;Sometimes Rademacher random variables introduce themselves as &lt;em&gt;random sign 
variables&lt;/em&gt;. Now, that we‚Äôve been properly introduced, let‚Äôs move on to the fun 
stuff.&lt;/p&gt;

&lt;h3 id=&quot;rademacher-averages&quot;&gt;Rademacher averages&lt;/h3&gt;

&lt;p&gt;To build a Rademacher average, we‚Äôll need a collection of real numbers that
are indexed by the number of variables we have &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; and an index set 
&lt;script type=&quot;math/tex&quot;&gt;\mathcal{T}&lt;/script&gt;. We‚Äôll also need some Rademacher variables &lt;script type=&quot;math/tex&quot;&gt;Y_1,\dots,Y_n&lt;/script&gt;.
The &lt;strong&gt;Rademacher average&lt;/strong&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z = \sup_{t \in \mathcal{T}} \sum_{i=1}^n a_{i,t} Y_i.&lt;/script&gt;

&lt;p&gt;The random variable &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; depends on the &lt;script type=&quot;math/tex&quot;&gt;Y_i&lt;/script&gt; in a somewhat complicated way
but we can see that &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; cannot change by much if we changed the value of a
single &lt;script type=&quot;math/tex&quot;&gt;Y_i&lt;/script&gt;. In particular, when we flip the value of &lt;script type=&quot;math/tex&quot;&gt;Y_i&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; can 
only change by &lt;script type=&quot;math/tex&quot;&gt;2\sup_{t \in \mathcal{T}} |a_{i,t}|&lt;/script&gt;. (The 2 comes from the
Rademacher variable and the supremum-absolute value combo reflects the 
‚Äúmaximum‚Äù change.)&lt;/p&gt;

&lt;p&gt;We can use this &lt;strong&gt;bounded differences property&lt;/strong&gt; and the Efron‚ÄìStein inequality
to bound the &lt;script type=&quot;math/tex&quot;&gt;\Var(Z)&lt;/script&gt; even though we can‚Äôt say much about the behavior of 
&lt;script type=&quot;math/tex&quot;&gt;\E Z&lt;/script&gt;, which is pretty rad.&lt;/p&gt;

&lt;p&gt;In particular, we can say that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Var(Z) \le \sum_{i=1}^n \frac{1}{4} \left( 
		 	2\sup_{t \in \mathcal{T}} |a_{i,t}| \right)^2 =
		 	\sum_{i=1}^n \sup_{t \in \mathcal{T}} a_{i,t}^2.&lt;/script&gt;

&lt;p&gt;The inequality results from the bounded differences inequality (which we can
prove using the Efron‚ÄìStein inequality).&lt;/p&gt;

&lt;p&gt;But‚Ä¶ we can do better.&lt;/p&gt;

&lt;h4 id=&quot;sneaking-the-supremum-past-the-sum&quot;&gt;Sneaking the supremum past the sum&lt;/h4&gt;

&lt;p&gt;Now, we define &lt;script type=&quot;math/tex&quot;&gt;X_1,\dots,X_n&lt;/script&gt; as an independent copy of &lt;script type=&quot;math/tex&quot;&gt;Y_1,\dots,Y_n&lt;/script&gt;.
We‚Äôll also set &lt;script type=&quot;math/tex&quot;&gt;t^\star&lt;/script&gt; to be the (possibly random) index that achieves
the supremum, &lt;em&gt;i.e.&lt;/em&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sup_{t \in \mathcal{T}} \sum_{i=1}^n a_{i,t} Y_i 
	= \sum_{i=1}^n a_{i,t^\star} Y_i.&lt;/script&gt;

&lt;p&gt;So, we can bound the difference between &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; with the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th
coordinate changed from &lt;script type=&quot;math/tex&quot;&gt;Y_i&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt;. That is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z - Z_i \le (Y_i - X_i) a_{i,t^\star}
\implies
(Z - Z_i)_+^2 \le (Y_i - X_i)^2 a_{i,t^\star}^2.&lt;/script&gt;

&lt;p&gt;where in the implied inequality we square only the positive portion of
&lt;script type=&quot;math/tex&quot;&gt;Z - Z_i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Now, we‚Äôll use an expectation trick and the independence of &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;
to say&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E \left[ (Z - Z_i)_+^2 \right] \le 
	\E \left[ \E \left( 
		(Y_i - X_i)^2 a_{i,t^\star}^2 \mid Y_1,\dots,Y_n)
	\right) \right] =
	2 \E \left[ a_{i,t^\star}^2 \right].&lt;/script&gt;

&lt;p&gt;We do the expectation-conditioning combo on &lt;script type=&quot;math/tex&quot;&gt;Y_1,\dots,Y_n&lt;/script&gt; to get rid of all
randomness expect for that of &lt;script type=&quot;math/tex&quot;&gt;t^\star&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Finally, we use the Efron‚ÄìStein inequality to say&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Var(Z) \le 2 \E \left( \sum_{i=1}^n a_{i,t^\star}^2 \right) \le 
	2 \sup_{t \in \mathcal{T}} \sum_{i=1}^n a_{i,t}^2.&lt;/script&gt;

&lt;p&gt;So, through some Efron‚ÄìStein inspired analysis, we moved the supremum outside
the sum at the expense of a factor of 2.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is based on material from Chapter 3 of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Concentration Inequalities: A Nonasymptotic Theory of Independence&lt;/em&gt; by 
S. Boucheron, G. Lugosi, and P. Masart.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;bounded-differences-property&quot;&gt;Bounded differences property&lt;/h4&gt;

&lt;p&gt;The bounded differences property states that the value of the function changes
by a bounded amount if the value of the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th variable is changed. A function
&lt;script type=&quot;math/tex&quot;&gt;f: \mathcal{S}^n \to \reals&lt;/script&gt; satisfies the bounded differences property if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sup_{\substack{x_1,\dots,x_n \\ x_i' \in \mathcal{S}}}
	| f(x_1,\dots,x_n) - f_i(x_1,\dots,x_{i-1},x_i',x_{i+1},\dots,x_n) |
	\le c_i,&lt;/script&gt;

&lt;p&gt;for &lt;script type=&quot;math/tex&quot;&gt;i = 1,\dots,n&lt;/script&gt; and constants &lt;script type=&quot;math/tex&quot;&gt;c_1,\dots,c_n&lt;/script&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">In this post, we‚Äôre going to play with Rademacher random variables, which are quite useful in many problems that arise in ‚Äúmodern‚Äù statistics and machine learning problems. They also pop up in empirical process theory and geometry. So, it‚Äôs safe to say that we should at least be acquainted with them.</summary></entry><entry><title type="html">Better bounds: moment vs. Chernoff</title><link href="http://localhost:4000/jekyll/update/2018/02/28/moment-v-chernoff.html" rel="alternate" type="text/html" title="Better bounds: moment vs. Chernoff" /><published>2018-02-28T20:00:00-07:00</published><updated>2018-02-28T20:00:00-07:00</updated><id>http://localhost:4000/jekyll/update/2018/02/28/moment-v-chernoff</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/02/28/moment-v-chernoff.html">&lt;p&gt;In this post, we‚Äôre going to work through a problem from Chapter 2 of
&lt;em&gt;Concentration Inequalities: A Nonasymptotic Theory of Independence&lt;/em&gt;. The exercise 
shows that moment bounds can be tighter than Cram√©r‚ÄìChernoff-type bounds.&lt;/p&gt;

&lt;h3 id=&quot;the-problem&quot;&gt;The problem&lt;/h3&gt;

&lt;p&gt;Our task to show that moment bounds are &lt;strong&gt;always&lt;/strong&gt; better than Cram√©r‚ÄìChernoff bounds.
Assume we have a nonnegative random variable &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; and a positive number &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;. The best
moment bound is &lt;script type=&quot;math/tex&quot;&gt;\prob(Y \ge t) = \min_{q \in \integers} \E(Y^q)t^{-q}&lt;/script&gt;. On the other
hand, the best Cram√©r‚ÄìChernoff bound is &lt;script type=&quot;math/tex&quot;&gt;\inf_{\lambda &gt; 0} \E e^{\lambda(Y-t)}&lt;/script&gt;. So,
our job is to show that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{q \in \integers} \E(Y^q)t^{-q} \le \inf_{\lambda &gt; 0} \E e^{\lambda(Y-t)}.&lt;/script&gt;

&lt;h3 id=&quot;the-proof&quot;&gt;The proof&lt;/h3&gt;

&lt;p&gt;We‚Äôre going to write the &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt;-th moment of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;m^q&lt;/script&gt;. Now, we‚Äôll write the 
moment generating function of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; as a series&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E e^{\lambda Y} = \sum_{k=1}^\infty \frac{\lambda^k m^k}{k!},&lt;/script&gt;

&lt;p&gt;which allows us to write the shifted moment generating function as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E e^{\lambda(Y-t)} = \left. \sum_{k=1}^\infty \frac{\lambda^k m^k}{k!} 
						 \middle/
						\sum_{k=1}^\infty \frac{\lambda^k t^k}{k!}  \right.&lt;/script&gt;

&lt;p&gt;By assumption, &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; is the integer that minimizes &lt;script type=&quot;math/tex&quot;&gt;\E(Y^q)t^{-q}&lt;/script&gt;, which means
that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E(Y^q)t^{-q} = \frac{m^q}{t^q}
	\le \left. \frac{\lambda^k m^k}{k!} \middle/ \frac{\lambda^k t^k}{k!} \right.
	= \frac{m^k}{t^k!} \qquad \textsf{for } t \in \reals_{++} \text{ and } k \in \integers_+.&lt;/script&gt;

&lt;p&gt;At this point, we‚Äôve got the result because this holds for all &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;, in particular, 
when &lt;script type=&quot;math/tex&quot;&gt;k = 0&lt;/script&gt;. So, when we sum up the right-hand-side of the inequality 
(from &lt;script type=&quot;math/tex&quot;&gt;k=0&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;\infty&lt;/script&gt;) and then take the infimum over &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;, we‚Äôre still 
larger than the (optimal) moment bound.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tips and tricks.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For the analysts out there, there‚Äôs a nice result on ratios of series that provides
(precise) justification for the result. Assume that &lt;script type=&quot;math/tex&quot;&gt;\sum_{k=0}^\infty x_k&lt;/script&gt; and 
&lt;script type=&quot;math/tex&quot;&gt;\sum_{k=0}^\infty z_k&lt;/script&gt; are finite and that &lt;script type=&quot;math/tex&quot;&gt;c \le x_k / z_k&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;. Then
it can be shown that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
c \le \frac{\sum_{k=0}^\infty x_k}{\sum_{k=0}^\infty z_k}
	\quad \text{and} \quad
c &lt; \frac{\sum_{k=0}^\infty x_k}{\sum_{k=0}^\infty z_k} %]]&gt;&lt;/script&gt;

&lt;p&gt;when &lt;script type=&quot;math/tex&quot;&gt;x_i / z_i \neq x_j / z_j&lt;/script&gt; for at least one &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; pair.&lt;/p&gt;

&lt;p&gt;We can use this result by setting&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;c = \min_{q \in \integers} \E(Y^q)t^{-q}, \qquad
	x_k = \frac{\lambda^k m^k}{k!}, \qquad 
	z_k = \frac{\lambda^k t^k}{k!},&lt;/script&gt;

&lt;p&gt;and noting that the ratio &lt;script type=&quot;math/tex&quot;&gt;x_k/z_k&lt;/script&gt; changes for different values of &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;final-thoughts&quot;&gt;Final thoughts&lt;/h3&gt;

&lt;p&gt;Although moment bounds can be much tighter than Cram√©r‚ÄìChernoff bounds, working with 
the latter is often (much, much) easier. For example, Cram√©r‚ÄìChernoff bounds play 
nicely with sums of random variables.&lt;/p&gt;

&lt;p&gt;Interestingly, Markov‚Äôs inequality and Chebyshev‚Äôs inequality, &lt;em&gt;i.e.&lt;/em&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\prob(Y \ge t) \le \frac{\E Y}{t} \quad \text{and} \quad
\prob(|Y - \E Y| \ge t) \le \frac{\Var(Y)}{t^2}&lt;/script&gt;

&lt;p&gt;are often not as sharp as Cram√©r‚ÄìChernoff bounds.&lt;/p&gt;

&lt;p&gt;For example, let &lt;script type=&quot;math/tex&quot;&gt;Y \sim \text{Poisson}(\beta)&lt;/script&gt;. The mean of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is 
&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; and the variance is &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;. The moment generating function of the centered
version of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;, &lt;em&gt;i.e.&lt;/em&gt;, &lt;script type=&quot;math/tex&quot;&gt;Z = Y - \beta&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E e^{\lambda Y} = e^{-\lambda \beta - \beta} e^{\beta e^\lambda}.&lt;/script&gt;

&lt;p&gt;The optimal value of &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; which minimizes this expression is 
&lt;script type=&quot;math/tex&quot;&gt;\lambda = \log(1 + t/\beta)&lt;/script&gt;. So, we have that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\inf_{\lambda &gt; 0} \E e^{\lambda(Y-t)} = e^t(1+t/\beta)^{-(\beta+t)}.&lt;/script&gt;

&lt;p&gt;Say &lt;script type=&quot;math/tex&quot;&gt;\beta = 100&lt;/script&gt; and we set &lt;script type=&quot;math/tex&quot;&gt;t = 20&lt;/script&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Markov: 100/20 = 5, which is useless;&lt;/li&gt;
  &lt;li&gt;Chebyshev: 100/400 = 0.25;&lt;/li&gt;
  &lt;li&gt;Cram√©r‚ÄìChernoff: see formula above = 0.153;&lt;/li&gt;
  &lt;li&gt;actual: 0.023.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now turn up &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; to 50.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Markov: 100/50 = 2, which is still useless;&lt;/li&gt;
  &lt;li&gt;Chebyshev: 100/2500 = 0.004;&lt;/li&gt;
  &lt;li&gt;Cram√©r‚ÄìChernoff: &lt;script type=&quot;math/tex&quot;&gt;2\times10^{-5}&lt;/script&gt;;&lt;/li&gt;
  &lt;li&gt;actual: &lt;script type=&quot;math/tex&quot;&gt;1.23\times10^{-6}&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is based on material from:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Concentration Inequalities: A Nonasymptotic Theory of Independence&lt;/em&gt; by 
S. Boucheron, G. Lugosi, and P. Masart.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">In this post, we‚Äôre going to work through a problem from Chapter 2 of Concentration Inequalities: A Nonasymptotic Theory of Independence. The exercise shows that moment bounds can be tighter than Cram√©r‚ÄìChernoff-type bounds.</summary></entry></feed>