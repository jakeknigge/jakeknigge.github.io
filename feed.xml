<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-05-25T22:21:19-06:00</updated><id>http://localhost:4000/</id><title type="html">Eggink Blog</title><subtitle>Jake W. Knigge's blah, blah, blog... a place to discuss statistics, math, and  whatever else comes to mind.
</subtitle><entry><title type="html">Fisherian vignette - part 2</title><link href="http://localhost:4000/jekyll/update/2018/05/26/fisher-2.html" rel="alternate" type="text/html" title="Fisherian vignette - part 2" /><published>2018-05-26T11:30:00-06:00</published><updated>2018-05-26T11:30:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/05/26/fisher-2</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/05/26/fisher-2.html">&lt;p&gt;This post picks up with Fisher’s idea of the &lt;strong&gt;plug-in principle&lt;/strong&gt;. As with the last post,
the point of emphasis is on the connection between &lt;em&gt;maximum likelihood estimates&lt;/em&gt; and
the &lt;em&gt;bootstrap&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;the-plug-in-principle&quot;&gt;The plug-in principle&lt;/h3&gt;

&lt;p&gt;Part of Fisher’s genius is his practicality, and the plug-in principle is a prime example
of his problem solving tactics. Let’s say that we’re doing a linear regression problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_i = \beta^T x_i + \varepsilon_i
      \stackrel{\textsf{notation}}{=} y = X\beta + \varepsilon
      \iff \E [y | x] = X\beta
      \stackrel{\textsf{notation}}{=} \E [y_i | x_i] = \beta^T x_i,&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\beta \in \reals^d&lt;/script&gt; is a vector of parameters to be estimated based off of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;
observations.&lt;/p&gt;

&lt;h4 id=&quot;fisher-style&quot;&gt;Fisher style&lt;/h4&gt;

&lt;p&gt;We’re interested in the standard error of the &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;th component of the
estimated coefficient vector &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathsf{se} \big( \hat{\beta}_k \big) = \sigma_\varepsilon \sqrt{(X^T X)^{-1}_{kk}}.&lt;/script&gt;

&lt;p&gt;Now, here’s where we go &lt;em&gt;Fisherian&lt;/em&gt; on this problem, use our plug-in estimates, and make
sure that everybody who should be wearing a hat &lt;em&gt;is&lt;/em&gt; wearing a hat&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\widehat{\mathsf{se}} \big( \hat{\beta}_k \big)
      = \hat{\sigma}_\varepsilon \sqrt{(X^T X)^{-1}_{kk}}.&lt;/script&gt;

&lt;p&gt;All these quantities can be estimated based on the data &lt;script type=&quot;math/tex&quot;&gt;(X,y)&lt;/script&gt;, which means that we’re
in business.&lt;/p&gt;

&lt;h4 id=&quot;efron-style&quot;&gt;Efron style&lt;/h4&gt;

&lt;p&gt;Now, let’s look at the bootstrap version of these estimates:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\widehat{\mathsf{se}}_\infty \big( \hat{\beta}_k \big)
      = \hat{\sigma}_\varepsilon \sqrt{(X^T X)^{-1}_{kk}},&lt;/script&gt;

&lt;p&gt;where we perform &lt;script type=&quot;math/tex&quot;&gt;B = \infty&lt;/script&gt; bootstrap samples. This results follows the fact that
the variance of a bootstrap estimate &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta}^\star&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Var\left( \hat{\beta}^\star \right) = (X^T X)^{-1} X^T \Var(y^\star) X (X^T X)^{-1}
      = \hat{\sigma}^2_\varepsilon (X^T X)^{-1}.&lt;/script&gt;

&lt;p&gt;After we take the square root, we’re done.&lt;/p&gt;

&lt;h4 id=&quot;take-aways&quot;&gt;Take-aways&lt;/h4&gt;

&lt;p&gt;The bootstrap gives us the same standard error estimates as Fisher’s plug-in estimates,
but it does so by reversing the order of operations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MLE world:&lt;/strong&gt; compute, then plug in.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Compute an approximate formula for the standard error as a function of the unknown
parameters.&lt;/li&gt;
  &lt;li&gt;Plug in the estimates.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Bootstrap world:&lt;/strong&gt; plug in, then compute.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Plug in estimates for the unknown parameters.&lt;/li&gt;
  &lt;li&gt;Compute standard errors via bootstrap sampling (&lt;em&gt;i.e&lt;/em&gt;, simulation).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, where Fisher was clever in his mathematics, the bootstrap is clever in exploiting
cheap computation. Moreover, the bootstrap allows us to side step some (potentially nasty)
mathematics via (lots of) computation.&lt;/p&gt;

&lt;h3 id=&quot;next-up&quot;&gt;Next up&lt;/h3&gt;

&lt;p&gt;… confidence intervals!&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is related to material from:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“R.A. Fisher in the 21st Century” by Bradley Efron.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Computer Age Statistical Inference: Algorithms, Evidence, and Data Science&lt;/em&gt; by
Bradley Efron and Trevor Hastie. A digital copy lives here: &lt;a href=&quot;http://web.stanford.edu/~hastie/CASI/&quot;&gt;CASI&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;An Introduction to the Bootstrap&lt;/em&gt; by Bradley Efron and Robert J. Tibshirani.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">This post picks up with Fisher’s idea of the plug-in principle. As with the last post, the point of emphasis is on the connection between maximum likelihood estimates and the bootstrap.</summary></entry><entry><title type="html">Fisherian vignette - part 1</title><link href="http://localhost:4000/jekyll/update/2018/05/23/fisher-1.html" rel="alternate" type="text/html" title="Fisherian vignette - part 1" /><published>2018-05-23T21:30:00-06:00</published><updated>2018-05-23T21:30:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/05/23/fisher-1</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/05/23/fisher-1.html">&lt;p&gt;Today’s post is our first (short) vignette of the Fisher series. These vignettes are taken
from Brad Efron’s paper and they illustrate Fisher’s continued impact on modern
statistics. For the first one, we’re going to connect the &lt;strong&gt;Fisher information&lt;/strong&gt; with the
bootstrap.&lt;/p&gt;

&lt;p&gt;In what follows, we’re going to assume that we have data (&lt;em&gt;i.e.&lt;/em&gt;, realizations of
random variables) &lt;script type=&quot;math/tex&quot;&gt;x_1,\dots,x_n&lt;/script&gt; from a fixed and unknown distribution with density
function &lt;script type=&quot;math/tex&quot;&gt;f_\theta&lt;/script&gt;. The density is indexed by our &lt;em&gt;parameter-of-interest&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;fisher-information&quot;&gt;Fisher information&lt;/h3&gt;

&lt;p&gt;The &lt;strong&gt;Fisher information&lt;/strong&gt; is the expected value of the second derivative of the
log-density (which is also the derivative of the so called &lt;strong&gt;score function&lt;/strong&gt;):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I_\theta = \E_\theta \left[\frac{-\partial^2 \log f_\theta(x)}{\partial \theta^2}\right].&lt;/script&gt;

&lt;p&gt;The Fisher information for the full sample is &lt;script type=&quot;math/tex&quot;&gt;n I_\theta&lt;/script&gt; because the observations are
(assumed to be) i.i.d.&lt;/p&gt;

&lt;p&gt;So, why do we care about the Fisher information? Well, Fisher showed us that the
asymptotic standard errors of his maximum likelihood estimates (MLEs) are related to the
information in the sample by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathsf{se}_\theta \big( \hat{\theta} \big) = \frac{1}{\sqrt{n I_\theta}}.&lt;/script&gt;

&lt;p&gt;Moreover, by the asymptotic efficiency of MLEs, no other (asymptotically) unbiased
estimator can do better (versus the Cramér–Rao lower bound).&lt;/p&gt;

&lt;p&gt;Okay, that’s cool but there’s a pesky &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; in the denominator. So let’s do what
Fisher would do and replace it by its &lt;em&gt;plug-in&lt;/em&gt; estimate, giving&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\widehat{\mathsf{se}} = \frac{1}{\sqrt{n I_{\hat{\theta}}}}.&lt;/script&gt;

&lt;p&gt;Heck yeah. Now we’ve got something actionable that we can use on MLEs as we please.&lt;/p&gt;

&lt;h3 id=&quot;the-bootstrap&quot;&gt;The bootstrap&lt;/h3&gt;

&lt;p&gt;If you haven’t met the bootstrap yet, do so pronto. It’s an unbelievably useful tool
that every statistician, machine learner, data scientist, computer scientist, and
person-who-works-with-real-data should know, use, and love.&lt;/p&gt;

&lt;p&gt;Here’s the gist of it.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;We compute our statistic of interest &lt;script type=&quot;math/tex&quot;&gt;\hat{\theta} = T(x_1,\dots,x_n)&lt;/script&gt; using the data
we have.&lt;/li&gt;
  &lt;li&gt;Then we resample &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; points from our data &lt;em&gt;with replacement&lt;/em&gt;, which gives us a
&lt;strong&gt;bootstrap sample&lt;/strong&gt;. Some data may appear multiple times and others not at all.&lt;/li&gt;
  &lt;li&gt;From our bootstrap sample, we compute a bootstrap statistic
&lt;script type=&quot;math/tex&quot;&gt;\hat{\theta}^\star_\textsf{boot} = T(x^\star_1,\dots,x^\star_n)&lt;/script&gt;, where
&lt;script type=&quot;math/tex&quot;&gt;x^\star_i&lt;/script&gt; is the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;-th bootstrap sample.&lt;/li&gt;
  &lt;li&gt;(Note that &lt;script type=&quot;math/tex&quot;&gt;x_i \neq x^\star_i&lt;/script&gt; in general.)&lt;/li&gt;
  &lt;li&gt;Do this many times and estimate the standard error,
&lt;script type=&quot;math/tex&quot;&gt;\widehat{\mathsf{se}}_\textsf{boot}&lt;/script&gt; of the statistic by the sample
standard deviation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For a large number of bootstrap samples, the bootstrap standard error estimate is
approximately equal to Fisher’s standard error estimate using the Fisher information.&lt;/p&gt;

&lt;p&gt;The bootstrap approach exploits computer-based computation, which is nice when our
statistic is some nasty function of the data; on the other hand, Fisher’s approach
often exploits clever manipulations, &lt;em&gt;i.e.&lt;/em&gt;, human-based computations to produce standard
error estimates. Both approaches are worth having in your pocket when standard errors
come a-knocking.&lt;/p&gt;

&lt;p&gt;That’s it for today.&lt;/p&gt;

&lt;h3 id=&quot;next-up&quot;&gt;Next up&lt;/h3&gt;

&lt;p&gt;… we’ll dig into the &lt;strong&gt;plug-in principle&lt;/strong&gt; (which we had a flavor of today) for our
second vignette!&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is related to material from:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“R.A. Fisher in the 21st Century” by Bradley Efron.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Computer Age Statistical Inference: Algorithms, Evidence, and Data Science&lt;/em&gt; by
Bradley Efron and Trevor Hastie. A digital copy lives here: &lt;a href=&quot;http://web.stanford.edu/~hastie/CASI/&quot;&gt;CASI&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Today’s post is our first (short) vignette of the Fisher series. These vignettes are taken from Brad Efron’s paper and they illustrate Fisher’s continued impact on modern statistics. For the first one, we’re going to connect the Fisher information with the bootstrap.</summary></entry><entry><title type="html">A few flavors of Fisher’s style</title><link href="http://localhost:4000/jekyll/update/2018/05/22/fisher-0.html" rel="alternate" type="text/html" title="A few flavors of Fisher's style" /><published>2018-05-22T12:30:00-06:00</published><updated>2018-05-22T12:30:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/05/22/fisher-0</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/05/22/fisher-0.html">&lt;p&gt;Today’s post is the first in a sequence about Sir Ronald Aylmer Fisher and his approach to
statistics. These posts are very, very much related to the wonderful 1998 paper by Brad
Efron, titled, “R.A. Fisher in the 21st Century”. A digital copy lives here:
&lt;a href=&quot;https://projecteuclid.org/euclid.ss/1028905930&quot;&gt;R. A. Fisher in the 21st century&lt;/a&gt;. To quote Efron’s last paragraph:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“Let me say finally that Fisher was a genius of the first rank, who has a solid claim to
being the most important applied mathematician of the 20th century. His work has a unique
quality of daring mathematical synthesis combined with the utmost practicality.”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If that doesn’t whet your appetite, then these posts aren’t for you. 😉&lt;/p&gt;

&lt;h3 id=&quot;fisherian-inference&quot;&gt;Fisherian inference&lt;/h3&gt;

&lt;p&gt;Statistics is often divided into two camps, the Bayesian (&lt;em&gt;i.e.&lt;/em&gt;, optimists, integrators,
subjectivists) and the frequentists (&lt;em&gt;i.e.&lt;/em&gt;, pessimists, “derivatators”, objectivists).
Although historically divided, we can related the two camps via decision theory,
but that’s not our purpose here. Instead, we’re going to talk about a third flavor of
inference, &lt;strong&gt;Fisherian inference&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;As Efron and Hastie say, Fisherian inference “often drew on ideas neither Bayesian nor
frequentist in nature, or sometimes the two in combination.” Fisher’s style balanced
the correctness (“coherency”) of Bayesian inference with the the accuracy (“optimality”)
of frequentist inference. Fisher’s style of statistics isn’t simply a convex combination
of the Bayesian and frequentist schools, but something unique with its own merits and
disadvantages. To make things less philosophical and more concrete let’s review
Fisher’s primary tool, and the one that we’re &lt;em&gt;almost surely&lt;/em&gt; used,
&lt;strong&gt;maximum likelihood&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;maximum-likelihood&quot;&gt;Maximum likelihood&lt;/h3&gt;

&lt;p&gt;Assume we have a family of probability distributions indexed by a finite-dimensional
parameter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, &lt;em&gt;e.g.&lt;/em&gt;, exponential family distributions. We’ll write the density
functions as &lt;script type=&quot;math/tex&quot;&gt;f_\theta&lt;/script&gt; and the &lt;strong&gt;log-likelihood&lt;/strong&gt; as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ell_x(\theta) = \log f_\theta(x),&lt;/script&gt;

&lt;p&gt;where the notation &lt;script type=&quot;math/tex&quot;&gt;\ell_x&lt;/script&gt; suggests that the likelihood is fixed with respect to the
data &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;. As a statistician, our goal is to estimate the parameter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. To do so,
we’ll use Fisher’s &lt;strong&gt;maximum likelihood estimate&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta} = \argmax_{\theta \in \Theta} \ell_x(\theta).&lt;/script&gt;

&lt;p&gt;For many distributions of “practical interest”, &lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt; is some subset of the real
numbers and the log-likelihood is a concave function. So, the maximization problem is
well posed (even if there are multiple maximizers). Moreover, maximum likelihood
estimation has achieved “iconic status” because it has&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a straightforward recipe—take derivatives, set them equal to zero, solve;&lt;/li&gt;
  &lt;li&gt;good frequentist properties;&lt;/li&gt;
  &lt;li&gt;a Bayesian interpretation;&lt;/li&gt;
  &lt;li&gt;a notion of optimality; and&lt;/li&gt;
  &lt;li&gt;a natural relationship with information and geometry.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Straightforward recipe.&lt;/strong&gt;
Maximum likelihood estimators are more-or-less automatic to constructs—numerical methods
may be required, but Newton’s method will (often) find the the problem in just a few
iterations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Good frequentist properties.&lt;/strong&gt;
Under “suitable” regularity conditions, which we’ll scoot under the rug, maximum
likelihood estimates are asymptotically normal, asymptotically efficient (&lt;em&gt;i.e.&lt;/em&gt;, optimal,
&lt;em&gt;i.e.&lt;/em&gt;, achieve the Cramér-Rao lower bound), amenable to frequentist style confidence
intervals. Moreover, it is a &lt;strong&gt;consistent&lt;/strong&gt; estimator, meaning the estimator converges
in probability to its true value&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}_n \stackrel{\textsf{prob.}}{\to} \theta \iff
\prob\left( \left| \hat{\theta}_n - \theta \right| &gt; \varepsilon \right) \to 0,&lt;/script&gt;

&lt;p&gt;for every &lt;script type=&quot;math/tex&quot;&gt;\varepsilon &gt; 0&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;n \to \infty&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bayesian interpretation.&lt;/strong&gt;
Maximum likelihood estimates can be viewed as Bayesian posterior estimates when the prior
distribution was uniform (flat) over the parameter space &lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Notion of optimality.&lt;/strong&gt;
As mentioned above, these guys have the smallest asymptotic variance, meaning they use
the data in an optimal way.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Natural relationship with information and geometry.&lt;/strong&gt;
The &lt;strong&gt;Fisher information&lt;/strong&gt; of a random variable quantifies how much information that
variable carries about the parameter of interest; it is closely related to the
information theoretic idea of &lt;strong&gt;relative entropy&lt;/strong&gt;. Moreover, the Fisher information
is a measure of curvature in the log-likelihood, where more curvature means more
information. Cool, huh?&lt;/p&gt;

&lt;h3 id=&quot;next-up&quot;&gt;Next up&lt;/h3&gt;

&lt;p&gt;… our first vignette, connecting the ideas of &lt;strong&gt;Fisher information&lt;/strong&gt; and the
&lt;strong&gt;bootstrap&lt;/strong&gt;!&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is related to material from:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“R.A. Fisher in the 21st Century” by Bradley Efron.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Computer Age Statistical Inference: Algorithms, Evidence, and Data Science&lt;/em&gt; by
Bradley Efron and Trevor Hastie. A digital copy lives here: &lt;a href=&quot;http://web.stanford.edu/~hastie/CASI/&quot;&gt;CASI&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Today’s post is the first in a sequence about Sir Ronald Aylmer Fisher and his approach to statistics. These posts are very, very much related to the wonderful 1998 paper by Brad Efron, titled, “R.A. Fisher in the 21st Century”. A digital copy lives here: R. A. Fisher in the 21st century. To quote Efron’s last paragraph:</summary></entry><entry><title type="html">Online learning theory - strong convexity</title><link href="http://localhost:4000/jekyll/update/2018/05/14/ol-example.html" rel="alternate" type="text/html" title="Online learning theory - strong convexity" /><published>2018-05-14T22:00:00-06:00</published><updated>2018-05-14T22:00:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/05/14/ol-example</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/05/14/ol-example.html">&lt;p&gt;Today’s post is a neat example of what &lt;em&gt;exploiting structure&lt;/em&gt; can do. It ties back to
the post that introduced &lt;a href=&quot;/jekyll/update/2018/05/10/learning-theory-2.html&quot;&gt;online learning&lt;/a&gt;. In that discussion, we had an
online gradient descent algorithm that relied on an auxiliary sequence of models.
We calculated the auxiliary sequence using gradient descent and then we projected back
into our model space, which was all fine and good; however, in this post we’re going to
show that we can avoid that projection step when we have a loss function that is strongly
convex.&lt;/p&gt;

&lt;h3 id=&quot;strong-convexity&quot;&gt;Strong convexity&lt;/h3&gt;

&lt;p&gt;Before we defined strong convexity, try to guess what it could look like. If you imagined
a function that has positive (more precisely, nonnegative) curvature everywhere, then
you’re spot on!&lt;/p&gt;

&lt;p&gt;We’ll assume that our loss function &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt;-strongly, meaning&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l(w) - l(u) \le \nabla l(w)^T (w-u) - \frac{\sigma}{2} \|(w-u)\|^2.&lt;/script&gt;

&lt;p&gt;(Technically, strong convexity is defined with respect to a norm—in this case, it’s
the &lt;script type=&quot;math/tex&quot;&gt;\ell_2&lt;/script&gt; norm.) So, strong convexity means that our function grows faster than
linearly in all directions.&lt;/p&gt;

&lt;h3 id=&quot;online-gradient-descent&quot;&gt;Online gradient descent&lt;/h3&gt;

&lt;p&gt;With strongly convex losses, the online gradient descent algorithm is a one-liner.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Algorithm&lt;/strong&gt; &lt;em&gt;online gradient descent with strongly convex losses.&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;given&lt;/strong&gt; parameters &lt;script type=&quot;math/tex&quot;&gt;\alpha_1,\alpha_2,\dots&lt;/script&gt;, an initial model &lt;script type=&quot;math/tex&quot;&gt;w_1 = 0&lt;/script&gt;, and a
bound on the norm of the gradient, &lt;em&gt;i.e.&lt;/em&gt;, &lt;script type=&quot;math/tex&quot;&gt;\|\nabla l(w)\| \le G&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;repeat&lt;/strong&gt; for &lt;script type=&quot;math/tex&quot;&gt;t=1,2,\dots&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;compute a gradient step:&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{t+1} = w_t - \alpha_t \nabla l_t(w_t).&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;That’s it! Now here’s the cool part. If we know the minimum eigenvalue
&lt;script type=&quot;math/tex&quot;&gt;\sigma = \lambda_\min(H)&lt;/script&gt;, associated with the Hessian of &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;, then we can choose
&lt;script type=&quot;math/tex&quot;&gt;\alpha_t = 1/t\sigma&lt;/script&gt; which gives&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{t=1}^T \big( l_t(w_t) - l_t(u^\star_T) \big) \le \frac{G^2}{2\sigma}
      \frac{\ln T+1}{T}.&lt;/script&gt;

&lt;p&gt;We can interpret this bound as: convergence is fast when we have global information about
the sequence of loss functions. No proof this time (although it’s very similar to the
online gradient descent proof), so that’s it for today!&lt;/p&gt;

&lt;h3 id=&quot;next-up&quot;&gt;Next up&lt;/h3&gt;

&lt;p&gt;… a foray into the mind of R.A. Fisher with Brad Efron’s help!&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is related to material from Nicolò Cesa-Bianchi’s lectures at the Bordeaux
Machine Learning Summer School in 2011. Watch them here: &lt;a href=&quot;http://videolectures.net/mlss2011_cesa_bianchi_learningtheory/&quot;&gt;MLSS 2011 lectures&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">Today’s post is a neat example of what exploiting structure can do. It ties back to the post that introduced online learning. In that discussion, we had an online gradient descent algorithm that relied on an auxiliary sequence of models. We calculated the auxiliary sequence using gradient descent and then we projected back into our model space, which was all fine and good; however, in this post we’re going to show that we can avoid that projection step when we have a loss function that is strongly convex.</summary></entry><entry><title type="html">Learning theory - reconciliations and connections</title><link href="http://localhost:4000/jekyll/update/2018/05/12/learning-theory-3.html" rel="alternate" type="text/html" title="Learning theory - reconciliations and connections" /><published>2018-05-12T22:00:00-06:00</published><updated>2018-05-12T22:00:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/05/12/learning-theory-3</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/05/12/learning-theory-3.html">&lt;p&gt;Howdy learning theorists! Today we’re going to connect statistical learning and online
learning. We’ll do so via model averaging and exploiting online learning’s local
perspective. To that end, this post could have easily been called “the power of local
optimization”, which has a nice ring to it.&lt;/p&gt;

&lt;p&gt;Our goal is address a few questions that pop up when the online learning approach is
unfamiliar.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;What does the online learning regret bound mean?&lt;/li&gt;
  &lt;li&gt;How can that bound be used?&lt;/li&gt;
  &lt;li&gt;How well does the loss rate on the model sequence generalize?&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;the-approach&quot;&gt;The approach&lt;/h3&gt;

&lt;p&gt;Because online learning generates a sequence of models rather than a single model, we
have to pick a model to compare against the empirical risk minimizer. In particular, we’ll
choose the average model. Then we can use the online analysis machinery to provide a
risk bound on that model.&lt;/p&gt;

&lt;p&gt;Assume we have&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;an &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;-dimensional sample &lt;script type=&quot;math/tex&quot;&gt;S = \{(x_1,y_1),\dots,(x_n,y_n)\}&lt;/script&gt; drawn
i.i.d. from a training set &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}&lt;/script&gt;;&lt;/li&gt;
  &lt;li&gt;a loss function &lt;script type=&quot;math/tex&quot;&gt;l_\mathcal{D}(w)&lt;/script&gt; for linear models &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;;&lt;/li&gt;
  &lt;li&gt;a “best” model &lt;script type=&quot;math/tex&quot;&gt;u^\star = \argmin_{u: \|u\| \le U} l_\mathcal{D}(u)&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We want to control the differences&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l_\mathcal{D}(\hat{w}) - l_\mathcal{D}(u^\star),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\hat{w}&lt;/script&gt; is the average model. Here’s how we get &lt;script type=&quot;math/tex&quot;&gt;\hat{w}&lt;/script&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;run online gradient descent on our sample set &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt;;&lt;/li&gt;
  &lt;li&gt;obtain a sequence of models &lt;script type=&quot;math/tex&quot;&gt;w_1,\dots,w_n&lt;/script&gt;;&lt;/li&gt;
  &lt;li&gt;compute the average model&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{w} = \frac{1}{n} \sum_{i=1}^n w_i.&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Aside on averaging.&lt;/strong&gt; Online analysis gives insight on the behavior of the model
sequence but not on any one particular model. So, we need to consolidate our sequence
into something we can analyze. Moreover, averaging is a good way (think of Leo Breiman
and &lt;strong&gt;bagging&lt;/strong&gt;) to reduce variance without impacting an estimator’s bias.&lt;/p&gt;

&lt;h3 id=&quot;expectation-bounds&quot;&gt;Expectation bounds&lt;/h3&gt;

&lt;p&gt;We start by showing that the expected value of the loss of the average model is less
than or equal to the average loss on the model sequence via Jensen’s inequality:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
l_\mathcal{D}\big( \hat{w} \big)
	&amp;= \E \left[ l\left( \frac{1}{n} \sum_{t=1}^n w_t, (X,Y) \right) \right] \\
	&amp;\le \frac{1}{n} \sum_{t=1}^n \E \, l\big(w_t, (X,Y) \big) \\
	&amp;= \frac{1}{n} \sum_{t=1}^n l_\mathcal{D}(w_t).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Cool, now we have something to compare to the best model in the class! But something feels
funny because we have something i.i.d. and something sequential—martingales to the
rescue!&lt;/p&gt;

&lt;h3 id=&quot;martingale-differences&quot;&gt;Martingale differences&lt;/h3&gt;

&lt;p&gt;Martingales are perfectly equipped for taming our sequential process. In particular,
we’ll define the &lt;strong&gt;conditional expectation&lt;/strong&gt; up to time &lt;script type=&quot;math/tex&quot;&gt;t-1&lt;/script&gt; as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E_{t-1} [\, \cdot \,] = \E[\, \cdot \mid z_1,\dots,z_{t-1}].&lt;/script&gt;

&lt;p&gt;For now, let’s focus on time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; and take the expectation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E_{t-1} \big[ \color{red}{l_\mathcal{D}(w_t)}
	- \color{royalblue}{l\left(w_t, (x_t, y_t) \right)} \big] = 0&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\color{red}{l_\mathcal{D}(w_t)}&lt;/script&gt; is the &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;th model and
&lt;script type=&quot;math/tex&quot;&gt;\color{royalblue}{l\left(w_t, (x_t, y_t) \right)}&lt;/script&gt; is the expected loss on the next
example. The expectation is zero because&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the first &lt;script type=&quot;math/tex&quot;&gt;t-1&lt;/script&gt; examples are held fixed, meaning &lt;script type=&quot;math/tex&quot;&gt;l_\mathcal{D}(w_t)&lt;/script&gt; is a constant;
and&lt;/li&gt;
  &lt;li&gt;the expected loss, &lt;em&gt;i.e.&lt;/em&gt;, risk, at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; is
&lt;script type=&quot;math/tex&quot;&gt;\E \, \color{royalblue}{l\left(w_t, (x_t, y_t) \right)} = l_\mathcal{D}(w_t)&lt;/script&gt; because
our data is i.i.d. from &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, we average across our &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; examples&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{n} \sum_{t=1}^n
	\E_{t-1} \big[ l_\mathcal{D}(w_t) - l\left(w_t, (x_t, y_t) \right) \big]
	\stackrel{\textsf{notation}}{=}
	\frac{1}{n} \sum_{t=1}^n \E_{t-1} Z_t = 0&lt;/script&gt;

&lt;p&gt;because &lt;script type=&quot;math/tex&quot;&gt;\E_{t-1} Z_t = 0&lt;/script&gt; for each &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;. The random variables &lt;script type=&quot;math/tex&quot;&gt;Z_1,\dots,Z_n&lt;/script&gt; form
a &lt;strong&gt;martingale difference sequence&lt;/strong&gt;, which means we can martingale concentration results
to bound the average. In particular, with probability at least &lt;script type=&quot;math/tex&quot;&gt;1 - \delta&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{n} \sum_{t=1}^n Z_t \le \E \frac{1}{n} \sum_{t=1}^n Z_t +
	\mathcal{O} \left( \sqrt{\frac{1}{n} \ln \frac{1}{\delta}} \right).&lt;/script&gt;

&lt;p&gt;With this fact, we can bound the risk of the average model!&lt;/p&gt;

&lt;h3 id=&quot;bounding-via-online-analysis&quot;&gt;Bounding via online analysis&lt;/h3&gt;

&lt;p&gt;Thanks to our martingale convergence result, this is a plug-and-play step. So, with
probability at least &lt;script type=&quot;math/tex&quot;&gt;1- \delta&lt;/script&gt; with respect to the random draw of our sample &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
l_\mathcal{D}(\hat{w}) &amp;\le \frac{1}{n} \sum_{t=1}^n l_\mathcal{D}(w_t) \\
	&amp;\le \frac{1}{n} \sum_{t=1}^n l\big(w_t, (x_t, y_t)\big) +
	\mathcal{O} \left( \sqrt{\frac{1}{n} \ln \frac{1}{\delta}} \right).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Neural connection.&lt;/strong&gt; This is the same object as what we had in the online learning
analysis because we have a loss rate on the first &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; models in the sequence—and
all we needed was a slightly modified (&lt;em&gt;i.e.&lt;/em&gt;, &lt;em&gt;martingalified&lt;/em&gt;) concentration result!&lt;/p&gt;

&lt;p&gt;Our last step is to use online analysis to bound the loss rate&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{n} \sum_{t=1}^n l\big(w_t, (x_t, y_t)\big),&lt;/script&gt;

&lt;p&gt;which will give us something that we can compare to the empirical risk minimizer. To make
things concrete, we’ll look at a specific example.&lt;/p&gt;

&lt;h3 id=&quot;linear-regression-with-squared-loss&quot;&gt;Linear regression with squared loss&lt;/h3&gt;

&lt;p&gt;We’re going to show that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{n} \sum_{t=1}^n l\big(w_t, (x_t, y_t)\big) \le
	\min_{u: \|u\| \le U} \frac{1}{n} \sum_{t=1}^n l\big(u, (x_t, y_t)\big) +
		\delta (UX)^2 \sqrt{\frac{2}{T}}.&lt;/script&gt;

&lt;p&gt;We’ll define &lt;script type=&quot;math/tex&quot;&gt;u^\star_t&lt;/script&gt; as the minimizer of
&lt;script type=&quot;math/tex&quot;&gt;\frac{1}{n} \sum_{t=1}^n l\big(u, (x_t, y_t)\big)&lt;/script&gt;, which allows us to say that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{n} \sum_{t=1}^n l\big(u_n^\star, (x_t, y_t)\big) \le
	\frac{1}{n} \sum_{t=1}^n l\big(u^\star, (x_t, y_t)\big),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;u^\star&lt;/script&gt; is the model with the smallest statistical risk over &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}&lt;/script&gt;,
not just &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt;. So, &lt;script type=&quot;math/tex&quot;&gt;u^\star&lt;/script&gt; may not have the smallest risk over &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt;.
(&lt;strong&gt;Neural connection!&lt;/strong&gt; This is the same trick we used in the Rademacher analysis in the
&lt;a href=&quot;/jekyll/update/2018/05/06/learning-theory-1.html&quot;&gt;statistical learning theory post&lt;/a&gt;!)&lt;/p&gt;

&lt;p&gt;Via the Chernoff bound, with probability at least &lt;script type=&quot;math/tex&quot;&gt;1-\delta&lt;/script&gt; we have that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{n} \sum_{t=1}^n l\big(u_n^\star, (x_t, y_t)\big) \le
	l_\mathcal{D}(u^\star) +
	\mathcal{O} \left( \sqrt{\frac{1}{n} \ln \frac{1}{\delta}} \right).&lt;/script&gt;

&lt;p&gt;This last expression yields the bound on the average model:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l_\mathcal{D}(\hat{w}) - l_\mathcal{D}(u^\star) \le + c\frac{UX^2}{\sqrt{n}}
	\mathcal{O} \left( \sqrt{\frac{1}{n} \ln \frac{1}{\delta}} \right),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt; is a constant. This is the same bound obtained from the Rademacher analysis!&lt;/p&gt;

&lt;h3 id=&quot;final-thoughts&quot;&gt;Final thoughts&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The average loss on a sequence of models is a good proxy for the risk of the average
model.&lt;/li&gt;
  &lt;li&gt;Local optimization performs well (up to constant factors), compared to empirical risk
minimization, &lt;em&gt;i.e.&lt;/em&gt;, globabl optimization.
 	+ So, local optimization isn’t doing anything crazy—it’s doing something quite
    similar to global optimization.
    &lt;ul&gt;
      &lt;li&gt;In other words, doing local optimization and then averaging is a good and viable
strategy, especially when we’re dealing with big problems.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, let’s revisit our questions.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;What does the online learning regret bound mean?
    &lt;ul&gt;
      &lt;li&gt;In some sense, it means that we’re bounding an object not unlike the empirical
risk.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;How can that bound be used?
    &lt;ul&gt;
      &lt;li&gt;The online learning bound can be used to bound an &lt;em&gt;average&lt;/em&gt; model, which is
comparable to a model found via empirical risk minimization.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;How well does the loss rate on the model sequence generalize?
    &lt;ul&gt;
      &lt;li&gt;Up to constant factors, it generalizes as well as well the model found through
empirical risk minimization.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;next-up&quot;&gt;Next up&lt;/h3&gt;

&lt;p&gt;… a short and sweet example of how strong convexity speeds things up in the online
learning setting! Then a foray into the mind of R.A. Fisher with Brad Efron’s help.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is related to material from Nicolò Cesa-Bianchi’s lectures at the Bordeaux
Machine Learning Summer School in 2011. Watch them here: &lt;a href=&quot;http://videolectures.net/mlss2011_cesa_bianchi_learningtheory/&quot;&gt;MLSS 2011 lectures&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">Howdy learning theorists! Today we’re going to connect statistical learning and online learning. We’ll do so via model averaging and exploiting online learning’s local perspective. To that end, this post could have easily been called “the power of local optimization”, which has a nice ring to it.</summary></entry><entry><title type="html">Learning theory - online learning</title><link href="http://localhost:4000/jekyll/update/2018/05/10/learning-theory-2.html" rel="alternate" type="text/html" title="Learning theory - online learning" /><published>2018-05-10T22:00:00-06:00</published><updated>2018-05-10T22:00:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/05/10/learning-theory-2</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/05/10/learning-theory-2.html">&lt;p&gt;Welcome back! Today’s post focuses on the &lt;strong&gt;online learning&lt;/strong&gt; perspective of learning.
The online learning approach differs from statistical learning because it makes no
assumption on the data source. In other words, the process generating the data is
arbitrary!&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; Instead of focusing on the data source, online learning makes assumptions
on the sequence of loss functions to attain some level of tractability. Yep, you read
that correctly: we will have a sequence of loss functions—not just one to
&lt;em&gt;rule them all&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;a-protocol-for-online-learning&quot;&gt;A protocol for online learning&lt;/h3&gt;

&lt;p&gt;Here’s how our online learning world works.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An algorithm generates an initial model &lt;script type=&quot;math/tex&quot;&gt;w_1&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;At each time &lt;script type=&quot;math/tex&quot;&gt;t = 1, 2, \dots&lt;/script&gt;, the model is tested on a data point.&lt;/li&gt;
  &lt;li&gt;After being tested, the algorithm updates &lt;script type=&quot;math/tex&quot;&gt;w_t \to w_{t+1}&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The second step suggests that we’re going to need some notion of loss—&lt;em&gt;i.e.&lt;/em&gt;, something
like risk—to help us &lt;em&gt;test&lt;/em&gt; our model. For our purposes, we’ll assume that
the loss functions we encounter are &lt;strong&gt;convex&lt;/strong&gt;, &lt;strong&gt;nonnegative&lt;/strong&gt;, and &lt;strong&gt;differentiable&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;For application-minded readers, model spaces are typically linear but can be finite
dimensional (&lt;em&gt;e.g.&lt;/em&gt;, regression or support vector machine parameters) or infinite
dimensional, via reproducing Kernel Hilbert spaces.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h3 id=&quot;assessing-performance&quot;&gt;Assessing performance&lt;/h3&gt;

&lt;p&gt;We’re going to assess the performance of our (model-generating) algorithm via its
&lt;strong&gt;loss rate&lt;/strong&gt;, defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{t=1}^T l_t(w_t).&lt;/script&gt;

&lt;p&gt;But this is only part of the story. A loss rate by itself is uninteresting because
losses are arbitrary. So, we’ll focus on controlling our &lt;strong&gt;regret&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{t=1}^T l_t(w_t) - l_t(w^\star_T),&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w^\star_T = \argmin_{u: \|u\| \le U} \frac{1}{T} \sum_{t=1}^T l_t(u)&lt;/script&gt;

&lt;p&gt;is the best model in the class. When the regret goes to zero, then we are learning and
also experiencing some type of consistency.&lt;/p&gt;

&lt;h3 id=&quot;the-algorithm-of-choice---online-gradient-descent&quot;&gt;The algorithm of choice - online gradient descent&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Online gradient descent&lt;/strong&gt; (OGD) is akin to empirical risk minimization in statistical
learning theory, but it’s a bit different than typical gradient descent because the losses
can vary over time. Nonetheless, the algorithm is quite simple.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Algorithm&lt;/strong&gt; &lt;em&gt;online gradient descent.&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;given&lt;/strong&gt; parameters &lt;script type=&quot;math/tex&quot;&gt;\alpha_1,\alpha_2,\dots&lt;/script&gt;, a radius &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;, and an initial model
&lt;script type=&quot;math/tex&quot;&gt;w_1 = 0&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;repeat&lt;/strong&gt; for &lt;script type=&quot;math/tex&quot;&gt;t=1,2,\dots&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;compute a tentative gradient step:&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\tilde{w}_{t+1} = w_t - \alpha_t \nabla l_t(w_t).&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;project back to the model space:&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{t+1} = \argmin_{w: \|w\| \le U} \|w - \tilde{w}_{t+1}\|.&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Neural connection.&lt;/strong&gt; Online gradient descent is similar to stochastic gradient except
there’s no stochasticity in the data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Food for thought.&lt;/strong&gt;
Online gradient descent is a local optimization perspective rather than global
optimization as in empirical risk minimization. Because of this, online gradient descent
scales gracefully to large problems.&lt;/p&gt;

&lt;h3 id=&quot;analyzing-online-gradient-descent&quot;&gt;Analyzing online gradient descent&lt;/h3&gt;

&lt;p&gt;Let’s fix a time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; and analyze the instantaneous regret&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l_t(w_t) - l_t(u).&lt;/script&gt;

&lt;p&gt;By convexity, the first-order Taylor expansion of &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt; around &lt;script type=&quot;math/tex&quot;&gt;w_t&lt;/script&gt; gives,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l_t(w_t) - l_t(u) \le \nabla l_t (w_t)^T (w_t - u).&lt;/script&gt;

&lt;p&gt;This is pretty much the definition of convexity.&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; By the OGD algorithm, we
calculate&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
l_t(w_t) - l_t(u)
&amp;\le \nabla l_t (w_t)^T (w_t - u) \\
&amp;= \frac{1}{\alpha_t} (\tilde{w}_{t+1} - w_t)^T (w_t - u) \\
&amp;\stackrel{\textsf{(a)}}{=}
	\frac{1}{\alpha_t} \left( \frac{1}{2} \|w_t - u\|^2
		- \frac{1}{2} \|\tilde{w}_{t+1} - u\|^2
		+ \frac{1}{2} \|\tilde{w}_{t+1} - w_t\|^2 \right) \\
&amp;\stackrel{\textsf{(b)}}{\le}
	\frac{1}{\alpha_t} \left( \frac{1}{2} \|w_t - u\|^2
		- \frac{1}{2} \|w_{t+1} - u\|^2
		+ \frac{1}{2} \|\tilde{w}_{t+1} - w_t\|^2 \right) \\
&amp;\stackrel{\textsf{(c)}}{=}
	\frac{1}{2\alpha_t} \|w_t - u\|^2
		- \frac{1}{2\alpha_{t+1}} \|w_{t+1} - u\|^2
		- \frac{1}{2\alpha_t} \|w_{t+1} - u\|^2 \\
		&amp;\qquad + \frac{1}{2\alpha_{t+1}} \|w_{t+1} - u\|^2
		+ \frac{1}{2\alpha_t} \|\tilde{w}_{t+1} - w_t\|^2.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;In (a) we compare the current model at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; to the best model &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt; plus a
penalty for changing models. In (b) we use the fact that projecting
&lt;script type=&quot;math/tex&quot;&gt;\tilde{w}_{t+1} \to w_{t+1}&lt;/script&gt; reduces the distance to &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt;; since that term is
negative, we get the inequality. In (c) we add and subtract the same term for reasons
which will become clear shortly.&lt;/p&gt;

&lt;h4 id=&quot;summing-it-up&quot;&gt;Summing it up&lt;/h4&gt;

&lt;p&gt;Now, we need to sum over &lt;script type=&quot;math/tex&quot;&gt;t = 1, 2,\dots&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\sum_{t=1}^T l_t(w_t) - l_t(u) &amp;\le
	\frac{1}{2\alpha_1} \|w_1 - u\|^2 -
	\frac{1}{2\alpha_{T+1}} \|w_{T+1} - u\|^2
		\\ &amp;\qquad +
	\frac{1}{2} \sum_{t=1}^T \|w_{t+1} - u\|^2 \left( \frac{1}{\alpha_{t+1}} -
		\frac{1}{\alpha_t} \right)
		\\ &amp;\qquad +
	\frac{1}{2} \sum_{t=1}^T \frac{1}{\alpha_t} \|\tilde{w}_{t+1} - w_t\|^2,
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;w_1 = 0&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\|w_{t+1} - u\|^2 \le 4U^2&lt;/script&gt;, and
&lt;script type=&quot;math/tex&quot;&gt;\|\tilde{w}_{t+1} - w_t\|^2 = \alpha_t^2 \|\nabla l_t(w_t)\|^2&lt;/script&gt;. Continuing from our
last line and using &lt;script type=&quot;math/tex&quot;&gt;\|\nabla l_t(w_t)\| \le G&lt;/script&gt;, we calculate&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\sum_{t=1}^T l_t(w_t) - l_t(u) &amp;\le
	\frac{U^2}{2\alpha_1} - \frac{1}{2\alpha_{T+1}} \|w_{T+1} - u\|^2
		\\ &amp;\qquad
	+ 2U^2 \sum_{t=1}^{T-1}
		\left( \frac{1}{\alpha_{t+1}} - \frac{1}{\alpha_t} \right)
		\\ &amp;\qquad
	+ \frac{1}{2\alpha_{T+1}} \|w_{T+1} - u\|^2
	- \frac{1}{2\alpha_T} \|w_T - u\|^2
		\\ &amp;\qquad
	+ \frac{G^2}{2} \sum_{t=1}^T \alpha_t,
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;by removing the last two terms,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{2\alpha_{T+1}} \|w_{T+1} - u\|^2 \quad \textsf{and} \quad
\frac{1}{2\alpha_T} \|w_T - u\|^2,&lt;/script&gt;

&lt;p&gt;from the summation term involving &lt;script type=&quot;math/tex&quot;&gt;\alpha_{t+1}^{-1} - \alpha_t^{-1}&lt;/script&gt;.
The &lt;script type=&quot;math/tex&quot;&gt;\|w_{T+1} - u\|^2&lt;/script&gt; terms cancel each other and we throw about the &lt;script type=&quot;math/tex&quot;&gt;\|w_T - u\|^2&lt;/script&gt;
term since it’s negative, leaving us with&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\sum_{t=1}^T l_t(w_t) - l_t(u) &amp;\le
	\frac{U^2}{2\alpha_1} + 2U^2 \sum_{t=1}^{T-1}
		\left( \frac{1}{\alpha_{t+1}} - \frac{1}{\alpha_t} \right)
	+ \frac{G^2}{2} \sum_{t=1}^T \alpha_t \\
&amp;\stackrel{\textsf{(a)}}{\le}
	\frac{2U^2}{\alpha_T} + \frac{G^2}{2} \sum_{t=1}^T \alpha_t,
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where we use telescoping and the &lt;em&gt;throw-away-a-negative-term&lt;/em&gt; trick to get the
inequality (a).&lt;/p&gt;

&lt;h4 id=&quot;putting-it-all-together&quot;&gt;Putting it all together&lt;/h4&gt;

&lt;p&gt;We can pick our &lt;script type=&quot;math/tex&quot;&gt;\alpha_t&lt;/script&gt; terms to equate terms in the sum. Specifically,
we choose them as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_t = \frac{\alpha}{\sqrt{t}}.&lt;/script&gt;

&lt;p&gt;So, choosing &lt;script type=&quot;math/tex&quot;&gt;\alpha_t&lt;/script&gt; as above, selecting &lt;script type=&quot;math/tex&quot;&gt;\alpha = U\sqrt{2}/G&lt;/script&gt;, and dividing by
&lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;, our final bound, &lt;em&gt;i.e.&lt;/em&gt;, our &lt;strong&gt;rate&lt;/strong&gt;, is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\frac{1}{T} \sum_{t=1}^T l_t(w_t) - l_t(u)
	&amp;\le \frac{2U^2}{\alpha\sqrt{T}} + \frac{\alpha G^2}{\sqrt{T}} \\
	&amp;= UG \sqrt{\frac{8}{T}}.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;linear-regression-with-squared-loss&quot;&gt;Linear regression with squared loss&lt;/h3&gt;

&lt;p&gt;Let’s do linear regression with squared loss using the assumptions&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;G = 4UX^2&lt;/script&gt;,&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\|x_t\| \le X&lt;/script&gt;, and&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\|y\| \le UX&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Under this set up our regret is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;8UX^2\sqrt{2/T}&lt;/script&gt;

&lt;p&gt;since &lt;script type=&quot;math/tex&quot;&gt;\nabla l(w) = 2(w^T x - y)x&lt;/script&gt;.
(The norm bound on &lt;script type=&quot;math/tex&quot;&gt;\nabla l(w)&lt;/script&gt; makes use of the Cauchy–Schwarz inequality.)
Now, hold on a sec… this is the same rate as the Rademacher complexity for the
statistical learning approach!&lt;/p&gt;

&lt;h3 id=&quot;next-up&quot;&gt;Next up&lt;/h3&gt;

&lt;p&gt;… reconciling and connecting statistical learning and online learning!&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is related to material from Nicolò Cesa-Bianchi’s lectures at the Bordeaux
Machine Learning Summer School in 2011. Watch them here: &lt;a href=&quot;http://videolectures.net/mlss2011_cesa_bianchi_learningtheory/&quot;&gt;MLSS 2011 lectures&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;If you’re like me, with more of stats background, then this might be an
“aha!” moment. If it is, take a moment to enjoy it. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;Don’t worry if you’ve never heard of reproducing Kernel Hilbert spaces. They won’t
show up here. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;The inequality may look more familiar as
&lt;script type=&quot;math/tex&quot;&gt;l_t(w_t) - \nabla l_t (w_t)^T w_t \le l_t(u) - \nabla l_t (w_t)^T u&lt;/script&gt;. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">Welcome back! Today’s post focuses on the online learning perspective of learning. The online learning approach differs from statistical learning because it makes no assumption on the data source. In other words, the process generating the data is arbitrary!1 Instead of focusing on the data source, online learning makes assumptions on the sequence of loss functions to attain some level of tractability. Yep, you read that correctly: we will have a sequence of loss functions—not just one to rule them all. If you’re like me, with more of stats background, then this might be an &amp;#8617;</summary></entry><entry><title type="html">Statistical learning theory - complexity example</title><link href="http://localhost:4000/jekyll/update/2018/05/07/slt-example.html" rel="alternate" type="text/html" title="Statistical learning theory - complexity example" /><published>2018-05-07T22:00:00-06:00</published><updated>2018-05-07T22:00:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/05/07/slt-example</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/05/07/slt-example.html">&lt;p&gt;This post illustrates how we use Rademacher complexities in statistical learning theory.
To that end, we’ll assume that we’re working with our toolbox developed in the
&lt;a href=&quot;/jekyll/update/2018/05/06/learning-theory-1.html&quot;&gt;last post&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;a-quick-recap&quot;&gt;A quick recap&lt;/h3&gt;

&lt;p&gt;Last post, our main result was that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E_S \left[\sup_{f \in \mathcal{F}} \big\{ R_{\mathcal{D}}(f) - R_n(f) \big\}\right] \le
	2 \E_S \E_\sigma \left[ \sup_{f \in \mathcal{F}} \frac{1}{n}
					\sum_{i=1}^n \sigma_i l_f(z_i) \right],&lt;/script&gt;

&lt;p&gt;which allowed us to bound the statistical risk as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_{\mathcal{D}}(f_n^\star) - R_{\mathcal{D}}(f^\star) \le
	2 \E_S \E_\sigma \left[ \sup_{f \in \mathcal{F}} \frac{1}{n}
					\sum_{i=1}^n \sigma_i l_f(z_i) \right] +
	\mathcal{O} \left( \sqrt{\frac{1}{n} \ln \frac{1}{\delta}} \right),&lt;/script&gt;

&lt;p&gt;with at least probability &lt;script type=&quot;math/tex&quot;&gt;1-\delta&lt;/script&gt;. In other words, Rademacher complexity helps
us measure the tendency of a model class to overfit a sample.&lt;/p&gt;

&lt;h3 id=&quot;warm-up&quot;&gt;Warm up&lt;/h3&gt;

&lt;p&gt;Let’s Rademacherize our minds with a straightforward application of last post’s bound.
We’ll assume that our loss functions are &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt;-Lipschitz. With this and this alone, we
can show that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E_\sigma \left[ \sup_{f \in \mathcal{F}} \frac{1}{n}
	\sum_{i=1}^n \sigma_i l_f(z_i) \right] \le
	L \E_\sigma \left[ \sup_{f \in \mathcal{F}} \frac{1}{n}
		\sum_{i=1}^n \sigma_i f(x_i) \right].&lt;/script&gt;

&lt;p&gt;This is cool for at least two reasons. The first is that the righthand side doesn’t
depend on &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; anymore—only &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;. The second is that the expectation no longer
depends on &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;, but only on &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;linear-regression-with-squared-loss&quot;&gt;Linear regression with squared loss&lt;/h3&gt;

&lt;p&gt;To make these ideas concrete, we’ll look at one of the most commone models arising in
statistics / machine learning: ordinary linear regression. Here’s our set up.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Our class of functions is &lt;script type=&quot;math/tex&quot;&gt;\mathcal{F} = \{\theta \mid x \mapsto \theta^T x\}&lt;/script&gt; where
&lt;script type=&quot;math/tex&quot;&gt;\|\theta\| \le B_\theta&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\|x\| \le B_x&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;By assumption on &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, we have &lt;script type=&quot;math/tex&quot;&gt;\|y\| \le B_\theta B_x&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Our loss function &lt;script type=&quot;math/tex&quot;&gt;l(\theta^T x, y) = (\theta^T x - y)^2&lt;/script&gt;, with &lt;script type=&quot;math/tex&quot;&gt;L = 4B_\theta B_x&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’re interested in the Rademacher complexity of our function class on &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; samples
&lt;script type=&quot;math/tex&quot;&gt;S = \{(x_1,y_1),\dots,(x_n,y_n)\}&lt;/script&gt;, &lt;em&gt;i.e.&lt;/em&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathcal{R}(\mathcal{F}, S) &amp;= \E_\sigma \left[ \sup_{\theta: \theta \le B_\theta}
 	\frac{1}{n} \sum_{i=1}^n \sigma_i \theta^T x_i \right] \\
	&amp;= \frac{1}{n} \E_\sigma \left[ \sup_\theta \theta^T \left(
		\sum_{i=1}^n \sigma_i x_i \right) \right]
		&amp;&amp; {\small \textsf{(linearity)}} \\
	&amp;= \frac{1}{n} \E_\sigma \left[ B_\theta \left\|
		\sum_{i=1}^n \sigma_i x_i \right\| \right]
		&amp;&amp; {\small \textsf{(bound on $\theta$)}} \\
	&amp;= \frac{B_\theta}{n} \E_\sigma \sqrt{ \left\|
		\sum_{i=1}^n \sigma_i x_i \right\|^2 }
		&amp;&amp; {\small \textsf{(square-square root trick)}} \\
	&amp;\le \frac{B_\theta}{n} \sqrt{\E_\sigma \left\|
		\sum_{i=1}^n \sigma_i x_i \right\|^2}
		&amp;&amp; {\small \textsf{(Jensen for concave functions)}} \\
	&amp;\le \frac{B_\theta}{n} \sqrt{n B_x^2}
		&amp;&amp; {\small \textsf{(independence of $\sigma_i$ and $\|\sigma_i\|=1$)}}\\
	&amp;= \frac{B_\theta B_x}{\sqrt{n}} \\
	&amp;= \frac{\textsf{size of model class}}{\textsf{convergence rate}}.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The trickiest step is the one that exploits the independence of the Rademacher
random variables and the bound on their norm. It’s easy to see if we write out
the double sum&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_k \sum_j \sigma_k \sigma_j x_k^T x_j = \sum_k \sigma_k^2 x_k^T x_k +
	\sum_{j \neq k} \sigma_j \sigma_k x_j^T x_k = \sum_k x_k^T x_k,&lt;/script&gt;

&lt;p&gt;because the &lt;script type=&quot;math/tex&quot;&gt;\sigma_j&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\sigma_k&lt;/script&gt; are independent.&lt;/p&gt;

&lt;h3 id=&quot;putting-it-all-together&quot;&gt;Putting it all together&lt;/h3&gt;

&lt;p&gt;From last post and the above calculation, we conclude that with probability at least
&lt;script type=&quot;math/tex&quot;&gt;1 - \delta&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
R_{\mathcal{D}}(f_n^\star) - R_{\mathcal{D}}(f^\star) &amp;\le
	\frac{8(B_\theta B_x)^2}{\sqrt{n}} +
	\mathcal{O}\left( \sqrt{\frac{1}{m} \ln \frac{1}{\delta}} \right) \\
	&amp;=\mathcal{R}(\mathcal{F}, S) L +
	\mathcal{O}\left( \sqrt{\frac{1}{m} \ln \frac{1}{\delta}} \right),
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;for empirical risk minimization. Hot diggity!&lt;/p&gt;

&lt;h3 id=&quot;next-up&quot;&gt;Next up&lt;/h3&gt;

&lt;p&gt;… the online learning perspective!&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is related to material from Nicolò Cesa-Bianchi’s lectures at the Bordeaux
Machine Learning Summer School in 2011. Watch them here: &lt;a href=&quot;http://videolectures.net/mlss2011_cesa_bianchi_learningtheory/&quot;&gt;MLSS 2011 lectures&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">This post illustrates how we use Rademacher complexities in statistical learning theory. To that end, we’ll assume that we’re working with our toolbox developed in the last post.</summary></entry><entry><title type="html">Learning theory - statistical learning</title><link href="http://localhost:4000/jekyll/update/2018/05/06/learning-theory-1.html" rel="alternate" type="text/html" title="Learning theory - statistical learning" /><published>2018-05-06T11:30:00-06:00</published><updated>2018-05-06T11:30:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/05/06/learning-theory-1</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/05/06/learning-theory-1.html">&lt;p&gt;Today’s post is the first in a set of three that will discuss two theories of learning.
In particular, today’s post will cover (a very narrow aspect of)
&lt;strong&gt;statistical learning theory&lt;/strong&gt;. The next post will present a game-theoretic
view of learning, using &lt;strong&gt;online learning&lt;/strong&gt; as its primary analytical framework. The
final post will connect the two perspectives.&lt;/p&gt;

&lt;h3 id=&quot;a-bit-of-background&quot;&gt;A bit of background&lt;/h3&gt;

&lt;p&gt;Learning problems, specifically machine learning problems can be analyzed from at
least two perspectives. In either case, our goal is to have a toolbox from which
we can analyze how well a learning system works. For example, we want to be able to
say when learning occurs, when it doesn’t, and how quickly it occurs or doesn’t. To
make this problem tractable, we’ll require&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a model for the data;&lt;/li&gt;
  &lt;li&gt;assumptions on the source of that data;&lt;/li&gt;
  &lt;li&gt;functions to generate predictions;&lt;/li&gt;
  &lt;li&gt;loss functions to assess the quality of our predictions; and&lt;/li&gt;
  &lt;li&gt;learning algorithms to help us improve our predictions by understanding our losses.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-set-up&quot;&gt;The set-up&lt;/h3&gt;

&lt;p&gt;We assume that our data consists of &lt;script type=&quot;math/tex&quot;&gt;(x, y)&lt;/script&gt; feature-output pairs with
&lt;script type=&quot;math/tex&quot;&gt;x \in \reals^d&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y \in \reals&lt;/script&gt;. The data comes from some &lt;strong&gt;fixed but unknown&lt;/strong&gt;
source that could be &lt;strong&gt;stochastic&lt;/strong&gt; (as in the statistical learning framework) or
&lt;strong&gt;nonstochastic&lt;/strong&gt; (as in online learning framework).&lt;/p&gt;

&lt;p&gt;We a function &lt;script type=&quot;math/tex&quot;&gt;f: \reals^d \to \reals&lt;/script&gt; that maps data to
estimates/forecasts/predictions. So, &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; gives us a mechanism to
generate our own &lt;script type=&quot;math/tex&quot;&gt;\hat{y}&lt;/script&gt;’s that we can compare against the true &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;’s; we assess
our estimates via a loss function &lt;script type=&quot;math/tex&quot;&gt;l: \reals^n \times \reals \to \reals&lt;/script&gt;. You probably
have many loss functions that you know and love…&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;squared-error loss: &lt;script type=&quot;math/tex&quot;&gt;l(y, \hat{y}) = \|y - \hat{y}\|^2_2&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;absolute-error: &lt;script type=&quot;math/tex&quot;&gt;l(y, \hat{y}) = \|y - \hat{y} \|_1&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Kullback–Leibler: &lt;script type=&quot;math/tex&quot;&gt;l(y, \hat{y}) = y \log \frac{y}{\hat{y}}&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;hinge loss: &lt;script type=&quot;math/tex&quot;&gt;l(y, \hat{y}) = \max \{0, 1 - y \cdot \hat{y} \}&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;logistic loss: &lt;script type=&quot;math/tex&quot;&gt;l(y, \hat{y}) = \ln \frac{\hat{y}}{1 - \hat{y}}&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The final ingredient is a &lt;strong&gt;learning algorithm&lt;/strong&gt;, which we can think of as a map from data
to models. This is way that V. Vapnik and A. Chervonenkis conceptualized learning
algorithms and we think that it’s a useful mental model.&lt;/p&gt;

&lt;p&gt;Before diving into either framework let’s give ourselves a learning goal: our algorithms
should output good models with respect to the data source.&lt;/p&gt;

&lt;h3 id=&quot;statistical-learning-theory&quot;&gt;Statistical learning theory&lt;/h3&gt;

&lt;p&gt;In the statistical learning setting, we assume that our data is generated by a probability
(&lt;em&gt;i.e.&lt;/em&gt;, stochastic) model with distribution
&lt;script type=&quot;math/tex&quot;&gt;\mathcal{D} \subseteq \reals^d \times \reals&lt;/script&gt;. In particular, we’ll assume&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(x, y) \stackrel{\textsf{i.i.d.}}{\sim} \mathcal{D}.&lt;/script&gt;

&lt;p&gt;We are interested in learning by controlling the &lt;strong&gt;statistical risk&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_{\mathcal{D}}(f) = \E \left[ l\big(f(X), Y\big) \right],&lt;/script&gt;

&lt;p&gt;where the expectation is over the randomness in the data &lt;script type=&quot;math/tex&quot;&gt;(X,Y)&lt;/script&gt; and the model &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;
belongs to some class of models &lt;script type=&quot;math/tex&quot;&gt;\mathcal{F}&lt;/script&gt;. The class of models is a design
parameter that we specify. For example, we could choose a set of linear predictors&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{F} = \left\{\theta \in \reals^d \mid x \mapsto \theta^T x\right\}.&lt;/script&gt;

&lt;p&gt;Since we’re restricting our models to the class &lt;script type=&quot;math/tex&quot;&gt;\mathcal{F}&lt;/script&gt;, we’re really interested
in controlling our risk over that class. So we want a model&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f^\star \in \argmin_{f \in \mathcal{F}} R_{\mathcal{D}}(f).&lt;/script&gt;

&lt;p&gt;But this is statistics, so we don’t have access to &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}&lt;/script&gt; at the population
level. Instead, we have knowledge of &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}&lt;/script&gt; through &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; samples
&lt;script type=&quot;math/tex&quot;&gt;(x_1,y_1),\dots,(x_n,y_n)&lt;/script&gt;. Hmmm…&lt;/p&gt;

&lt;h3 id=&quot;empirical-risk-minimization&quot;&gt;Empirical risk minimization&lt;/h3&gt;

&lt;p&gt;As statistically-minded individuals, we’ll use the &lt;strong&gt;empirical risk&lt;/strong&gt; as a surrogate
for the population risk and pick our model as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_n^\star \in \argmin_{f \in \mathcal{F}} R_n(f)
	= \argmin_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n l\big(f(x_i), y_i\big).&lt;/script&gt;

&lt;p&gt;We want the empirical risk to a be a good proxy for the statistical risk. More
specifically, we want it to be uniformly close to the statistical risk. By &lt;em&gt;uniformly
close&lt;/em&gt;, we mean that we’re close over all models within our class and all data.&lt;/p&gt;

&lt;p&gt;This might seem a bit extreme, but it prevents us from being lulled into believing we have
a “good” model by getting easy training data or having an overly complex model
class—think interpolating the data.&lt;/p&gt;

&lt;h3 id=&quot;rademacher-complexity&quot;&gt;Rademacher complexity&lt;/h3&gt;

&lt;p&gt;To help us assess our algorithms and models, we turn to &lt;strong&gt;Rademacher averages&lt;/strong&gt;, which
is a Vapnik–Chervonenkis type notion of complexity that (also) has its origins in the
Russian school of statistics (~1970s). We defined the empirical Rademacher average&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{R}_n(\mathcal{F}, S) = \E_\sigma \left[ \sup_{f \in \mathcal{F}}
 	\frac{1}{n} \sum_{i=1}^n \sigma_i f(x_i) \right],&lt;/script&gt;

&lt;p&gt;for a class of functions &lt;script type=&quot;math/tex&quot;&gt;\mathcal{F}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; samples &lt;script type=&quot;math/tex&quot;&gt;S = \{x_1,\dots,x_n\}&lt;/script&gt;.
Before moving on, note that the empirical Rademacher average does not depend on
&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;, but only on the data. This will prove useful in the sequel.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Aside:&lt;/strong&gt; see the &lt;a href=&quot;/jekyll/update/2018/03/04/rademacher.html&quot;&gt;Rademachers are rad - part I&lt;/a&gt; and
&lt;a href=&quot;/jekyll/update/2018/03/06/rademacher_2.html&quot;&gt;Rademachers are rad - part II&lt;/a&gt; post for Rademacher fun in the spirit of
Rademacher complexity.&lt;/p&gt;

&lt;h3 id=&quot;assessing-empirical-risk&quot;&gt;Assessing empirical risk&lt;/h3&gt;

&lt;p&gt;Okay, now we’re going ready to assess how well empirical risk minimization works by
studying the difference&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_{\mathcal{D}}(f) - R_{\mathcal{D}}(f^\star),&lt;/script&gt;

&lt;p&gt;which is the difference between a suboptimal model and the optimal model—note that we
don’t have an empirical risk term… yet.&lt;/p&gt;

&lt;p&gt;Nonetheless, here goes. We can add and subtract the empirical risk getting&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
R_{\mathcal{D}}(f_n^\star) - R_{\mathcal{D}}(f^\star) &amp;=
	R_{\mathcal{D}}(f_n^\star) - R_n(f_n^\star) +
		R_n(f_n^\star) - R_{\mathcal{D}}(f^\star) \\
	&amp;\le \sup_{f \in \mathcal{F}} \big\{ R_{\mathcal{D}}(f) - R_n(f) \big\} +
		R_n(f^\star) - R_{\mathcal{D}}(f^\star).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The &lt;script type=&quot;math/tex&quot;&gt;\sup&lt;/script&gt; term is a comparison of the selected model on the whole distribution versus
the sample. The second term is the error of using the “best” model on the training sample
versus the whole distribution. The inequality follows from the &lt;script type=&quot;math/tex&quot;&gt;\sup&lt;/script&gt; and the fact that
&lt;script type=&quot;math/tex&quot;&gt;R_n(f_n^\star) \le R_n(f^\star)&lt;/script&gt; by definition of &lt;script type=&quot;math/tex&quot;&gt;f_n^\star&lt;/script&gt;. Also, we can interpret
the first term as a measurement of how much the model overfits the data. And, in some
sense, this is what we want to understand!&lt;/p&gt;

&lt;h3 id=&quot;high-probability-bounds&quot;&gt;High probability bounds&lt;/h3&gt;

&lt;p&gt;Both terms on the righthand side are random variables because they depend on the
training data. So via Chernoff bounds (which we’re excluding from the current discussion),
we can say that with probability &lt;script type=&quot;math/tex&quot;&gt;1 - \delta&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\sup_{f \in \mathcal{F}} \big\{ R_{\mathcal{D}}(f) - R_n(f) \big\} &amp;\le
	\E \left[\sup_{f \in \mathcal{F}} \big\{ R_{\mathcal{D}}(f) - R_n(f) \big\}\right]+
	\mathcal{O} \left( \sqrt{\frac{1}{n} \ln \frac{1}{\delta}} \right) \\
R_n(f^\star) - R_{\mathcal{D}}(f^\star) &amp;\le
	\E \bigg[ R_n(f^\star) - R_{\mathcal{D}}(f^\star) \bigg]+
	\mathcal{O} \left( \sqrt{\frac{1}{n} \ln \frac{1}{\delta}} \right).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The expected value of the second term is 0 because the expected value of the empirical
risk, by our i.i.d. assumption, is the statistical risk. You can think of this term as
a bias-like term. The first term is more interesting; it may have caused your
Rademacher neurons to fire, but it’s not quite a Rademacher average… yet.&lt;/p&gt;

&lt;h3 id=&quot;the-ghost-sample&quot;&gt;The ghost sample&lt;/h3&gt;

&lt;p&gt;Now, we’re going to use one of “modern” statistics favorite tricks: the &lt;strong&gt;ghost sample&lt;/strong&gt;.
The ghost sample allows us to replace a (population) quantity by the sample quantity by
pretending that we have access to a second, made-up sample from the distribution—it’s
like we have a second training set. By the ghost sample trick (first equality) and
Jensen’s inequality, our term is bounded as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\E_S \left[\sup_{f \in \mathcal{F}} \big\{ R_{\mathcal{D}}(f) - R_n(f) \big\}\right]
	&amp;= \E_S \left[\sup_{f \in \mathcal{F}}
		\bigg\{\E_{\tilde{S}} R_{\tilde{n}}(f) - R_n(f)\bigg\}\right]\\
	&amp;\le \E_S \E_{\tilde{S}}
		\left[\sup_{f \in \mathcal{F}}\big\{R_{\tilde{n}}(f) - R_n(f)\big\}\right].
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Okay, cool. The righthand side is the difference between two empirical risk quantities,
one for the original training sample and the other for the ghost sample. We’re getting
closer to bounding the overfitting term…&lt;/p&gt;

&lt;h3 id=&quot;rademacher-magic&quot;&gt;Rademacher magic&lt;/h3&gt;

&lt;p&gt;Now, we introduce Rademacher random variables &lt;script type=&quot;math/tex&quot;&gt;\sigma_i \sim \mathsf{Rademacher}&lt;/script&gt; into
the problem which will allow us to bound the overfitting. Think of the Rademachers as
randomly permuting an example from the training set with an example from the ghost sample.
We want to control how much the risk changes when we exchange examples and do so via the
empirical Rademacher average&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E_\sigma \left[ \sup_{f \in \mathcal{F}}
 	\frac{1}{n} \sum_{i=1}^n \sigma_i \big( l_f(\tilde{z}_i) - l_f(z_i) \big) \right],&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;z_i = (x_i, y_i)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;l_f(z_i)&lt;/script&gt; is the loss function for &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; evaluated on
example &lt;script type=&quot;math/tex&quot;&gt;z_i&lt;/script&gt;. Now let’s manipulate this quantity, first distributing terms
and then exchanging the supremum with a sum to get an inequality:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\E_\sigma &amp;\left[ \sup_{f \in \mathcal{F}}
 	\frac{1}{n} \sum_{i=1}^n \sigma_i \big( l_f(\tilde{z}_i) - l_f(z_i) \big) \right]\\
	&amp;= \E_\sigma \left[ \sup_{f \in \mathcal{F}}
	   	\frac{1}{n} \sum_{i=1}^n \sigma_i l_f(\tilde{z}_i) +
	   	\frac{1}{n} \sum_{i=1}^n (-\sigma_i) l_f(z_i) \right] \\
	&amp;\le \E_\sigma \left[ \sup_{f \in \mathcal{F}}
	   	\frac{1}{n} \sum_{i=1}^n \sigma_i l_f(\tilde{z}_i) + \sup_{f \in \mathcal{F}}
	   	\frac{1}{n} \sum_{i=1}^n (-\sigma_i) l_f(z_i) \right] \\
	&amp;= \E_\sigma \left[ \sup_{f \in \mathcal{F}}
	      \frac{1}{n} \sum_{i=1}^n \sigma_i l_f(\tilde{z}_i) \right] +
		\E_\sigma \left[ \sup_{f \in \mathcal{F}}
	      \frac{1}{n} \sum_{i=1}^n (-\sigma_i) l_f(z_i) \right].
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;So, this last calculation, the fact that &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt; has the same distribution as
&lt;script type=&quot;math/tex&quot;&gt;-\sigma&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;z \sim \tilde{z}&lt;/script&gt;, imply that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E_S \left[\sup_{f \in \mathcal{F}} \big\{ R_{\mathcal{D}}(f) - R_n(f) \big\}\right] \le
	2 \E_S \E_\sigma \left[ \sup_{f \in \mathcal{F}} \frac{1}{n}
					\sum_{i=1}^n \sigma_i l_f(z_i) \right].&lt;/script&gt;

&lt;h3 id=&quot;next-up&quot;&gt;Next up&lt;/h3&gt;

&lt;p&gt;… a short-and-sweet example of the material from this post and then the online learning
perspective!&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is related to material from Nicolò Cesa-Bianchi’s lectures at the Bordeaux
Machine Learning Summer School in 2011. Watch them here: &lt;a href=&quot;http://videolectures.net/mlss2011_cesa_bianchi_learningtheory/&quot;&gt;MLSS 2011 lectures&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">Today’s post is the first in a set of three that will discuss two theories of learning. In particular, today’s post will cover (a very narrow aspect of) statistical learning theory. The next post will present a game-theoretic view of learning, using online learning as its primary analytical framework. The final post will connect the two perspectives.</summary></entry><entry><title type="html">The entropy method</title><link href="http://localhost:4000/jekyll/update/2018/04/29/entropy_method_intro.html" rel="alternate" type="text/html" title="The entropy method" /><published>2018-04-29T22:00:00-06:00</published><updated>2018-04-29T22:00:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/04/29/entropy_method_intro</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/04/29/entropy_method_intro.html">&lt;p&gt;Welcome back! This post is going to start us down the path of the &lt;strong&gt;entropy method&lt;/strong&gt;
for obtaining concentration inequalities. The idea is pretty straightforward. We
take a (relevant) modified logarithmic Sobolev inequality and, via a Herbst-type
argument, derive exponential concentration inequalities.&lt;/p&gt;

&lt;p&gt;In a deviation from previous posts, we’re going to take as given the concentration
behavior (&lt;em&gt;i.e.&lt;/em&gt;, we’re doing an example instead of a proof) and look at a neat
application that puts it to work.&lt;/p&gt;

&lt;h3 id=&quot;an-eigenvalue-bound&quot;&gt;An eigenvalue bound&lt;/h3&gt;

&lt;p&gt;The largest eigenvalue of a random symmetric matrix is a quantity that naturally
arises in many “modern” statistical problems—&lt;em&gt;e.g.&lt;/em&gt;, in random correlation matrices.
We can describe the behavior of the variance of this eigenvalue using the Efron–Stein
inequality; however, we can obtain concentration convergence rates using the entropy
method. In particular, we’ll use bounded differences-inspired condition to evoke
a fast concentration inequality.&lt;/p&gt;

&lt;p&gt;Assume we have independent random &lt;script type=&quot;math/tex&quot;&gt;X_{ij}, 1 \le i \le j \le n&lt;/script&gt; that are bounded
by 1 in absolute value. The symmetric matrix &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; has entries &lt;script type=&quot;math/tex&quot;&gt;X_{ij}&lt;/script&gt;. Our
object of interest is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z = \lambda_{\max} = \sup_{\|u\| = 1} u^T A u = v^T A v,&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; is the eigenvector associated with &lt;script type=&quot;math/tex&quot;&gt;\lambda_{\max}&lt;/script&gt;. Assume that
&lt;script type=&quot;math/tex&quot;&gt;\tilde{X}_{ij}&lt;/script&gt; is an independent copy of &lt;script type=&quot;math/tex&quot;&gt;X_{ij}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\tilde{A}_{ij}&lt;/script&gt;
is the matrix &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; with entropy &lt;script type=&quot;math/tex&quot;&gt;(i,j)&lt;/script&gt; replaced by the independent copy.
Similarly, let &lt;script type=&quot;math/tex&quot;&gt;\tilde{Z}_{ij}&lt;/script&gt; be the maximum eigenvalue of &lt;script type=&quot;math/tex&quot;&gt;\tilde{A}_{ij}&lt;/script&gt;.
Here’s where it starts to get good.&lt;/p&gt;

&lt;p&gt;In the spirit of bounded differences, we can calculate&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\big(Z - \tilde{Z}_{ij}\big)_+ &amp;\le
	(v^TAv - v^T\tilde{A}_{ij}v) \ind{} \{Z &gt; \tilde{Z}_{ij}\} \\
	&amp;= \big( v^T(A-\tilde{A}_{ij})v \big) \ind{} \{Z &gt; \tilde{Z}_{ij}\} \\
	&amp;\le 2 \left( v_i v_j \left( X_{ij} - \tilde{X}_{ij} \right) \right)_+ \\
	&amp;\le 4 |v_i v_j|.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The first line results from the assumption on &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; and the use of the indicator variable.
The third line follows from a rewriting of the matrix multiplication and the observation
that all but two entries of the sum cancel each other.&lt;/p&gt;

&lt;p&gt;If we throw this guy into the Efron–Stein inequality, we get that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Var(Z) \le 16.&lt;/script&gt;

&lt;p&gt;Now, we can use the logarithmic Sobolev machinery to get the exponential
concentration inequality&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\prob(Z - \E Z &gt; t) \le e^{-t^2 / (2c)} = e^{-t^2 / (32)},&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt; is a constant that bounds &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^n (Z - \tilde{Z}_{ij})^2&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;linear-algebra-aside&quot;&gt;Linear algebra aside&lt;/h3&gt;

&lt;p&gt;A theorem from linear algebra tells us that the maximum eigenvalue of a symmetric
matrix is upper-bounded by the largest row sum and lower-bounded the smallest row
sum. So, the above ideas become interesting and useful when have big matrices…
precisely the kind arising in &lt;em&gt;modern&lt;/em&gt; applications!&lt;/p&gt;

&lt;h3 id=&quot;next-up&quot;&gt;Next up&lt;/h3&gt;

&lt;p&gt;… honestly, I’m not sure! I’m thinking we could start to unpack the machinery
that supported our blind use of the entropy method. Or it could be about bandits,
prediction with expert advice, Markov chains, something from convex optimization…&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is related to material from Chapters 3 and 6 of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Concentration Inequalities: A Nonasymptotic Theory of Independence&lt;/em&gt; by
S. Boucheron, G. Lugosi, and P. Masart.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Welcome back! This post is going to start us down the path of the entropy method for obtaining concentration inequalities. The idea is pretty straightforward. We take a (relevant) modified logarithmic Sobolev inequality and, via a Herbst-type argument, derive exponential concentration inequalities.</summary></entry><entry><title type="html">Rademachers are rad - part III</title><link href="http://localhost:4000/jekyll/update/2018/04/25/rademacher_3.html" rel="alternate" type="text/html" title="Rademachers are rad - part III" /><published>2018-04-25T17:00:00-06:00</published><updated>2018-04-25T17:00:00-06:00</updated><id>http://localhost:4000/jekyll/update/2018/04/25/rademacher_3</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/04/25/rademacher_3.html">&lt;p&gt;After some ambivalence, I decided that we’ll pay homage to our Rademacher friends
once again. We’re going to prove a minimax lower bound for an expert prediction
problem. As we’ll see, Rademacher random variables will come to our rescue 
(twice, in fact).&lt;/p&gt;

&lt;h3 id=&quot;prediction-with-experts&quot;&gt;Prediction with experts&lt;/h3&gt;

&lt;p&gt;Let’s say that we disagree with statistical learning theory on philosophical grounds,
&lt;em&gt;i.e.&lt;/em&gt;, we don’t believe that our data are generated via a stochastic model that
assumes independence (or exchangeability). Can we predict in the absence of a
statistical assumptions? If so, how would we do it?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Can we predict in the absence of a statistical assumptions?&lt;/em&gt; &lt;strong&gt;Yes!&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;If so, how could we do it?&lt;/em&gt; &lt;strong&gt;Using the advice of experts!&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;strong&gt;prediction with expert advice&lt;/strong&gt; framework has the following set up:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a decision space &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}&lt;/script&gt;;&lt;/li&gt;
  &lt;li&gt;an outcome space &lt;script type=&quot;math/tex&quot;&gt;\mathcal{Y}&lt;/script&gt;;&lt;/li&gt;
  &lt;li&gt;a loss function &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;; and&lt;/li&gt;
  &lt;li&gt;a set of expert indices &lt;script type=&quot;math/tex&quot;&gt;\mathcal{E}&lt;/script&gt;;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At each time &lt;script type=&quot;math/tex&quot;&gt;t = 1, 2, \dots&lt;/script&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;the environment chooses the next outcome &lt;script type=&quot;math/tex&quot;&gt;y_t&lt;/script&gt; and the experts choose
 &lt;script type=&quot;math/tex&quot;&gt;\left\{ f_{E,t} \in \mathcal{D} \mid E \in \mathcal{E} \right\}&lt;/script&gt;;&lt;/li&gt;
  &lt;li&gt;the expert advice is provided to the forecaster;&lt;/li&gt;
  &lt;li&gt;the forecast chooses a prediction &lt;script type=&quot;math/tex&quot;&gt;\widehat{p}_t \in \mathcal{D}&lt;/script&gt;;&lt;/li&gt;
  &lt;li&gt;the environment reveals the next outcome &lt;script type=&quot;math/tex&quot;&gt;y_t \in \mathcal{Y}&lt;/script&gt;;&lt;/li&gt;
  &lt;li&gt;the forecaster incurs a loss &lt;script type=&quot;math/tex&quot;&gt;l(\widehat{p}_t, y_t)&lt;/script&gt; and each expert &lt;script type=&quot;math/tex&quot;&gt;E&lt;/script&gt; incurs
 a loss &lt;script type=&quot;math/tex&quot;&gt;l(f_{E,t}, y_t)&lt;/script&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;minimax-lower-bound&quot;&gt;Minimax lower bound&lt;/h3&gt;

&lt;p&gt;We want to show that if &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; is the absolute loss, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sup_{n,N} \frac{\mathfrak{M}(n,N,l)}{\sqrt{(n/2) \ln N}} \ge 1&lt;/script&gt;

&lt;p&gt;where the minimax regret &lt;script type=&quot;math/tex&quot;&gt;\mathfrak{M}&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; rounds is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathfrak{M}(n,N,l) = \inf_P \sup_{ \{ \mathcal{F}: |\mathcal{F}| = N \} } 
	\sup_{y^n \in \mathcal{Y}^n} \max_{i=1,\dots,N}
	\left\{ \sum_{i=1}^n l(\hat{p}_t, y_t) - l \left( f_{i,t}, y_t \right) \right\},&lt;/script&gt;

&lt;p&gt;with &lt;script type=&quot;math/tex&quot;&gt;\mathcal{F}&lt;/script&gt; is a set of &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; static experts, &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; is a forecasting strategy, 
&lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; is a &lt;script type=&quot;math/tex&quot;&gt;[0,1]&lt;/script&gt;-valued convex loss function, and &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; is the number of rounds.
(A static expert is one with constant-valued predictions, &lt;em&gt;i.e.&lt;/em&gt;, they only depend on the 
current round.)&lt;/p&gt;

&lt;h3 id=&quot;proof&quot;&gt;Proof&lt;/h3&gt;

&lt;p&gt;We’re going to lower bound a lower bound on &lt;script type=&quot;math/tex&quot;&gt;\mathfrak{M}(n,N,l)&lt;/script&gt;, which may seem
odd, but is a fairly common technique. (So keep it in your back pocket the next time
you have to bound something.) In particular, we’re going to use the fact that a fixed
class of static experts lower bounds the minimax regret as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathfrak{M}(n,N,l) \ge 
	\sup_{ \{ \mathcal{F}: |\mathcal{F}| = N \} } \mathfrak{M}(n, \mathcal{F}, l)&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathfrak{M}(n, \mathcal{F}, l) = \inf_P \sup_{y^n \in \{0,1\}} \sup_{f \in \mathcal{F}}
	\left\{ \sum_{t=1}^n |\hat{p}_t - y_t| -  |f_t - y_t| \right\}.&lt;/script&gt;

&lt;p&gt;Then we’ll lower bound &lt;script type=&quot;math/tex&quot;&gt;\mathfrak{M}(n, \mathcal{F}, l)&lt;/script&gt;, which implies the minimax
regret bound.&lt;/p&gt;

&lt;p&gt;First, we use the &lt;em&gt;average-is-less-than-the-max&lt;/em&gt; trick to get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathfrak{M}(n, \mathcal{F}, l) \ge \inf_P \E \left[ \sup_{f \in \mathcal{F}}
	\left\{ \sum_{t=1}^n |\hat{p}_t - Y_t| -  |f_t - Y_t| \right\} \right],&lt;/script&gt;

&lt;p&gt;where the expectation is over the randomness in &lt;script type=&quot;math/tex&quot;&gt;Y_t&lt;/script&gt;. By the definitions of the 
infimum and supremum, we can rewrite this guy as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\inf_P \E \left[ \sum_{t=1}^n |\hat{p}_t - Y_t| \right] -  
 \E \left[ \inf_{f \in \mathcal{F}} \sum_{t=1}^n |f_t - Y_t| \right].&lt;/script&gt;

&lt;p&gt;Okay, cool. Now we’re going to use the fact that 
&lt;script type=&quot;math/tex&quot;&gt;\E \sum_{t=1}^n |\hat{p}_t - Y_t| = n/2&lt;/script&gt; for all
forecasting strategies because of the randomness of the &lt;script type=&quot;math/tex&quot;&gt;Y_t&lt;/script&gt;. (To see this, split the
absolute value into two parts, enjoy some nice cancellations and sum it all up.)
We’ll use this fact and then introduce some Rademacher random variables defined as 
&lt;script type=&quot;math/tex&quot;&gt;W_t = 1 - 2Y_t&lt;/script&gt; to write&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathfrak{M}(n, \mathcal{F}, l) &amp;\ge 
\frac{n}{2} - \E \left[ \inf_{f \in \mathcal{F}} \sum_{t=1}^n |f_t - Y_t| \right] \\
	&amp;= \E \left[ \sup_{f \in \mathcal{F}} \sum_{t=1}^n \frac{1}{2} - |f_t - Y_t| \right]\\
	&amp;= \E \left[ \sup_{f \in \mathcal{F}} \sum_{t=1}^n 
		\left( \frac{1}{2} - f_t \right)  W_t \right].
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now, let’s take supremums of both sides, do another Rademacher trick, and then 
exploit the symmetry of Rademachers:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\sup_{ \{ \mathcal{F}: |\mathcal{F}| = N \} } \mathfrak{M}(n, \mathcal{F}, l) 
	&amp;\ge \sup_{ \{ \mathcal{F}: |\mathcal{F}| = N \} } 
		\E \left[ \sup_{f \in \mathcal{F}} \sum_{t=1}^n \left( \frac{1}{2} - f_t \right) 
		 W_t \right] \\
	&amp;\ge \frac{1}{2} \E \left[ \max_{i=1,...,N} \sum_{t=1}^n Z_{i,t} W_t \right] \\
	&amp;= \frac{1}{2} \E \left[ \max_{i=1,...,N} \sum_{t=1}^n Z_{i,t} \right],
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where we used that &lt;script type=&quot;math/tex&quot;&gt;(1/2)(1 - 2f_t) = (1/2)Z_{i,t}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Z_{i,t}W_t \sim Z_{i,t}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The final step is an appeal to &lt;em&gt;ye-old&lt;/em&gt; central limit theorem and a maximal inequality
for (sub-)Gaussian random variables. The CLT says that for each &lt;script type=&quot;math/tex&quot;&gt;i = 1,\dots,N&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{\sqrt{n}} \sum_{t=1}^n Z_{i,t} \sim N(0,1) \implies
\lim_{n \to \infty} \E \left[ \frac{1}{\sqrt{n}} \max_{i=1,...,N} 
								\sum_{t=1}^n Z_{i,t} \right] =
	\E \left[ \max_{i=1,...,N} X_i \right].&lt;/script&gt;

&lt;p&gt;for standard normals &lt;script type=&quot;math/tex&quot;&gt;X_1,\dots,X_n&lt;/script&gt;. The maximal inequality gives&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\E \left[ \max_{i=1,...,N} X_i \right] \le \sqrt{2 \ln N}.&lt;/script&gt;

&lt;p&gt;Now, we stitch everything back together and conclude the proof with&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathfrak{M}(n,N,l) &amp;\ge 
	\sup_{ \{ \mathcal{F}: |\mathcal{F}| = N \} } \mathfrak{M}(n, \mathcal{F}, l) \\
	&amp;\ge \frac{\E \left[ \max_{i=1,...,N} X_i \right]}{\sqrt{2 \ln N}} \\
	&amp;\ge 1.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Rademacher’s to the rescue once again.&lt;/p&gt;

&lt;h3 id=&quot;next-up&quot;&gt;Next up&lt;/h3&gt;

&lt;p&gt;… modified logarithmic Sobolev inequalities &lt;strong&gt;or&lt;/strong&gt; bandits!&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;This post is related to material from Chapters 2 and 3 of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Prediction, Learning, and Games&lt;/em&gt; by N. Cesa-Bianchi and G. Lugosi.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">After some ambivalence, I decided that we’ll pay homage to our Rademacher friends once again. We’re going to prove a minimax lower bound for an expert prediction problem. As we’ll see, Rademacher random variables will come to our rescue (twice, in fact).</summary></entry></feed>