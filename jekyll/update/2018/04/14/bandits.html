<!DOCTYPE html>
<html>
	<head>
  	  <title>
  	  	Bandits!
  	  </title>
	</head>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Bandits! | Eggink Blog</title>
<meta name="generator" content="Jekyll v3.7.2" />
<meta property="og:title" content="Bandits!" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Today we’re talking about bandits. Bandit problems, not dissimilar from Markov chains, are one of those simple models that apply to a wide variety of problems. Indeed, many results of “sequential decision making” stem from the study of bandit problems. Plus the name is cool, so that should provide some motivation for wanting to study them. The name bandit originates from the term one-armed bandit, which was used to describe slot machines." />
<meta property="og:description" content="Today we’re talking about bandits. Bandit problems, not dissimilar from Markov chains, are one of those simple models that apply to a wide variety of problems. Indeed, many results of “sequential decision making” stem from the study of bandit problems. Plus the name is cool, so that should provide some motivation for wanting to study them. The name bandit originates from the term one-armed bandit, which was used to describe slot machines." />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2018/04/14/bandits.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2018/04/14/bandits.html" />
<meta property="og:site_name" content="Eggink Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-04-14T17:00:00-06:00" />
<script type="application/ld+json">
{"description":"Today we’re talking about bandits. Bandit problems, not dissimilar from Markov chains, are one of those simple models that apply to a wide variety of problems. Indeed, many results of “sequential decision making” stem from the study of bandit problems. Plus the name is cool, so that should provide some motivation for wanting to study them. The name bandit originates from the term one-armed bandit, which was used to describe slot machines.","@type":"BlogPosting","url":"http://localhost:4000/jekyll/update/2018/04/14/bandits.html","headline":"Bandits!","dateModified":"2018-04-14T17:00:00-06:00","datePublished":"2018-04-14T17:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2018/04/14/bandits.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="alternate" type="application/rss+xml" title="Eggink Blog" href="/feed.xml">
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <a class="site-title" rel="author" href="/">Eggink Blog</a>

    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
            
          
            
            
            <a class="page-link" href="/about/">About</a>
            
          
            
            
          
            
            
          
            
            
          
        </div>
      </nav>
    
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Bandits!</h1>
    <p class="post-meta">Apr 14, 2018</p>
  </header>

  <article class="post-content">
    <p>Today we’re talking about bandits. Bandit problems, not dissimilar from Markov 
chains, are one of those simple models that apply to a wide variety of
problems. Indeed, many results of “sequential decision making” stem from
the study of bandit problems. Plus the name is cool, so that should provide
some motivation for wanting to study them. The name <strong>bandit</strong> originates 
from the term <strong>one-armed bandit</strong>, which was used to describe slot machines.</p>

<p>In their <a href="http://sbubeck.com/SurveyBCB12.pdf">survey on bandits</a>, Bubeck and Cesa-Bianchi define 
a multi-armed bandit problem as “a sequential allocation problem defined by a 
set of actions.” The allocation problem is similar to a (repeated) game: at 
each point in time, the player chooses an action <em>i.e.</em>, an arm, and receives 
a reward.</p>

<p>The player’s goal is to maximize her cumulative reward, which naturally leads to
an <strong>exploitation vs. exploration</strong> problem. The player needs to simultaneously 
exploit arms known to yield high rewards and explore other arms in the hopes 
of finding other high-yield rewards.</p>

<p>In bandit problems, we assess a player’s performance by her <strong>regret</strong>, which
compares her performance to (something like) the best-possible performance. 
After the first <script type="math/tex">n</script> rounds of the game, the player’s regret is</p>

<script type="math/tex; mode=display">R_n = \max_{i=1,\dots,K} \sum_{i=1}^n X_{i,t} - \sum_{i=1}^n X_{I_t,t},</script>

<p>where <script type="math/tex">K</script> is the number of bandit arms, <script type="math/tex">X_{i,t}</script> is the reward of arm <script type="math/tex">i</script> at
time <script type="math/tex">t</script>, and <script type="math/tex">I_t</script> is the player’s (possibly randomly) chosen arm.</p>

<h3 id="stochastic-bandit-problems">Stochastic bandit problems</h3>

<p>In this post, we’re going to focus on <strong>stochastic bandit problems</strong> as opposed
to <strong>adversarially bandit problems</strong>. Here’s the set-up.</p>

<ul>
  <li>There are <script type="math/tex">\{1,\dots,K\}</script> arms.</li>
  <li>Each arm <script type="math/tex">i</script> corresponds to an unknown probability distribution <script type="math/tex">p_i</script>.</li>
  <li>At each time <script type="math/tex">t</script>, the player selects an arm <script type="math/tex">I_t</script> (independently of 
previous choices) and receives a reward <script type="math/tex">X_{I_t,t} \sim p_{I_t}</script>.</li>
</ul>

<p>We’ll denote the mean of distribution <script type="math/tex">i</script> with <script type="math/tex">\mu_i</script> and define</p>

<script type="math/tex; mode=display">\mu^\star = \max_{i=1,\dots,K} \mu_i \quad \textsf{and} \quad
	i^\star = \argmax_{i=1,\dots,K} \mu_i</script>

<p>as optimal parameters. Instead of using the regret to assess our player’s performance,
we’ll use the <strong>pseudo-regret</strong>, which is defined as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\bar{R}_n &= \max_{i=1,\dots,K} \E 
	\left[ \sum_{i=1}^n X_{i,t} - \sum_{i=1}^n X_{I_t,t} \right] \\
	&= n \mu^\star - \E \sum_{t=1}^n \mu_{I_t}.
\end{align*} %]]></script>

<p>We know that <script type="math/tex">\bar{R}_n \le \E R_n</script> because of Jensen’s inequality for convex 
functions. For our purposes, a more informative form of the pseudo-regret is</p>

<script type="math/tex; mode=display">\bar{R}_n = \sum_{i=1}^K \Delta_i \E T_i(n),</script>

<p>where <script type="math/tex">\Delta_i = \mu^\star - \mu_i</script> is the suboptimality of arm <script type="math/tex">i</script> and the
random variable <script type="math/tex">T_i(n) = \sum_{i=1}^n \ind{}\{I_t = i\}</script> is the number of times 
arm <script type="math/tex">i</script> is selected in the first <script type="math/tex">n</script> rounds.</p>

<p>This alternative expression for the pseudo-regret sums over arms rather than rounds.
To see how we switched from arms to rounds, play around with random variables 
<script type="math/tex">T_i(n)</script>.</p>

<h3 id="upper-confidence-bounds">Upper confidence bounds</h3>

<p>One approach for simultaneously performing <strong>exploration</strong> and <strong>exploitation</strong> is 
to use <strong>upper confidence bound</strong> (UCB) strategies. UCB strategies produce upper
bounds (based on a confidence level) on the current estimate for each arm’s mean 
reward. The player selects the arm that is the best using the UCB-modified estimate.</p>

<p>In the spirit of brevity, we’re going to gloss over a few important details 
(<em>e.g.</em>, Legendre–Fenchel duals and Hoeffding’s lemma) but here’s the gist of 
the UCB set-up.</p>

<ul>
  <li>Define <script type="math/tex">\widehat{\mu}_{i,n} = \frac{1}{n} \sum_{j=1}^n X_{i,j}</script> as the sample
mean of arm <script type="math/tex">i</script> after being played <script type="math/tex">n</script> times.</li>
  <li>For a convex function <script type="math/tex">\psi</script> with convex conjugate <script type="math/tex">\psi^*</script> and parameter 
<script type="math/tex">\lambda > 0</script>, the cumulant generating function of <script type="math/tex">X</script> is bounded by <script type="math/tex">\psi</script>:</li>
</ul>

<script type="math/tex; mode=display">\ln \E e^{\lambda(X - \E X)} \le \psi(\lambda) \quad \textsf{and} \quad
\ln \E e^{\lambda(\E X - X)} \le \psi(\lambda).</script>

<ul>
  <li>By Markov’s inequality and the Cramér–Chernoff technique</li>
</ul>

<script type="math/tex; mode=display">\prob \left( \mu_i - \widehat{\mu}_{i,n} \ge \varepsilon \right) \le
	e^{-n \psi^*(\varepsilon)}.</script>

<ul>
  <li>Markov’s inequality means that, with probability <script type="math/tex">1 - \delta</script>, we have an 
upper bound on the estimate of the <script type="math/tex">i</script>th arm:</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\mu_i < \widehat{\mu}_{i,n} + (\psi^*)^{-1} 
	\left(\frac{1}{n} \ln \frac{1}{\delta} \right). %]]></script>

<p>The UCB strategy is (very) simple; at each time <script type="math/tex">t</script>, select the arm <script type="math/tex">I_t</script> by</p>

<script type="math/tex; mode=display">I_t \in \argmax_{i=1,\dots,K} \left\{ \widehat{\mu}_{i,T_i(t-1)} + 
	(\psi^*)^{-1} \left( \frac{\alpha \ln t}{T_i(t-1)} \right) \right\}.</script>

<p><strong>The tradeoff.</strong>
Let’s pick apart this term and see how it encourages exploration and exploitation.
We’re going to use <script type="math/tex">\psi(x) = x^2/8</script> with convex conjugate 
<script type="math/tex">\psi^*(z) = 2z^2</script>, so that <script type="math/tex">(\psi^*)^{-1}(z) = (x/2)^{1/2}</script>.</p>

<p>When arm <script type="math/tex">i</script> has not been played many times before time $t$, <em>i.e.</em>, when 
<script type="math/tex">T_i(t-1)</script> is small numerator of <script type="math/tex">\frac{\alpha \ln t}{T_i(t-1)}</script> dominates 
and the UCB-adjusted estimate of arm <script type="math/tex">i</script> is likely to relatively high, which
encourages arm <script type="math/tex">i</script> to be chosen. In other words, it encourages exploration.</p>

<p>As arm <script type="math/tex">i</script> is chosen more, <script type="math/tex">T_i(t-1)</script> increases and the estimate
<script type="math/tex">\widehat{\mu}_{i,T_i(t-1)}</script> improves. If arm <script type="math/tex">i</script> is the optimal arm, then the 
<script type="math/tex">\widehat{\mu}_{i,T_i(t-1)}</script> term dominates the estimates for other arms and the
UCB-adjustment further encourages selection of this arm. This means that the 
algorithm exploits high-yielding arms.</p>

<p><strong>Upper bound on pseudo-regret.</strong>
If our player follows the UCB scheme specified above, then for <script type="math/tex">\alpha > 2</script>
her pseudo-regret is upperbounded as</p>

<script type="math/tex; mode=display">\bar{R}_n \le \sum_{i: \Delta_i > 0} \left( 
\frac{\alpha \Delta_i}{\psi^*(\Delta_i / 2)} \ln n + \frac{\alpha}{\alpha - 2} 
\right).</script>

<p>We’re going to prove the bound in the next post.</p>

<h3 id="bandit-simulations">Bandit simulations</h3>

<p>Here’s an <a href="https://github.com/jakeknigge/ucb-bandits/blob/master/bandits.R">R script</a> which uses the upper confidence bound algorithm.
For the plots below, there are <script type="math/tex">K = 3</script> arms and <script type="math/tex">n = 1000</script> rounds. The reward 
distributions are Bernoulli’s and the mean parameters the three arms are 0.46, 0.72, 
and 0.55.</p>

<p>The first plot shows the evolution of each mean estimate as different rounds are
played.
<img src="/assets/2018-04-14_bandit_arm_est.png" alt="Bandit arm estimates" /></p>

<p>The next plot illustrates the exploration vs. exploitation trade-off. Even
though arm 2 yields the highest rewards, the algorithm occasionally plays 
arms 1 and 3 in later rounds.
<img src="/assets/2018-04-14_bandit_arms_played.png" alt="Bandit regret" /></p>

<p>The final plot shows the player’s cumulative regret compared to the UCB-implied
upper bound on the regret; the lower bound is distribution-dependent and
asymptotic, which is why it exceeds the pseudo-regret until round 400ish.
<img src="/assets/2018-04-14_bandit_regret.png" alt="Bandit regret" /></p>

<h3 id="next-up">Next up</h3>

<p>… a proof of the upper confidence bound regret rate!</p>

<h3 id="references">References</h3>

<p>This post is based on material from:</p>

<ul>
  <li><em>Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems</em>
by S. Bubeck and N. Cesa-Bianchi. A digital copy can be found 
<a href="http://sbubeck.com/SurveyBCB12.pdf">here</a>.</li>
  <li><em>Prediction, Learning, and Games</em> (a.k.a. “Perdition, Burning, and Flames”) 
by N. Cesa-Bianchi and G. Lugosi. Find a description of the book and its 
table of contents <a href="http://cesa-bianchi.di.unimi.it/predbook/">here</a>.</li>
</ul>

<p>I would also recommend checking out Sébastien Bubeck’s blog 
<a href="https://blogs.princeton.edu/imabandit">“I’m a bandit”</a> and, in particular, his bandit tutorials:
<a href="https://blogs.princeton.edu/imabandit/2016/05/11/bandit-theory-part-i/">part 1</a> and <a href="https://blogs.princeton.edu/imabandit/2016/05/13/bandit-theory-part-ii/">part 2</a>. Also check out his new YouTube
channel: https://www.youtube.com/user/sebastienbubeck/videos.</p>


  </article>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://jakeknigge-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

                           
<script type="text/javascript" 
	src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  
<script type="text/x-mathjax-config">

MathJax.Hub.Config({

    showProcessingMessages: false,
    messageStyle: "none",
	CommonHTML: { linebreaks: { automatic: true } },
	SVG: { linebreaks: { automatic: true } },
    "HTML-CSS": {
      linebreaks: { automatic: true},
      availableFonts: ["TeX", "Neo-Euler", "STIX"],
      preferredFont: "TeX",
      webFont: "TeX",
        matchFontHeight: true,
        scale: 100,
        styles: {
            ".MathJax_Display": {
                margin: "0.85em 0em"
            }
        }
    },
    MathMenu: {  
        showFontMenu: true
    },
    TeX: {
      TagIndent: "0.25em",
      extensions: ["AMSmath.js","AMSsymbols.js","cancel.js"], //cancel.js needed only for I_29
      equationNumbers: {
        autoNumber: "AMS", formatNumber: function (n) { return EqnumPrefix() + n }
      },
      MultLineWidth: "80%",
		Macros: {  
			mat: ["{\\overset \\leftrightarrow #1}",1],  
			bra: ["{\\langle #1 |}",1],  
			ket: ["{| #1 \\rangle}",1],  
			sprod: ["{\\left\\langle #1 | #2 \\right\\rangle}",2],  
			tr: "\\text{tr}",  
			ev: ["{\\left\\langle #1 \\right \\rangle}",1],  
			threej: ["{\\left(\\begin{array}{ccc} #1 & #3 & #5 \\\\ #2 & #4 & #6 \\end{array} \\right)}",6],  
			dbar: "đ",
			d: "\\mathrm{d}",
			i: "\\mathrm{i}",
			eps: "\\varepsilon",
			sla: ["\\displaystyle{\\not} #1",1],  
			spinvec: ["\\left( \\begin{array}{c} #1 \\\\ #2 \\end{array} \\right)",2],  
			spinmat: ["\\left( \\begin{array}{cc} #1 & #2 \\\\ #3 & #4 \\end{array} \\right)",4],  
			reals: "\\mathbf{R}",
			integers: "\\mathbf{Z}",
			naturals: "\\mathbf{N}",
			symm: "\\mathbf{S}",
			ones: "\\mathbf{1}",
			prox: "\\mathbf{prox}",
			dom: "\\mathop{\\mathbf{dom}}",
			dist: "\\mathop{\\mathbf{dist}}",
			argmin: "\\mathop{\\mathrm{argmin}}",
			argmax: "\\mathop{\\mathrm{argmax}}",
			epi: "\\mathop{\\mathbf{epi}}",
			E: "\\mathbf{E}",
			Var: "\\mathbf{Var}",
			Cov: "\\mathbf{Cov}",
			prob: "\\mathbf{P}",
			ind: ["{\\mathbf{I}_{#1}}", 1],
			tvec: ["{\\boldsymbol{#1}}",1],
			relint: "\\mathop{\\mathbf{relint}}",
		}
    }
});
</script>

</div>
      </div>
    </div>

    <footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Eggink Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">
            
              Eggink Blog
            
            </li>
            
            <li><a class="u-email" href="mailto:jake@knigge.us">jake@knigge.us</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
  
  
  
  <li><a href="https://github.com/jakeknigge"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jakeknigge</span></a></li>
  
  
  
  <li><a href="https://www.twitter.com/JakeKnigge"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">JakeKnigge</span></a></li>
  
  
  
</ul>

      </div>

      <div class="footer-col footer-col-3">
        <p>Jake W. Knigge&#39;s blah, blah, blog... a place to discuss statistics, math, and  whatever else comes to mind.
</p>
      </div>
    </div>

  </div>

</footer>

    
  </body>

</html>