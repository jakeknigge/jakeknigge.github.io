<!DOCTYPE html>
<html>
	<head>
  	  <title>
  	  	Learning theory - reconciliations and connections
  	  </title>
	</head>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Learning theory - reconciliations and connections | Eggink Blog</title>
<meta name="generator" content="Jekyll v3.7.2" />
<meta property="og:title" content="Learning theory - reconciliations and connections" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Howdy learning theorists! Today we’re going to connect statistical learning and online learning. We’ll do via model averaging and exploiting online learning’s local perspective. To that end, this post could have easily been called “the power of local optimization”, which has a nice ring to it." />
<meta property="og:description" content="Howdy learning theorists! Today we’re going to connect statistical learning and online learning. We’ll do via model averaging and exploiting online learning’s local perspective. To that end, this post could have easily been called “the power of local optimization”, which has a nice ring to it." />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2018/05/12/learning-theory-3.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2018/05/12/learning-theory-3.html" />
<meta property="og:site_name" content="Eggink Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-05-12T22:00:00-06:00" />
<script type="application/ld+json">
{"description":"Howdy learning theorists! Today we’re going to connect statistical learning and online learning. We’ll do via model averaging and exploiting online learning’s local perspective. To that end, this post could have easily been called “the power of local optimization”, which has a nice ring to it.","@type":"BlogPosting","url":"http://localhost:4000/jekyll/update/2018/05/12/learning-theory-3.html","headline":"Learning theory - reconciliations and connections","dateModified":"2018-05-12T22:00:00-06:00","datePublished":"2018-05-12T22:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2018/05/12/learning-theory-3.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="alternate" type="application/rss+xml" title="Eggink Blog" href="/feed.xml">
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <a class="site-title" rel="author" href="/">Eggink Blog</a>

    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
            
          
            
            
            <a class="page-link" href="/about/">About</a>
            
          
            
            
          
            
            
          
            
            
          
        </div>
      </nav>
    
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Learning theory - reconciliations and connections</h1>
    <p class="post-meta">May 12, 2018</p>
  </header>

  <article class="post-content">
    <p>Howdy learning theorists! Today we’re going to connect statistical learning and online
learning. We’ll do via model averaging and exploiting online learning’s local perspective.
To that end, this post could have easily been called “the power of local optimization”,
which has a nice ring to it.</p>

<p>Our goal is address a few questions that pop up when the online learning approach is
unfamiliar.</p>

<ol>
  <li>What does the online learning regret bound mean?</li>
  <li>How can that bound be used?</li>
  <li>How well does the loss rate on the model sequence generalize?</li>
</ol>

<h3 id="the-approach">The approach</h3>

<p>Because online learning generates a sequence of models rather than a single model, we
have to pick a model to compare to the empirical risk minimizer. In particular, we’ll
choose the average model. Then we can use the online analysis machinery to provide a
risk bound on that model.</p>

<p>Assume we have</p>

<ul>
  <li>an <script type="math/tex">n</script>-dimensional sample <script type="math/tex">S = \{(x_1,y_1),\dots,(x_n,y_n)\}</script> drawn
i.i.d. from a training set <script type="math/tex">\mathcal{D}</script>;</li>
  <li>a loss function <script type="math/tex">l_\mathcal{D}(w)</script> for linear models <script type="math/tex">w</script>;</li>
  <li>a “best” model <script type="math/tex">u^\star = \argmin_{u: \|u\| \le U} l_\mathcal{D}(u)</script>.</li>
</ul>

<p>We want to control the differences</p>

<script type="math/tex; mode=display">l_\mathcal{D}(\hat{w}) - l_\mathcal{D}(u^\star),</script>

<p>where <script type="math/tex">\hat{w}</script> is the average model. Here’s how we get <script type="math/tex">\hat{w}</script>.</p>

<ul>
  <li>run online gradient descent on our sample set <script type="math/tex">S</script>;</li>
  <li>obtain a sequence of models <script type="math/tex">w_1,\dots,w_n</script>;</li>
  <li>compute the average model</li>
</ul>

<script type="math/tex; mode=display">\hat{w} = \frac{1}{n} \sum_{i=1}^n w_i.</script>

<p><strong>Aside on averaging.</strong> Online analysis gives insight on the behavior of the model
sequence but not on any one particular model. So, we need to consolidate our sequence
into something we can analyze. Moreover, averaging is a good way (think of Leo Breiman
and <strong>bagging</strong>) to reduce variance without impacting an estimator’s bias.</p>

<h3 id="expectation-bounds">Expectation bounds</h3>

<p>We start by showing that the expected value of the loss of the average model is less
than or equal to the average loss on the model sequence via Jensen’s inequality:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
l_\mathcal{D}\big( \hat{w} \big)
	&= \E \left[ l\left( \frac{1}{n} \sum_{t=1}^n w_t, (X,Y) \right) \right] \\
	&\le \frac{1}{n} \sum_{t=1}^n \E \, l\big(w_t, (X,Y) \big) \\
	&= \frac{1}{n} \sum_{t=1}^n l_\mathcal{D}(w_t).
\end{align*} %]]></script>

<p>Cool, now we have something to compare to the best model in the class! But something feels
funny because we have something i.i.d. and something sequential—martingales to the
rescue!</p>

<h3 id="martingale-differences">Martingale differences</h3>

<p>Martingales are perfectly equipped for taming our sequential process. In particular,
we’ll define the <strong>conditional expectation</strong> up to time <script type="math/tex">t-1</script> as</p>

<script type="math/tex; mode=display">\E_{t-1} [\, \cdot \,] = \E[\, \cdot \mid z_1,\dots,z_{t-1}].</script>

<p>For now, let’s focus on time <script type="math/tex">t</script> and take the expectation</p>

<script type="math/tex; mode=display">\E_{t-1} \big[ \color{red}{l_\mathcal{D}(w_t)}
	- \color{royalblue}{l\left(w_t, (x_t, y_t) \right)} \big] = 0</script>

<p>where <script type="math/tex">\color{red}{l_\mathcal{D}(w_t)}</script> is the <script type="math/tex">t</script>th model and
<script type="math/tex">\color{royalblue}{l\left(w_t, (x_t, y_t) \right)}</script> is the expected loss on the next
example. The expectation is zero because</p>

<ul>
  <li>the first <script type="math/tex">t-1</script> examples are held fixed, meaning <script type="math/tex">l_\mathcal{D}(w_t)</script> is a constant;
and</li>
  <li>the expected loss, <em>i.e.</em>, risk, at time <script type="math/tex">t</script> is
<script type="math/tex">\E \, \color{royalblue}{l\left(w_t, (x_t, y_t) \right)} = l_\mathcal{D}(w_t)</script> because
our data is i.i.d. from <script type="math/tex">\mathcal{D}</script>.</li>
</ul>

<p>Now, we average across our <script type="math/tex">n</script> examples</p>

<script type="math/tex; mode=display">\frac{1}{n} \sum_{t=1}^n
	\E_{t-1} \big[ l_\mathcal{D}(w_t) - l\left(w_t, (x_t, y_t) \right) \big]
	\stackrel{\textsf{notation}}{=}
	\frac{1}{n} \sum_{t=1}^n \E_{t-1} Z_t = 0</script>

<p>because <script type="math/tex">\E_{t-1} Z_t = 0</script> for each <script type="math/tex">t</script>. The random variables <script type="math/tex">Z_1,\dots,Z_n</script> form
a <strong>martingale difference sequence</strong>, which means we can martingale concentration results
to bound the average. In particular, with probability at least <script type="math/tex">1 - \delta</script>,</p>

<script type="math/tex; mode=display">\frac{1}{n} \sum_{t=1}^n Z_t \le \E \frac{1}{n} \sum_{t=1}^n Z_t +
	\mathcal{O} \left( \sqrt{\frac{1}{n} \ln \frac{1}{\delta}} \right).</script>

<p>With this fact, we can bound the risk of the average model!</p>

<h3 id="bounding-via-online-analysis">Bounding via online analysis</h3>

<p>Thanks to our martingale convergence result, this is a plug-and-play step. So, with
probability at least <script type="math/tex">1- \delta</script> with respect to the random draw of our sample <script type="math/tex">S</script>,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
l_\mathcal{D}(\hat{w}) &\le \frac{1}{n} \sum_{t=1}^n l_\mathcal{D}(w_t) \\
	&\le \frac{1}{n} \sum_{t=1}^n l\big(w_t, (x_t, y_t)\big) +
	\mathcal{O} \left( \sqrt{\frac{1}{n} \ln \frac{1}{\delta}} \right).
\end{align*} %]]></script>

<p><strong>Neural connection.</strong> This is the same object as what we had in the online learning
analysis because we have a loss rate on the first <script type="math/tex">n</script> models in the sequence—and
all we needed was a slightly modified (<em>i.e.</em>, <em>martingalified</em>) concentration result!</p>

<p>Our last step is to use online analysis to bound the loss rate</p>

<script type="math/tex; mode=display">\frac{1}{n} \sum_{t=1}^n l\big(w_t, (x_t, y_t)\big),</script>

<p>which will give us something that we can compare to the empirical risk minimizer. To make
things concrete, we’ll look at a specific example.</p>

<h3 id="linear-regression-with-squared-loss">Linear regression with squared loss</h3>

<p>We’re going to show that</p>

<script type="math/tex; mode=display">\frac{1}{n} \sum_{t=1}^n l\big(w_t, (x_t, y_t)\big) \le
	\min_{u: \|u\| \le U} \frac{1}{n} \sum_{t=1}^n l\big(u, (x_t, y_t)\big) +
		\delta (UX)^2 \sqrt{\frac{2}{T}}.</script>

<p>We’ll define <script type="math/tex">u^\star_t</script> as the minimizer of
<script type="math/tex">\frac{1}{n} \sum_{t=1}^n l\big(u, (x_t, y_t)\big)</script>, which allows us to say that</p>

<script type="math/tex; mode=display">\frac{1}{n} \sum_{t=1}^n l\big(u_n^\star, (x_t, y_t)\big) \le
	\frac{1}{n} \sum_{t=1}^n l\big(u^\star, (x_t, y_t)\big),</script>

<p>where <script type="math/tex">u^\star</script> is the model with the smallest statistical risk over <script type="math/tex">\mathcal{D}</script>,
not just <script type="math/tex">S</script>. So, <script type="math/tex">u^\star</script> may not have the smallest risk over <script type="math/tex">S</script>.
(<strong>Neural connection!</strong> This is the same trick we used in the Rademacher analysis in the
<a href="/jekyll/update/2018/05/06/learning-theory-1.html">statistical learning theory post</a>!)</p>

<p>Via the Chernoff bound, with probability at least <script type="math/tex">1-\delta</script> we have that</p>

<script type="math/tex; mode=display">\frac{1}{n} \sum_{t=1}^n l\big(u_n^\star, (x_t, y_t)\big) \le
	l_\mathcal{D}(u^\star) +
	\mathcal{O} \left( \sqrt{\frac{1}{n} \ln \frac{1}{\delta}} \right).</script>

<p>This last expression yields the bound on the average model:</p>

<script type="math/tex; mode=display">l_\mathcal{D}(\hat{w}) - l_\mathcal{D}(u^\star) \le + c\frac{UX^2}{\sqrt{n}}
	\mathcal{O} \left( \sqrt{\frac{1}{n} \ln \frac{1}{\delta}} \right),</script>

<p>where <script type="math/tex">c</script> is a constant. This is the same bound obtained from the Rademacher analysis!</p>

<h3 id="final-thoughts">Final thoughts</h3>

<ul>
  <li>The average loss on a sequence of models is a good proxy for the risk of the average
model.</li>
  <li>Local optimization performs well (up to constant factors), compared to empirical risk
minimization, <em>i.e.</em>, globabl optimization.
 	+ So, local optimization isn’t doing anything crazy—it’s doing something quite
    similar to global optimization.
    <ul>
      <li>In other words, doing local optimization and then averaging is a good and viable
strategy, especially when we’re dealing with big problems.</li>
    </ul>
  </li>
</ul>

<p>Now, let’s revisit our questions.</p>

<ol>
  <li>What does the online learning regret bound mean?
    <ul>
      <li>In some sense, it means that we’re bounding an object not unlike the empirical
risk.</li>
    </ul>
  </li>
  <li>How can that bound be used?
    <ul>
      <li>The online learning bound can be used to bound an <em>average</em> model, which is
comparable to a model found via empirical risk minimization.</li>
    </ul>
  </li>
  <li>How well does the loss rate on the model sequence generalize?
    <ul>
      <li>Up to constant factors, it generalizes as well as well the model found through
empirical risk minimization.</li>
    </ul>
  </li>
</ol>

<h3 id="next-up">Next up</h3>

<p>… a short and sweet example of how strong convexity speeds things up in the online
learning setting! Then a foray into the mind of R.A. Fisher with Brad Efron’s help.</p>

<h3 id="references">References</h3>

<p>This post is related to material from Nicolò Cesa-Bianchi’s lectures at the Bordeaux
Machine Learning Summer School in 2011. Watch them here: <a href="http://videolectures.net/mlss2011_cesa_bianchi_learningtheory/">MLSS 2011 lectures</a>.</p>


  </article>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://jakeknigge-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

                           
<script type="text/javascript" 
	src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  
<script type="text/x-mathjax-config">

MathJax.Hub.Config({

    showProcessingMessages: false,
    messageStyle: "none",
	CommonHTML: { linebreaks: { automatic: true } },
	SVG: { linebreaks: { automatic: true } },
    "HTML-CSS": {
      linebreaks: { automatic: true},
      availableFonts: ["TeX", "Neo-Euler", "STIX"],
      preferredFont: "TeX",
      webFont: "TeX",
        matchFontHeight: true,
        scale: 100,
        styles: {
            ".MathJax_Display": {
                margin: "0.85em 0em"
            }
        }
    },
    MathMenu: {  
        showFontMenu: true
    },
    TeX: {
      TagIndent: "0.25em",
      extensions: ["AMSmath.js","AMSsymbols.js","cancel.js"], //cancel.js needed only for I_29
      equationNumbers: {
        autoNumber: "AMS", formatNumber: function (n) { return EqnumPrefix() + n }
      },
      MultLineWidth: "80%",
		Macros: {  
			mat: ["{\\overset \\leftrightarrow #1}",1],  
			bra: ["{\\langle #1 |}",1],  
			ket: ["{| #1 \\rangle}",1],  
			sprod: ["{\\left\\langle #1 | #2 \\right\\rangle}",2],  
			tr: "\\text{tr}",  
			ev: ["{\\left\\langle #1 \\right \\rangle}",1],  
			threej: ["{\\left(\\begin{array}{ccc} #1 & #3 & #5 \\\\ #2 & #4 & #6 \\end{array} \\right)}",6],  
			dbar: "đ",
			d: "\\mathrm{d}",
			i: "\\mathrm{i}",
			eps: "\\varepsilon",
			sla: ["\\displaystyle{\\not} #1",1],  
			spinvec: ["\\left( \\begin{array}{c} #1 \\\\ #2 \\end{array} \\right)",2],  
			spinmat: ["\\left( \\begin{array}{cc} #1 & #2 \\\\ #3 & #4 \\end{array} \\right)",4],  
			reals: "\\mathbf{R}",
			integers: "\\mathbf{Z}",
			naturals: "\\mathbf{N}",
			symm: "\\mathbf{S}",
			ones: "\\mathbf{1}",
			prox: "\\mathbf{prox}",
			dom: "\\mathop{\\mathbf{dom}}",
			dist: "\\mathop{\\mathbf{dist}}",
			argmin: "\\mathop{\\mathrm{argmin}}",
			argmax: "\\mathop{\\mathrm{argmax}}",
			epi: "\\mathop{\\mathbf{epi}}",
			E: "\\mathbf{E}",
			Var: "\\mathbf{Var}",
			Cov: "\\mathbf{Cov}",
			prob: "\\mathbf{P}",
			ind: ["{\\mathbf{I}_{#1}}", 1],
			tvec: ["{\\boldsymbol{#1}}",1],
			relint: "\\mathop{\\mathbf{relint}}",
		}
    }
});
</script>

</div>
      </div>
    </div>

    <footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Eggink Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">
            
              Eggink Blog
            
            </li>
            
            <li><a class="u-email" href="mailto:jake@knigge.us">jake@knigge.us</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
  
  
  
  <li><a href="https://github.com/jakeknigge"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jakeknigge</span></a></li>
  
  
  
  <li><a href="https://www.twitter.com/JakeKnigge"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">JakeKnigge</span></a></li>
  
  
  
</ul>

      </div>

      <div class="footer-col footer-col-3">
        <p>Jake W. Knigge&#39;s blah, blah, blog... a place to discuss statistics, math, and  whatever else comes to mind.
</p>
      </div>
    </div>

  </div>

</footer>

    
  </body>

</html>